{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkHs1tyQ-4FJ",
        "outputId": "1f11ceab-d5fd-4620-8868-76d91bb0a6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting executorch\n",
            "  Downloading executorch-0.6.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting expecttest (from executorch)\n",
            "  Downloading expecttest-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from executorch) (25.2.10)\n",
            "Collecting hypothesis (from executorch)\n",
            "  Downloading hypothesis-6.135.31-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from executorch) (24.2)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.2.2)\n",
            "Collecting parameterized (from executorch)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from executorch) (8.3.5)\n",
            "Collecting pytest-xdist (from executorch)\n",
            "  Downloading pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pytest-rerunfailures (from executorch)\n",
            "  Downloading pytest_rerunfailures-15.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from executorch) (6.0.2)\n",
            "Collecting ruamel.yaml (from executorch)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from executorch) (1.13.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from executorch) (0.9.0)\n",
            "Collecting torch==2.7.0 (from executorch)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchaudio==2.7.0 (from executorch)\n",
            "  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchvision==0.22.0 (from executorch)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: torchao==0.10.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from executorch) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.18.0)\n",
            "Collecting sympy (from executorch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->executorch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->executorch)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.22.0->executorch) (11.2.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->executorch) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis->executorch) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis->executorch) (2.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->executorch) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->executorch) (1.6.0)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->executorch)\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->executorch)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->executorch) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->executorch) (3.0.2)\n",
            "Downloading executorch-0.6.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading expecttest-0.3.0-py3-none-any.whl (8.2 kB)\n",
            "Downloading hypothesis-6.135.31-py3-none-any.whl (522 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.8/522.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pytest_rerunfailures-15.1-py3-none-any.whl (13 kB)\n",
            "Downloading pytest_xdist-3.8.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, ruamel.yaml.clib, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hypothesis, expecttest, execnet, ruamel.yaml, pytest-xdist, pytest-rerunfailures, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio, executorch\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed execnet-2.1.1 executorch-0.6.0 expecttest-0.3.0 hypothesis-6.135.31 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 parameterized-0.9.0 pytest-rerunfailures-15.1 pytest-xdist-3.8.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install executorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qZqYR51d-fkX"
      },
      "outputs": [],
      "source": [
        "#@title SAMNet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FastSal(nn.Module):\n",
        "    def __init__(self, pretrained=None):\n",
        "        super(FastSal, self).__init__()\n",
        "        self.context_path = VAMM_backbone(pretrained)\n",
        "        self.pyramid_pooling = PyramidPooling(128, 128)\n",
        "        self.prepare = nn.ModuleList([\n",
        "                convbnrelu(128, 128, k=1, s=1, p=0, relu=False),\n",
        "                convbnrelu(96, 96, k=1, s=1, p=0, relu=False),\n",
        "                convbnrelu(64, 64, k=1, s=1, p=0, relu=False),\n",
        "                convbnrelu(32, 32, k=1, s=1, p=0, relu=False),\n",
        "                convbnrelu(16, 16, k=1, s=1, p=0, relu=False)\n",
        "                ])\n",
        "        self.fuse = nn.ModuleList([\n",
        "                DSConv3x3(128, 96, dilation=1),\n",
        "                DSConv3x3(96, 64, dilation=2),\n",
        "                DSConv5x5(64, 32, dilation=2),\n",
        "                DSConv5x5(32, 16, dilation=2),\n",
        "                DSConv5x5(16, 16, dilation=2)\n",
        "                ])\n",
        "        self.heads = nn.ModuleList([\n",
        "                SalHead(in_channel=96),\n",
        "                SalHead(in_channel=64),\n",
        "                SalHead(in_channel=32),\n",
        "                SalHead(in_channel=16),\n",
        "                SalHead(in_channel=16)\n",
        "                ])\n",
        "\n",
        "    def forward(self, x): # (3, 1)\n",
        "        ct_stage1, ct_stage2, ct_stage3, ct_stage4, ct_stage5 = self.context_path(x)\n",
        "        # (16, 1/2) (32, 1/4) (64, 1/8)  (96, 1/16) (128, 1/32)\n",
        "        ct_stage6 = self.pyramid_pooling(ct_stage5)                          # (128, 1/32)\n",
        "\n",
        "        fused_stage1 = self.fuse[0](self.prepare[0](ct_stage5) + ct_stage6)  # (96, 1/32)\n",
        "        refined1 = interpolate(fused_stage1, ct_stage4.size()[2:])           # (96, 1/16)\n",
        "\n",
        "        fused_stage2 = self.fuse[1](self.prepare[1](ct_stage4) + refined1)   # (64, 1/16)\n",
        "        refined2 = interpolate(fused_stage2, ct_stage3.size()[2:])           # (64, 1/8)\n",
        "\n",
        "        fused_stage3 = self.fuse[2](self.prepare[2](ct_stage3) + refined2)   # (32, 1/8)\n",
        "        refined3 = interpolate(fused_stage3, ct_stage2.size()[2:]) \t\t     # (32, 1/4)\n",
        "\n",
        "        fused_stage4 = self.fuse[3](self.prepare[3](ct_stage2) + refined3)   # (16, 1/4)\n",
        "        refined4 = interpolate(fused_stage4, ct_stage1.size()[2:])\t\t     # (16, 1/2)\n",
        "\n",
        "        fused_stage5 = self.fuse[4](self.prepare[4](ct_stage1) + refined4)   # (16, 1/2)\n",
        "\n",
        "        output_side1 = interpolate(self.heads[0](fused_stage1), x.size()[2:])\n",
        "        output_side2 = interpolate(self.heads[1](fused_stage2), x.size()[2:])\n",
        "        output_side3 = interpolate(self.heads[2](fused_stage3), x.size()[2:])\n",
        "        output_side4 = interpolate(self.heads[3](fused_stage4), x.size()[2:])\n",
        "        output_main  = interpolate(self.heads[4](fused_stage5), x.size()[2:])\n",
        "\n",
        "        return output_main, output_side1, output_side2, output_side3,output_side4  #torch.cat([output_main, output_side1, output_side2, output_side3, output_side4], dim=1)\n",
        "\n",
        "\n",
        "interpolate = lambda x, size: F.interpolate(x, size=size, mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "class convbnrelu(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, k=3, s=1, p=1, g=1, d=1, bias=False, bn=True, relu=True):\n",
        "        super(convbnrelu, self).__init__()\n",
        "        conv = [nn.Conv2d(in_channel, out_channel, k, s, p, dilation=d, groups=g, bias=bias)]\n",
        "        if bn:\n",
        "            conv.append(nn.BatchNorm2d(out_channel))\n",
        "        if relu:\n",
        "            conv.append(nn.ReLU(inplace=True))\n",
        "        self.conv = nn.Sequential(*conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DSConv3x3(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride=1, dilation=1, relu=True):\n",
        "        super(DSConv3x3, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                convbnrelu(in_channel, in_channel, k=3, s=stride, p=dilation, d=dilation, g=in_channel),\n",
        "                convbnrelu(in_channel, out_channel, k=1, s=1, p=0, relu=relu)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DSConv5x5(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride=1, dilation=1, relu=True):\n",
        "        super(DSConv5x5, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                convbnrelu(in_channel, in_channel, k=5, s=stride, p=2*dilation, d=dilation, g=in_channel),\n",
        "                convbnrelu(in_channel, out_channel, k=1, s=1, p=0, relu=relu)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class SalHead(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super(SalHead, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                nn.Dropout2d(p=0.1),\n",
        "                nn.Conv2d(in_channel, 1, 1, stride=1, padding=0),\n",
        "                nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class VAMM_backbone(nn.Module):\n",
        "    def __init__(self, pretrained=None):\n",
        "        super(VAMM_backbone, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                convbnrelu(3, 16, k=3, s=2, p=1),\n",
        "                VAMM(16, dilation_level=[1,2,3])\n",
        "                )\n",
        "        self.layer2 = nn.Sequential(\n",
        "                DSConv3x3(16, 32, stride=2),\n",
        "                VAMM(32, dilation_level=[1,2,3])\n",
        "                )\n",
        "        self.layer3 = nn.Sequential(\n",
        "                DSConv3x3(32, 64, stride=2),\n",
        "                VAMM(64, dilation_level=[1,2,3]),\n",
        "                VAMM(64, dilation_level=[1,2,3]),\n",
        "                VAMM(64, dilation_level=[1,2,3])\n",
        "                )\n",
        "        self.layer4 = nn.Sequential(\n",
        "                DSConv3x3(64, 96, stride=2),\n",
        "                VAMM(96, dilation_level=[1,2,3]),\n",
        "                VAMM(96, dilation_level=[1,2,3]),\n",
        "                VAMM(96, dilation_level=[1,2,3]),\n",
        "                VAMM(96, dilation_level=[1,2,3]),\n",
        "                VAMM(96, dilation_level=[1,2,3]),\n",
        "                VAMM(96, dilation_level=[1,2,3])\n",
        "                )\n",
        "        self.layer5 = nn.Sequential(\n",
        "                DSConv3x3(96, 128, stride=2),\n",
        "                VAMM(128, dilation_level=[1,2]),\n",
        "                VAMM(128, dilation_level=[1,2]),\n",
        "                VAMM(128, dilation_level=[1,2])\n",
        "                )\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.load_state_dict(torch.load(pretrained))\n",
        "            print('Pretrained model loaded!')\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = self.layer5(out4)\n",
        "\n",
        "        return out1, out2, out3, out4, out5\n",
        "\n",
        "\n",
        "class VAMM(nn.Module):\n",
        "    def __init__(self, channel, dilation_level=[1,2,4,8], reduce_factor=4):\n",
        "        super(VAMM, self).__init__()\n",
        "        self.planes = channel\n",
        "        self.dilation_level = dilation_level\n",
        "        self.conv = DSConv3x3(channel, channel, stride=1)\n",
        "        self.branches = nn.ModuleList([\n",
        "                DSConv3x3(channel, channel, stride=1, dilation=d) for d in dilation_level\n",
        "                ])\n",
        "        ### ChannelGate\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = convbnrelu(channel, channel, 1, 1, 0, bn=True, relu=True)\n",
        "        self.fc2 = nn.Conv2d(channel, (len(self.dilation_level) + 1) * channel, 1, 1, 0, bias=False)\n",
        "        self.fuse = convbnrelu(channel, channel, k=1, s=1, p=0, relu=False)\n",
        "        ### SpatialGate\n",
        "        self.convs = nn.Sequential(\n",
        "                convbnrelu(channel, channel // reduce_factor, 1, 1, 0, bn=True, relu=True),\n",
        "                DSConv3x3(channel // reduce_factor, channel // reduce_factor, stride=1, dilation=2),\n",
        "                DSConv3x3(channel // reduce_factor, channel // reduce_factor, stride=1, dilation=4),\n",
        "                nn.Conv2d(channel // reduce_factor, 1, 1, 1, 0, bias=False)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.conv(x)\n",
        "        brs = [branch(conv) for branch in self.branches]\n",
        "        brs.append(conv)\n",
        "        gather = sum(brs)\n",
        "\n",
        "        ### ChannelGate\n",
        "        d = self.gap(gather)\n",
        "        d = self.fc2(self.fc1(d))\n",
        "        d = torch.unsqueeze(d, dim=1).view(-1, len(self.dilation_level) + 1, self.planes, 1, 1)\n",
        "\n",
        "        ### SpatialGate\n",
        "        s = self.convs(gather).unsqueeze(1)\n",
        "\n",
        "        ### Fuse two gates\n",
        "        f = d * s\n",
        "        f = F.softmax(f, dim=1)\n",
        "\n",
        "        return self.fuse(sum([brs[i] * f[:, i, ...] for i in range(len(self.dilation_level) + 1)]))\t+ x\n",
        "\n",
        "\n",
        "class PyramidPooling(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "        hidden_channel = int(in_channel / 4)\n",
        "        self.conv1 = convbnrelu(in_channel, hidden_channel, k=1, s=1, p=0)\n",
        "        self.conv2 = convbnrelu(in_channel, hidden_channel, k=1, s=1, p=0)\n",
        "        self.conv3 = convbnrelu(in_channel, hidden_channel, k=1, s=1, p=0)\n",
        "        self.conv4 = convbnrelu(in_channel, hidden_channel, k=1, s=1, p=0)\n",
        "        self.out = convbnrelu(in_channel*2, out_channel, k=1, s=1, p=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "        feat1 = interpolate(self.conv1(F.adaptive_avg_pool2d(x, 1)), size)\n",
        "        feat2 = interpolate(self.conv2(F.adaptive_avg_pool2d(x, 2)), size)\n",
        "        feat3 = interpolate(self.conv3(F.adaptive_avg_pool2d(x, 3)), size)\n",
        "        feat4 = interpolate(self.conv4(F.adaptive_avg_pool2d(x, 6)), size)\n",
        "        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "HVayiRvVBPD1"
      },
      "outputs": [],
      "source": [
        "#@title HVPNet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.PReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class gycblock(nn.Module):\n",
        "    def __init__(self,channels_in,channels_out):\n",
        "        super(gycblock, self).__init__()\n",
        "        self.recp7 = nn.Sequential(\n",
        "            BasicConv(channels_in*1, channels_in, kernel_size=7, dilation=1, padding=3, groups=channels_in, bias=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False),\n",
        "\n",
        "            BasicConv(channels_in, channels_in, kernel_size=3, dilation=7, padding=7, groups=channels_in, bias=False,\n",
        "                      relu=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.recp5 = nn.Sequential(\n",
        "            BasicConv(channels_in*2, channels_in, kernel_size=5, dilation=1, padding=2, groups=channels_in, bias=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False),\n",
        "\n",
        "            BasicConv(channels_in, channels_in, kernel_size=3, dilation=5, padding=5, groups=channels_in, bias=False,\n",
        "                      relu=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.recp3 = nn.Sequential(\n",
        "            BasicConv(channels_in*3, channels_in, kernel_size=3, dilation=1, padding=1, groups=channels_in, bias=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False),\n",
        "\n",
        "            BasicConv(channels_in, channels_in, kernel_size=3, dilation=3, padding=3, groups=channels_in, bias=False,\n",
        "                      relu=False),\n",
        "            BasicConv(channels_in, channels_in, kernel_size=1, dilation=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.recp1 = nn.Sequential(\n",
        "            BasicConv(channels_in*4, channels_out, kernel_size=1, dilation=1, bias=False,relu=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.recp7(x)\n",
        "        x1 = self.recp5(torch.cat([x,x0],dim=1))\n",
        "        x2 = self.recp3(torch.cat([x,x0,x1],dim=1))\n",
        "        out = self.recp1(torch.cat([x,x0,x1,x2],dim=1))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Channelatt(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(Channelatt, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class Spatialatt(nn.Module):\n",
        "    def __init__(self,channels_in):\n",
        "        super(Spatialatt, self).__init__()\n",
        "        kernel_size = 3\n",
        "        self.spatial = BasicConv(channels_in, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_out = self.spatial(x)\n",
        "        scale = torch.sigmoid(x_out) # broadcasting\n",
        "\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class residual_att(nn.Module):\n",
        "    def __init__(self,channels_in, reduction=4):\n",
        "        super(residual_att, self).__init__()\n",
        "        self.channel_att=Channelatt(channels_in, reduction=reduction)\n",
        "        self.spatialatt=Spatialatt(channels_in)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.spatialatt(self.channel_att(x))\n",
        "\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, nIn, nOut, add=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(nIn, nOut, 3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(nOut)\n",
        "        self.act = nn.PReLU(nOut)\n",
        "        self.add = add\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        if self.add:\n",
        "            output = input + output\n",
        "\n",
        "        output = self.bn(output)\n",
        "        output = self.act(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class DilatedParallelConvBlockD2(nn.Module):\n",
        "    def __init__(self, nIn, nOut, add=False):\n",
        "        super(DilatedParallelConvBlockD2, self).__init__()\n",
        "        n = int(np.ceil(nOut / 2.))\n",
        "        n2 = nOut - n\n",
        "\n",
        "        self.conv0 = nn.Conv2d(nIn, nOut, 1, stride=1, padding=0, dilation=1, bias=False)\n",
        "        self.conv1 = nn.Conv2d(n, n, 3, stride=1, padding=1, dilation=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(n2, n2, 3, stride=1, padding=2, dilation=2, bias=False)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(nOut)\n",
        "        #self.act = nn.PReLU(nOut)\n",
        "        self.add = add\n",
        "\n",
        "    def forward(self, input):\n",
        "        in0 = self.conv0(input)\n",
        "        in1, in2 = torch.chunk(in0, 2, dim=1)\n",
        "        b1 = self.conv1(in1)\n",
        "        b2 = self.conv2(in2)\n",
        "        output = torch.cat([b1, b2], dim=1)\n",
        "\n",
        "        if self.add:\n",
        "            output = input + output\n",
        "        output = self.bn(output)\n",
        "        #output = self.act(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class DownsamplerBlockDepthwiseConv(nn.Module):\n",
        "    def __init__(self, nIn, nOut):\n",
        "        super(DownsamplerBlockDepthwiseConv, self).__init__()\n",
        "        self.nIn = nIn\n",
        "        self.nOut = nOut\n",
        "\n",
        "        if self.nIn < self.nOut:\n",
        "            self.conv0 = nn.Conv2d(nIn, nOut-nIn, 1, stride=1, padding=0, dilation=1, groups=1, bias=False)\n",
        "            self.conv1 = nn.Conv2d(nOut-nIn, nOut-nIn, 5, stride=2, padding=2, dilation=1, groups=nOut-nIn, bias=False)\n",
        "            #self.pool = nn.MaxPool2d(2, stride=2)\n",
        "            self.pool = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        else:\n",
        "            self.conv0 = nn.Conv2d(nIn, nOut, 1, stride=1, padding=0, dilation=1, groups=1, bias=False)\n",
        "            self.conv1 = nn.Conv2d(nOut, nOut, 5, stride=2, padding=2, dilation=1, groups=nOut, bias=False)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(nOut)\n",
        "        self.act = nn.PReLU(nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.nIn < self.nOut:\n",
        "            output = torch.cat([self.conv1(self.conv0(input)), self.pool(input)], 1)\n",
        "        else:\n",
        "            output = self.conv1(self.conv0(input))\n",
        "\n",
        "        output = self.bn(output)\n",
        "        output = self.act(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class DownsamplerBlockConv(nn.Module):\n",
        "    def __init__(self, nIn, nOut):\n",
        "        super(DownsamplerBlockConv, self).__init__()\n",
        "        self.nIn = nIn\n",
        "        self.nOut = nOut\n",
        "\n",
        "        if self.nIn < self.nOut:\n",
        "            self.conv = nn.Conv2d(nIn, nOut-nIn, 3, stride=2, padding=1, bias=False)\n",
        "            #self.pool = nn.MaxPool2d(2, stride=2)\n",
        "            self.pool = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(nIn, nOut, 3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(nOut)\n",
        "        self.act = nn.PReLU(nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.nIn < self.nOut:\n",
        "            output = torch.cat([self.conv(input), self.pool(input)], 1)\n",
        "        else:\n",
        "            output = self.conv(input)\n",
        "\n",
        "        output = self.bn(output)\n",
        "        output = self.act(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    def __init__(self, P1=1, P2=1, P3=3, P4=5, reduction=4, pretrained=None):\n",
        "        super(Backbone, self).__init__()\n",
        "        self.level1_0 = DownsamplerBlockConv(3, 16)\n",
        "        self.level1 = nn.ModuleList()\n",
        "        for i in range(0, P1):\n",
        "            self.level1.append(ConvBlock(16, 16))\n",
        "        self.level1.append(residual_att(16, reduction=reduction))\n",
        "        self.branch1 = nn.Conv2d(16, 16, 1, stride=1, padding=0,bias=False)\n",
        "        self.br1 = nn.Sequential(nn.BatchNorm2d(16), nn.PReLU(16))\n",
        "\n",
        "        self.level2_0 = DownsamplerBlockDepthwiseConv(16, 32)\n",
        "        self.level2 = nn.ModuleList()\n",
        "        for i in range(0, P2):\n",
        "            self.level2.append(nn.Dropout2d(0.1, True))\n",
        "            self.level2.append(gycblock(32, 32))\n",
        "            self.level2.append(residual_att(32, reduction=reduction))\n",
        "        self.branch2 = nn.Conv2d(32, 32, 1, stride=1, padding=0,bias=False)\n",
        "        self.br2 = nn.Sequential(nn.BatchNorm2d(32), nn.PReLU(32))\n",
        "\n",
        "        self.level3_0 = DownsamplerBlockDepthwiseConv(32, 64)\n",
        "        self.level3 = nn.ModuleList()\n",
        "        for i in range(0, P3):\n",
        "            self.level3.append(nn.Dropout2d(0.1, True))\n",
        "            self.level3.append(gycblock(64, 64))\n",
        "            self.level3.append(residual_att(64, reduction=reduction))\n",
        "        self.branch3 = nn.Conv2d(64, 64, 1, stride=1, padding=0,bias=False)\n",
        "        self.br3 = nn.Sequential(nn.BatchNorm2d(64), nn.PReLU(64))\n",
        "\n",
        "        self.level4_0 = DownsamplerBlockDepthwiseConv(64, 128)\n",
        "        self.level4 = nn.ModuleList()\n",
        "        for i in range(0, P4):\n",
        "            self.level4.append(nn.Dropout2d(0.1, True))\n",
        "            self.level4.append(gycblock(128, 128))\n",
        "            self.level4.append(residual_att(128, reduction=reduction))\n",
        "        self.branch4 = nn.Conv2d(128, 128, 1, stride=1, padding=0,bias=False)\n",
        "        self.br4 = nn.Sequential(nn.BatchNorm2d(128), nn.PReLU(128))\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.load_state_dict(torch.load(pretrained))\n",
        "            print('Pretrained Model Loaded!')\n",
        "\n",
        "    def forward(self, input):\n",
        "        output1_0 = self.level1_0(input)\n",
        "\n",
        "        output1 = output1_0\n",
        "        for layer in self.level1:\n",
        "            output1 = layer(output1)\n",
        "        output1 = self.br1(self.branch1(output1_0) + output1)\n",
        "\n",
        "        output2_0 = self.level2_0(output1)\n",
        "        output2 = output2_0\n",
        "        for layer in self.level2:\n",
        "            output2 = layer(output2)\n",
        "        output2 = self.br2(self.branch2(output2_0) + output2)\n",
        "\n",
        "        output3_0 = self.level3_0(output2)\n",
        "        output3 = output3_0\n",
        "        for layer in self.level3:\n",
        "            output3 = layer(output3)\n",
        "        output3 = self.br3(self.branch3(output3_0) + output3)\n",
        "\n",
        "        output4_0 = self.level4_0(output3)\n",
        "        output4 = output4_0\n",
        "        for layer in self.level4:\n",
        "            output4 = layer(output4)\n",
        "        output4 = self.br4(self.branch4(output4_0) + output4)\n",
        "\n",
        "        return output1, output2, output3, output4\n",
        "\n",
        "\n",
        "class FastSal(nn.Module):\n",
        "    '''\n",
        "    This class defines the MiniNetV2 network\n",
        "    '''\n",
        "    def __init__(self, P1=1, P2=1, P3=3, P4=5, reduction=4, pretrained=None):\n",
        "        super(FastSal, self).__init__()\n",
        "        self.backbone = Backbone(P1, P2, P3, P4, reduction, pretrained)\n",
        "\n",
        "        self.up3_conv4 = DilatedParallelConvBlockD2(128, 64)\n",
        "        self.up3_conv3 = nn.Conv2d(64, 64, 1, stride=1, padding=0,bias=False)\n",
        "        self.up3_bn3 = nn.BatchNorm2d(64)\n",
        "        self.up3_act = nn.PReLU(64)\n",
        "\n",
        "        self.up2_conv3 = DilatedParallelConvBlockD2(64, 32)\n",
        "        self.up2_conv2 = nn.Conv2d(32, 32, 1, stride=1, padding=0,bias=False)\n",
        "        self.up2_bn2 = nn.BatchNorm2d(32)\n",
        "        self.up2_act = nn.PReLU(32)\n",
        "\n",
        "        self.up1_conv2 = DilatedParallelConvBlockD2(32, 16)\n",
        "        self.up1_conv1 = nn.Conv2d(16, 16, 1, stride=1, padding=0,bias=False)\n",
        "        self.up1_bn1 = nn.BatchNorm2d(16)\n",
        "        self.up1_act = nn.PReLU(16)\n",
        "\n",
        "        self.classifier4 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(128, 1, 1, stride=1, padding=0, bias=False))\n",
        "        self.classifier3 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, 1, 1, stride=1, padding=0, bias=False))\n",
        "        self.classifier2 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False))\n",
        "        self.classifier1 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(16, 1, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "    def forward(self, input):\n",
        "        output1, output2, output3, output4 = self.backbone(input)\n",
        "\n",
        "        up4 = F.interpolate(output4, output3.size()[2:], mode='bilinear', align_corners=False)\n",
        "        up3_conv4 = self.up3_conv4(up4)\n",
        "        up3_conv3 = self.up3_bn3(self.up3_conv3(output3))\n",
        "        up3 = self.up3_act(up3_conv4 + up3_conv3)\n",
        "\n",
        "        up3 = F.interpolate(up3, output2.size()[2:], mode='bilinear', align_corners=False)\n",
        "        up2_conv3 = self.up2_conv3(up3)\n",
        "        up2_conv2 = self.up2_bn2(self.up2_conv2(output2))\n",
        "        up2 = self.up2_act(up2_conv3 + up2_conv2)\n",
        "\n",
        "        up2 = F.interpolate(up2, output1.size()[2:], mode='bilinear', align_corners=False)\n",
        "        up1_conv2 = self.up1_conv2(up2)\n",
        "        up1_conv1 = self.up1_bn1(self.up1_conv1(output1))\n",
        "        up1 = self.up1_act(up1_conv2 + up1_conv1)\n",
        "\n",
        "        classifier4 = torch.sigmoid(self.classifier4(up4))\n",
        "        classifier3 = torch.sigmoid(self.classifier3(up3))\n",
        "        classifier2 = torch.sigmoid(self.classifier2(up2))\n",
        "        classifier1 = torch.sigmoid(self.classifier1(up1))\n",
        "        classifier4 = F.interpolate(classifier4, input.size()[2:], mode='bilinear', align_corners=False)\n",
        "        classifier3 = F.interpolate(classifier3, input.size()[2:], mode='bilinear', align_corners=False)\n",
        "        classifier2 = F.interpolate(classifier2, input.size()[2:], mode='bilinear', align_corners=False)\n",
        "        classifier1 = F.interpolate(classifier1, input.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        return classifier1, classifier2, classifier3, classifier4 #torch.cat([classifier1, classifier2, classifier3, classifier4], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedInterpolate(nn.Module):\n",
        "    def __init__(self, target_size):\n",
        "        super().__init__()\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.interpolate(x, size=self.target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "# Example use\n",
        "self.upsample_to_224 = FixedInterpolate((224, 224))\n",
        "\n",
        "x = self.upsample_to_224(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Atv6C7uu2mOE",
        "outputId": "3627b432-0d70-4db3-9c59-644bf4062981"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-120378210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Example use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_to_224\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFixedInterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_to_224\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "VxVtZkxlFi1T"
      },
      "outputs": [],
      "source": [
        "#@title Seanet\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.nn import Parameter\n",
        "\n",
        "\n",
        "try:\n",
        "    from torchvision.models.utils import load_state_dict_from_url  # torchvision 0.4+\n",
        "except ModuleNotFoundError:\n",
        "    try:\n",
        "        from torch.hub import load_state_dict_from_url  # torch 1.x\n",
        "    except ModuleNotFoundError:\n",
        "        from torch.utils.model_zoo import load_url as load_state_dict_from_url  # torch 0.4.1\n",
        "\n",
        "model_urls = {\n",
        "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, dilation=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        if dilation != 1:\n",
        "            padding = dilation\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, dilation=dilation,\n",
        "                      bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, dilation=1):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, dilation=dilation),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, pretrained=None, num_classes=1000, width_mult=1.0):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "        inverted_residual_setting = [\n",
        "            # t, c, n, s, d\n",
        "            [1, 16, 1, 1, 1], # conv1 112*112*16\n",
        "            [6, 24, 2, 2, 1], # conv2 56*56*24\n",
        "            [6, 32, 3, 2, 1], # conv3 28*28*32\n",
        "            [6, 64, 4, 2, 1],\n",
        "            [6, 96, 3, 1, 1], # conv4 14*14*96\n",
        "            [6, 160, 3, 2, 1],\n",
        "            [6, 320, 1, 1, 1], # conv5 7*7*320\n",
        "        ]\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = int(input_channel * width_mult)\n",
        "        self.last_channel = int(last_channel * max(1.0, width_mult))\n",
        "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s, d in inverted_residual_setting:\n",
        "            output_channel = int(c * width_mult)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                dilation = d if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, dilation=d))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for idx, m in enumerate(self.features):\n",
        "            x = m(x)\n",
        "            if idx in [1, 3, 6, 13, 17]:\n",
        "                res.append(x)\n",
        "        return res\n",
        "\n",
        "\n",
        "def mobilenet_v2(pretrained=True, progress=True, **kwargs):\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
        "                                              progress=progress)\n",
        "        print(\"loading imagenet pretrained mobilenetv2\")\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(\"loaded imagenet pretrained mobilenetv2\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class seanet_convbnrelu(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, k=3, s=1, p=1, g=1, d=1, bias=False, bn=True, relu=True):\n",
        "        super(seanet_convbnrelu, self).__init__()\n",
        "        conv = [nn.Conv2d(in_channel, out_channel, k, s, p, dilation=d, groups=g, bias=bias)]\n",
        "        if bn:\n",
        "            conv.append(nn.BatchNorm2d(out_channel))\n",
        "        if relu:\n",
        "            conv.append(nn.PReLU(out_channel))\n",
        "        self.conv = nn.Sequential(*conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DSConv3x3(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride=1, dilation=1, relu=True):\n",
        "        super(DSConv3x3, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            seanet_convbnrelu(in_channel, in_channel, k=3, s=stride, p=dilation, d=dilation, g=in_channel),\n",
        "            seanet_convbnrelu(in_channel, out_channel, k=1, s=1, p=0, relu=relu)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=4):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        out = max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=3):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = max_out\n",
        "        x = self.conv1(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "\n",
        "# Channel-wise Correlation\n",
        "class CCorrM(nn.Module):\n",
        "    def __init__(self, all_channel=24, all_dim=128):\n",
        "        super(CCorrM, self).__init__()\n",
        "        self.linear_e = nn.Linear(all_channel, all_channel, bias=False) #weight\n",
        "        self.channel = all_channel\n",
        "        self.dim = all_dim * all_dim\n",
        "        self.conv1 = DSConv3x3(all_channel, all_channel, stride=1)\n",
        "        self.conv2 = DSConv3x3(all_channel, all_channel, stride=1)\n",
        "\n",
        "    def forward(self, exemplar, query):  # exemplar: f1, query: f2\n",
        "        fea_size = query.size()[2:]\n",
        "        exemplar = F.interpolate(exemplar, size=fea_size, mode=\"bilinear\", align_corners=True)\n",
        "        all_dim = fea_size[0] * fea_size[1]\n",
        "        exemplar_flat = exemplar.view(-1, self.channel, all_dim)  # N,C1,H,W -> N,C1,H*W\n",
        "        query_flat = query.view(-1, self.channel, all_dim)  # N,C2,H,W -> N,C2,H*W\n",
        "        exemplar_t = torch.transpose(exemplar_flat, 1, 2).contiguous()  # batchsize x dim x num, N,H*W,C1\n",
        "        exemplar_corr = self.linear_e(exemplar_t)  # batchsize x dim x num, N,H*W,C1\n",
        "        A = torch.bmm(query_flat, exemplar_corr)  # ChannelCorrelation: N,C2,H*W x N,H*W,C1 = N,C2,C1\n",
        "\n",
        "        A1 = F.softmax(A.clone(), dim=2)  # N,C2,C1. dim=2 is row-wise norm. Sr\n",
        "        B = F.softmax(torch.transpose(A, 1, 2), dim=2)  # N,C1,C2 column-wise norm. Sc\n",
        "        query_att = torch.bmm(A1, exemplar_flat).contiguous()  # N,C2,C1 X N,C1,H*W = N,C2,H*W\n",
        "        exemplar_att = torch.bmm(B, query_flat).contiguous()  # N,C1,C2 X N,C2,H*W = N,C1,H*W\n",
        "\n",
        "        exemplar_att = exemplar_att.view(-1, self.channel, fea_size[0], fea_size[1])  # N,C1,H*W -> N,C1,H,W\n",
        "        exemplar_out = self.conv1(exemplar_att + exemplar)\n",
        "\n",
        "        query_att = query_att.view(-1, self.channel, fea_size[0], fea_size[1])  # N,C2,H*W -> N,C2,H,W\n",
        "        #query_out = self.conv1(query_att + query) #https://github.com/MathLee/SeaNet/issues/2\n",
        "\n",
        "        query_out = self.conv2(query_att + query)\n",
        "\n",
        "        return exemplar_out, query_out\n",
        "\"\"\"\n",
        "class CCorrM(nn.Module):\n",
        "    def __init__(self, all_channel=24, all_dim=128):\n",
        "        super(CCorrM, self).__init__()\n",
        "        self.linear_e = nn.Linear(all_channel, all_channel, bias=False) #weight\n",
        "        self.channel = all_channel\n",
        "        self.dim = all_dim * all_dim\n",
        "        self.conv1 = DSConv3x3(all_channel, all_channel, stride=1)\n",
        "        self.conv2 = DSConv3x3(all_channel, all_channel, stride=1)\n",
        "\n",
        "    def forward(self, exemplar, query):  # exemplar: f1, query: f2\n",
        "        fea_size = query.size()[2:]\n",
        "        exemplar = F.interpolate(exemplar, size=fea_size, mode=\"bilinear\", align_corners=True)\n",
        "        all_dim = fea_size[0] * fea_size[1]\n",
        "        exemplar_flat = exemplar.view(-1, self.channel, all_dim)  # N,C1,H,W -> N,C1,H*W\n",
        "        query_flat = query.view(-1, self.channel, all_dim)  # N,C2,H,W -> N,C2,H*W\n",
        "        exemplar_t = torch.transpose(exemplar_flat, 1, 2).contiguous()  # batchsize x dim x num, N,H*W,C1\n",
        "        exemplar_corr = self.linear_e(exemplar_t)  # batchsize x dim x num, N,H*W,C1\n",
        "        A = torch.bmm(query_flat, exemplar_corr)  # ChannelCorrelation: N,C2,H*W x N,H*W,C1 = N,C2,C1\n",
        "\n",
        "        A1 = F.softmax(A.clone(), dim=2)  # N,C2,C1. dim=2 is row-wise norm. Sr\n",
        "        B = F.softmax(torch.transpose(A, 1, 2), dim=2)  # N,C1,C2 column-wise norm. Sc\n",
        "        query_att = torch.bmm(A1, exemplar_flat).contiguous()  # N,C2,C1 X N,C1,H*W = N,C2,H*W\n",
        "        exemplar_att = torch.bmm(B, query_flat).contiguous()  # N,C1,C2 X N,C2,H*W = N,C1,H*W\n",
        "\n",
        "        exemplar_att = exemplar_att.view(-1, self.channel, fea_size[0], fea_size[1])  # N,C1,H*W -> N,C1,H,W\n",
        "        exemplar_out = self.conv1(exemplar_att + exemplar)\n",
        "\n",
        "        query_att = query_att.view(-1, self.channel, fea_size[0], fea_size[1])  # N,C2,H*W -> N,C2,H,W\n",
        "        query_out = self.conv1(query_att + query)\n",
        "\n",
        "        return exemplar_out, query_out\n",
        "\"\"\"\n",
        "\n",
        "# Edge-based Enhancement Unit (EEU)\n",
        "class EEU(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super(EEU, self).__init__()\n",
        "        self.avg_pool = nn.AvgPool2d((3, 3), stride=1, padding=1)\n",
        "        self.conv_1 = nn.Conv2d(in_channel, in_channel, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channel)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.PReLU = nn.PReLU(in_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        edge = x - self.avg_pool(x)  # Xi=X-Avgpool(X)\n",
        "        weight = self.sigmoid(self.bn1(self.conv_1(edge)))\n",
        "        out = weight * x + x\n",
        "        return self.PReLU(edge), out\n",
        "\n",
        "\n",
        "# Edge Self-Alignment Module (ESAM)\n",
        "class ESAM(nn.Module):\n",
        "    def __init__(self, channel1=16, channel2=24):\n",
        "        super(ESAM, self).__init__()\n",
        "\n",
        "        self.smooth1 = DSConv3x3(channel1, channel2, stride=1, dilation=1)  # 16channel-> 24channel\n",
        "\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.smooth2 = DSConv3x3(channel2, channel2, stride=1, dilation=1)  # 24channel-> 24channel\n",
        "\n",
        "        self.eeu1 = EEU(channel2)\n",
        "        self.eeu2 = EEU(channel2)\n",
        "        self.ChannelCorrelation = CCorrM(channel2, 128)\n",
        "\n",
        "    def forward(self, x1, x2):  # x1 16*144*14; x2 24*72*72\n",
        "\n",
        "        x1_1 = self.smooth1(x1)\n",
        "        edge1, x1_2 = self.eeu1(x1_1)\n",
        "\n",
        "        x2_1 = self.smooth2(self.upsample2(x2))\n",
        "        edge2, x2_2 = self.eeu2(x2_1)\n",
        "\n",
        "        # Channel-wise Correlation\n",
        "        x1_out, x2_out = self.ChannelCorrelation(x1_2, x2_2)\n",
        "\n",
        "        return edge1, edge2, torch.cat([x1_out, x2_out], 1)  # (24*2)*144*144\n",
        "\n",
        "\n",
        "# Dynamic Semantic Matching Module (DSMM)\n",
        "class DSMM(nn.Module):\n",
        "    def __init__(self, channel4=96, channel3=32):\n",
        "        super(DSMM, self).__init__()\n",
        "\n",
        "        self.fuse4 = seanet_convbnrelu(channel4, channel4, k=1, s=1, p=0, relu=True)\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.smooth4 = DSConv3x3(channel4, channel4, stride=1, dilation=1)  # 96channel-> 96channel\n",
        "\n",
        "        self.fuse3 = seanet_convbnrelu(channel3, channel3, k=1, s=1, p=0, relu=True)\n",
        "        self.smooth3 = DSConv3x3(channel3, channel4, stride=1, dilation=1)  # 32channel-> 96channel\n",
        "        self.ChannelCorrelation = CCorrM(channel4, 32)\n",
        "\n",
        "    def forward(self, x4, k4, x3, k3):  # x4:96*18*18 k4:96*5*5; x3:32*36*36 k3:32*5*5\n",
        "        B4, C4, H4, W4 = k4.size()\n",
        "        B3, C3, H3, W3 = k3.size()\n",
        "\n",
        "        x_B4, x_C4, x_H4, x_W4 = x4.size()  # 8*96*18*18\n",
        "        x_B3, x_C3, x_H3, x_W3 = x3.size()  # 8*32*36*36\n",
        "\n",
        "        x4_new = x4.clone()\n",
        "        x3_new = x3.clone()\n",
        "        # k4 = k4.view(C4, 1, H4, W4)\n",
        "        # k3 = k3.view(C3, 1, H3, W3)\n",
        "        for i in range(1, B4):\n",
        "            kernel4 = k4[i, :, :, :]\n",
        "            kernel3 = k3[i, :, :, :]\n",
        "            kernel4 = kernel4.view(C4, 1, H4, W4)\n",
        "            kernel3 = kernel3.view(C3, 1, H3, W3)\n",
        "            # DDconv\n",
        "            x4_r1 = F.conv2d(x4[i, :, :, :].view(1, C4, x_H4, x_W4), kernel4, stride=1, padding=2, dilation=1,\n",
        "                             groups=C4)\n",
        "            x4_r2 = F.conv2d(x4[i, :, :, :].view(1, C4, x_H4, x_W4), kernel4, stride=1, padding=4, dilation=2,\n",
        "                             groups=C4)\n",
        "            x4_r3 = F.conv2d(x4[i, :, :, :].view(1, C4, x_H4, x_W4), kernel4, stride=1, padding=6, dilation=3,\n",
        "                             groups=C4)\n",
        "            x4_new[i, :, :, :] = x4_r1 + x4_r2 + x4_r3\n",
        "\n",
        "            # DDconv\n",
        "            x3_r1 = F.conv2d(x3[i, :, :, :].view(1, C3, x_H3, x_W3), kernel3, stride=1, padding=2, dilation=1,\n",
        "                             groups=C3)\n",
        "            x3_r2 = F.conv2d(x3[i, :, :, :].view(1, C3, x_H3, x_W3), kernel3, stride=1, padding=4, dilation=2,\n",
        "                             groups=C3)\n",
        "            x3_r3 = F.conv2d(x3[i, :, :, :].view(1, C3, x_H3, x_W3), kernel3, stride=1, padding=6, dilation=3,\n",
        "                             groups=C3)\n",
        "            x3_new[i, :, :, :] = x3_r1 + x3_r2 + x3_r3\n",
        "        # Pconv\n",
        "        x4_all = self.fuse4(x4_new)\n",
        "        x4_smooth = self.smooth4(self.upsample2(x4_all))\n",
        "        # Pconv\n",
        "        x3_all = self.fuse3(x3_new)\n",
        "        x3_smooth = self.smooth3(x3_all)\n",
        "\n",
        "        # Channel-wise Correlation\n",
        "        x3_out, x4_out = self.ChannelCorrelation(x3_smooth, x4_smooth)\n",
        "\n",
        "        return torch.cat([x3_out, x4_out], 1)  # (96*2)*32*32\n",
        "\n",
        "\n",
        "class SalHead(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super(SalHead, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Dropout2d(p=0.1),\n",
        "            nn.Conv2d(in_channel, 1, 1, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class prediction_decoder(nn.Module):\n",
        "    def __init__(self, channel5=320, channel34=192, channel12=48):\n",
        "        super(prediction_decoder, self).__init__()\n",
        "        # 9*9\n",
        "        self.decoder5 = nn.Sequential(\n",
        "            DSConv3x3(channel5, channel5, stride=1),\n",
        "            DSConv3x3(channel5, channel5, stride=1),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),  # 36*36\n",
        "            DSConv3x3(channel5, channel34, stride=1)\n",
        "        )\n",
        "        self.s5 = SalHead(channel34)  # 36*36\n",
        "\n",
        "        # 36*36\n",
        "        self.decoder34 = nn.Sequential(\n",
        "            DSConv3x3(channel34 * 2, channel34, stride=1),\n",
        "            DSConv3x3(channel34, channel34, stride=1),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),  # 144*144\n",
        "            DSConv3x3(channel34, channel12, stride=1)\n",
        "        )\n",
        "        self.s34 = SalHead(channel12)  # 144*144\n",
        "\n",
        "        # 144*144\n",
        "        self.decoder12 = nn.Sequential(\n",
        "            DSConv3x3(channel12 * 2, channel12, stride=1),\n",
        "            DSConv3x3(channel12, channel12, stride=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),  # 288*288\n",
        "            DSConv3x3(channel12, channel12, stride=1)\n",
        "        )\n",
        "        self.s12 = SalHead(channel12)\n",
        "\n",
        "    def forward(self, x5, x34, x12):\n",
        "        x5_decoder = self.decoder5(x5)\n",
        "        s5 = self.s5(x5_decoder)\n",
        "\n",
        "        x34_decoder = self.decoder34(torch.cat([x5_decoder, x34], 1))\n",
        "        s34 = self.s34(x34_decoder)\n",
        "\n",
        "        x12_decoder = self.decoder12(torch.cat([x34_decoder, x12], 1))\n",
        "        s12 = self.s12(x12_decoder)\n",
        "\n",
        "        return s12, s34, s5\n",
        "\n",
        "\n",
        "class SeaNet(nn.Module):\n",
        "    def __init__(self, pretrained=True, channel=128):\n",
        "        super(SeaNet, self).__init__()\n",
        "        # Backbone model\n",
        "        self.backbone = mobilenet_v2(pretrained)\n",
        "        # input 256*256*3\n",
        "        # conv1 128*128*16\n",
        "        # conv2 64*64*24\n",
        "        # conv3 32*32*32\n",
        "        # conv4 16*16*96\n",
        "        # conv5 8*8*320\n",
        "\n",
        "        # Semantic Knowledge Compression(SKC) unit, k3 and k4\n",
        "        self.conv5_conv4 = DSConv3x3(320, 96, stride=1)\n",
        "        self.conv5_conv3 = DSConv3x3(320, 32, stride=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(5)\n",
        "\n",
        "        self.dsmm = DSMM(96, 32)\n",
        "        self.esam = ESAM(16, 24)\n",
        "\n",
        "        self.prediction_decoder = prediction_decoder(320, 192, 48)\n",
        "\n",
        "        self.upsample8 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # generate backbone features\n",
        "        conv1, conv2, conv3, conv4, conv5 = self.backbone(input)\n",
        "\n",
        "        # Semantic Knowledge Compression(SKC) unit, kernel_conv4 (k4) and kernel_conv3 (k3)\n",
        "        kernel_conv4 = self.pool(self.conv5_conv4(conv5))  # 96*5*5\n",
        "        kernel_conv3 = self.pool(self.conv5_conv3(conv5))  # 32*5*5\n",
        "\n",
        "        # conv34 is f_dsmm\n",
        "        conv34 = self.dsmm(conv4, kernel_conv4, conv3, kernel_conv3)\n",
        "        # conv12 is f_esam\n",
        "        edge1, edge2, conv12 = self.esam(conv1, conv2)\n",
        "\n",
        "        s12, s34, s5 = self.prediction_decoder(conv5, conv34, conv12)\n",
        "\n",
        "        s5_up = self.upsample8(s5)\n",
        "        s34_up = self.upsample2(s34)\n",
        "\n",
        "        sigmoid1 = self.sigmoid(s12)\n",
        "        sigmoid2 = self.sigmoid(s34_up)\n",
        "        sigmoid3  = self.sigmoid(s5_up)\n",
        "\n",
        "        return s12, s34_up, s5_up, sigmoid1, sigmoid2, sigmoid3, edge1, edge2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "SjQPS0nEItgV"
      },
      "outputs": [],
      "source": [
        "#@title u2net\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class REBNCONV(nn.Module):\n",
        "    def __init__(self,in_ch=3,out_ch=3,dirate=1):\n",
        "        super(REBNCONV,self).__init__()\n",
        "\n",
        "        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n",
        "        self.bn_s1 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu_s1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))\n",
        "\n",
        "        return xout\n",
        "\n",
        "## upsample tensor 'src' to have the same spatial size with tensor 'tar'\n",
        "def _upsample_like(src,tar):\n",
        "\n",
        "    src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n",
        "\n",
        "    return src\n",
        "\n",
        "\n",
        "### RSU-7 ###\n",
        "class RSU7(nn.Module):#UNet07DRES(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
        "        super(RSU7,self).__init__()\n",
        "\n",
        "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
        "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
        "\n",
        "        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "        hxin = self.rebnconvin(hx)\n",
        "\n",
        "        hx1 = self.rebnconv1(hxin)\n",
        "        hx = self.pool1(hx1)\n",
        "\n",
        "        hx2 = self.rebnconv2(hx)\n",
        "        hx = self.pool2(hx2)\n",
        "\n",
        "        hx3 = self.rebnconv3(hx)\n",
        "        hx = self.pool3(hx3)\n",
        "\n",
        "        hx4 = self.rebnconv4(hx)\n",
        "        hx = self.pool4(hx4)\n",
        "\n",
        "        hx5 = self.rebnconv5(hx)\n",
        "        hx = self.pool5(hx5)\n",
        "\n",
        "        hx6 = self.rebnconv6(hx)\n",
        "\n",
        "        hx7 = self.rebnconv7(hx6)\n",
        "\n",
        "        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),1))\n",
        "        hx6dup = _upsample_like(hx6d,hx5)\n",
        "\n",
        "        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),1))\n",
        "        hx5dup = _upsample_like(hx5d,hx4)\n",
        "\n",
        "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
        "        hx4dup = _upsample_like(hx4d,hx3)\n",
        "\n",
        "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "        return hx1d + hxin\n",
        "\n",
        "### RSU-6 ###\n",
        "class RSU6(nn.Module):#UNet06DRES(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
        "        super(RSU6,self).__init__()\n",
        "\n",
        "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
        "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
        "\n",
        "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        hxin = self.rebnconvin(hx)\n",
        "\n",
        "        hx1 = self.rebnconv1(hxin)\n",
        "        hx = self.pool1(hx1)\n",
        "\n",
        "        hx2 = self.rebnconv2(hx)\n",
        "        hx = self.pool2(hx2)\n",
        "\n",
        "        hx3 = self.rebnconv3(hx)\n",
        "        hx = self.pool3(hx3)\n",
        "\n",
        "        hx4 = self.rebnconv4(hx)\n",
        "        hx = self.pool4(hx4)\n",
        "\n",
        "        hx5 = self.rebnconv5(hx)\n",
        "\n",
        "        hx6 = self.rebnconv6(hx5)\n",
        "\n",
        "\n",
        "        hx5d =  self.rebnconv5d(torch.cat((hx6,hx5),1))\n",
        "        hx5dup = _upsample_like(hx5d,hx4)\n",
        "\n",
        "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
        "        hx4dup = _upsample_like(hx4d,hx3)\n",
        "\n",
        "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "        return hx1d + hxin\n",
        "\n",
        "### RSU-5 ###\n",
        "class RSU5(nn.Module):#UNet05DRES(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
        "        super(RSU5,self).__init__()\n",
        "\n",
        "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
        "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
        "\n",
        "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        hxin = self.rebnconvin(hx)\n",
        "\n",
        "        hx1 = self.rebnconv1(hxin)\n",
        "        hx = self.pool1(hx1)\n",
        "\n",
        "        hx2 = self.rebnconv2(hx)\n",
        "        hx = self.pool2(hx2)\n",
        "\n",
        "        hx3 = self.rebnconv3(hx)\n",
        "        hx = self.pool3(hx3)\n",
        "\n",
        "        hx4 = self.rebnconv4(hx)\n",
        "\n",
        "        hx5 = self.rebnconv5(hx4)\n",
        "\n",
        "        hx4d = self.rebnconv4d(torch.cat((hx5,hx4),1))\n",
        "        hx4dup = _upsample_like(hx4d,hx3)\n",
        "\n",
        "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "        return hx1d + hxin\n",
        "\n",
        "### RSU-4 ###\n",
        "class RSU4(nn.Module):#UNet04DRES(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
        "        super(RSU4,self).__init__()\n",
        "\n",
        "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
        "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
        "\n",
        "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
        "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        hxin = self.rebnconvin(hx)\n",
        "\n",
        "        hx1 = self.rebnconv1(hxin)\n",
        "        hx = self.pool1(hx1)\n",
        "\n",
        "        hx2 = self.rebnconv2(hx)\n",
        "        hx = self.pool2(hx2)\n",
        "\n",
        "        hx3 = self.rebnconv3(hx)\n",
        "\n",
        "        hx4 = self.rebnconv4(hx3)\n",
        "\n",
        "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "        return hx1d + hxin\n",
        "\n",
        "### RSU-4F ###\n",
        "class RSU4F(nn.Module):#UNet04FRES(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
        "        super(RSU4F,self).__init__()\n",
        "\n",
        "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
        "\n",
        "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
        "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
        "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n",
        "\n",
        "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n",
        "\n",
        "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n",
        "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n",
        "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        hxin = self.rebnconvin(hx)\n",
        "\n",
        "        hx1 = self.rebnconv1(hxin)\n",
        "        hx2 = self.rebnconv2(hx1)\n",
        "        hx3 = self.rebnconv3(hx2)\n",
        "\n",
        "        hx4 = self.rebnconv4(hx3)\n",
        "\n",
        "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
        "        hx2d = self.rebnconv2d(torch.cat((hx3d,hx2),1))\n",
        "        hx1d = self.rebnconv1d(torch.cat((hx2d,hx1),1))\n",
        "\n",
        "        return hx1d + hxin\n",
        "\n",
        "\n",
        "##### U^2-Net ####\n",
        "class U2NET(nn.Module):\n",
        "\n",
        "    def __init__(self,in_ch=3,out_ch=1):\n",
        "        super(U2NET,self).__init__()\n",
        "\n",
        "        self.stage1 = RSU7(in_ch,32,64)\n",
        "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage2 = RSU6(64,32,128)\n",
        "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage3 = RSU5(128,64,256)\n",
        "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage4 = RSU4(256,128,512)\n",
        "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage5 = RSU4F(512,256,512)\n",
        "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage6 = RSU4F(512,256,512)\n",
        "\n",
        "        # decoder\n",
        "        self.stage5d = RSU4F(1024,256,512)\n",
        "        self.stage4d = RSU4(1024,128,256)\n",
        "        self.stage3d = RSU5(512,64,128)\n",
        "        self.stage2d = RSU6(256,32,64)\n",
        "        self.stage1d = RSU7(128,16,64)\n",
        "\n",
        "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n",
        "        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n",
        "        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n",
        "        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n",
        "\n",
        "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        #stage 1\n",
        "        hx1 = self.stage1(hx)\n",
        "        hx = self.pool12(hx1)\n",
        "\n",
        "        #stage 2\n",
        "        hx2 = self.stage2(hx)\n",
        "        hx = self.pool23(hx2)\n",
        "\n",
        "        #stage 3\n",
        "        hx3 = self.stage3(hx)\n",
        "        hx = self.pool34(hx3)\n",
        "\n",
        "        #stage 4\n",
        "        hx4 = self.stage4(hx)\n",
        "        hx = self.pool45(hx4)\n",
        "\n",
        "        #stage 5\n",
        "        hx5 = self.stage5(hx)\n",
        "        hx = self.pool56(hx5)\n",
        "\n",
        "        #stage 6\n",
        "        hx6 = self.stage6(hx)\n",
        "        hx6up = _upsample_like(hx6,hx5)\n",
        "\n",
        "        #-------------------- decoder --------------------\n",
        "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
        "        hx5dup = _upsample_like(hx5d,hx4)\n",
        "\n",
        "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
        "        hx4dup = _upsample_like(hx4d,hx3)\n",
        "\n",
        "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "\n",
        "        #side output\n",
        "        d1 = self.side1(hx1d)\n",
        "\n",
        "        d2 = self.side2(hx2d)\n",
        "        d2 = _upsample_like(d2,d1)\n",
        "\n",
        "        d3 = self.side3(hx3d)\n",
        "        d3 = _upsample_like(d3,d1)\n",
        "\n",
        "        d4 = self.side4(hx4d)\n",
        "        d4 = _upsample_like(d4,d1)\n",
        "\n",
        "        d5 = self.side5(hx5d)\n",
        "        d5 = _upsample_like(d5,d1)\n",
        "\n",
        "        d6 = self.side6(hx6)\n",
        "        d6 = _upsample_like(d6,d1)\n",
        "\n",
        "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
        "\n",
        "        out1 = F.sigmoid(d0)\n",
        "        out2 = F.sigmoid(d1)\n",
        "        out3 = F.sigmoid(d3)\n",
        "        out4 = F.sigmoid(d4)\n",
        "        out5 = F.sigmoid(d5)\n",
        "        out6 = F.sigmoid(d6)\n",
        "        return out1, out2, out3, out4, out5, out6\n",
        "\n",
        "        #return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n",
        "\n",
        "### U^2-Net small ###\n",
        "class U2NETP(nn.Module):\n",
        "\n",
        "    def __init__(self,in_ch=3,out_ch=1):\n",
        "        super(U2NETP,self).__init__()\n",
        "\n",
        "        self.stage1 = RSU7(in_ch,16,64)\n",
        "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage2 = RSU6(64,16,64)\n",
        "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage3 = RSU5(64,16,64)\n",
        "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage4 = RSU4(64,16,64)\n",
        "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage5 = RSU4F(64,16,64)\n",
        "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
        "\n",
        "        self.stage6 = RSU4F(64,16,64)\n",
        "\n",
        "        # decoder\n",
        "        self.stage5d = RSU4F(128,16,64)\n",
        "        self.stage4d = RSU4(128,16,64)\n",
        "        self.stage3d = RSU5(128,16,64)\n",
        "        self.stage2d = RSU6(128,16,64)\n",
        "        self.stage1d = RSU7(128,16,64)\n",
        "\n",
        "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side3 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side4 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side5 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "        self.side6 = nn.Conv2d(64,out_ch,3,padding=1)\n",
        "\n",
        "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        hx = x\n",
        "\n",
        "        #stage 1\n",
        "        hx1 = self.stage1(hx)\n",
        "        hx = self.pool12(hx1)\n",
        "\n",
        "        #stage 2\n",
        "        hx2 = self.stage2(hx)\n",
        "        hx = self.pool23(hx2)\n",
        "\n",
        "        #stage 3\n",
        "        hx3 = self.stage3(hx)\n",
        "        hx = self.pool34(hx3)\n",
        "\n",
        "        #stage 4\n",
        "        hx4 = self.stage4(hx)\n",
        "        hx = self.pool45(hx4)\n",
        "\n",
        "        #stage 5\n",
        "        hx5 = self.stage5(hx)\n",
        "        hx = self.pool56(hx5)\n",
        "\n",
        "        #stage 6\n",
        "        hx6 = self.stage6(hx)\n",
        "        hx6up = _upsample_like(hx6,hx5)\n",
        "\n",
        "        #decoder\n",
        "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
        "        hx5dup = _upsample_like(hx5d,hx4)\n",
        "\n",
        "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
        "        hx4dup = _upsample_like(hx4d,hx3)\n",
        "\n",
        "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
        "        hx3dup = _upsample_like(hx3d,hx2)\n",
        "\n",
        "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
        "        hx2dup = _upsample_like(hx2d,hx1)\n",
        "\n",
        "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
        "\n",
        "\n",
        "        #side output\n",
        "        d1 = self.side1(hx1d)\n",
        "\n",
        "        d2 = self.side2(hx2d)\n",
        "        d2 = _upsample_like(d2,d1)\n",
        "\n",
        "        d3 = self.side3(hx3d)\n",
        "        d3 = _upsample_like(d3,d1)\n",
        "\n",
        "        d4 = self.side4(hx4d)\n",
        "        d4 = _upsample_like(d4,d1)\n",
        "\n",
        "        d5 = self.side5(hx5d)\n",
        "        d5 = _upsample_like(d5,d1)\n",
        "\n",
        "        d6 = self.side6(hx6)\n",
        "        d6 = _upsample_like(d6,d1)\n",
        "\n",
        "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
        "\n",
        "        return F.sigmoid(d1) #F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FLIM\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "\n",
        "arch_path = 'arch.json' # Must be a .json file with the architecture definition\n",
        "\n",
        "\n",
        "\n",
        "# Load JSON config\n",
        "with open(arch_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MeanBasedDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    based on paper https://arxiv.org/pdf/2504.20872 FLIM-based Salient Object Detection Networks with Adaptive Decoders\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MeanBasedDecoder, self).__init__()\n",
        "\n",
        "    def forward(self, features, original_size, decoder_weights):\n",
        "        B, C, H, W = features.shape\n",
        "        decoder_weights = decoder_weights.to(dtype=torch.float32, device=features.device)\n",
        "\n",
        "        # Expand decoder_weights for broadcasting: (C,) → (1, C, 1, 1)\n",
        "        alpha_weights = decoder_weights.view(1, C, 1, 1)  # foreground mask (1s & 0s)\n",
        "\n",
        "        # Compute μ1 and μ2 without branching\n",
        "        foreground_mask = alpha_weights\n",
        "        background_mask = 1.0 - alpha_weights\n",
        "\n",
        "        foreground_sum = (features * foreground_mask).sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
        "        background_sum = (features * background_mask).sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
        "\n",
        "        # Count foreground and background channels\n",
        "        fg_count = torch.sum(foreground_mask)\n",
        "        bg_count = torch.sum(background_mask)\n",
        "\n",
        "        # Avoid divide-by-zero\n",
        "        fg_count = fg_count.clamp(min=1.0)\n",
        "        bg_count = bg_count.clamp(min=1.0)\n",
        "\n",
        "        μ1 = foreground_sum / fg_count\n",
        "        μ2 = background_sum / bg_count\n",
        "\n",
        "        # Compare μ1 and μ2, build alpha vector\n",
        "        μ1_gt_μ2 = (μ1 > μ2).float()  # (B, 1, H, W)\n",
        "        μ1_lt_μ2 = (μ1 < μ2).float()\n",
        "\n",
        "        # Expand boolean results to match feature shape\n",
        "        μ1_gt_μ2_exp = μ1_gt_μ2.expand(-1, C, -1, -1)\n",
        "        μ1_lt_μ2_exp = μ1_lt_μ2.expand(-1, C, -1, -1)\n",
        "\n",
        "        # Expand decoder weights to match feature shape\n",
        "        decoder_mask = decoder_weights.view(1, C, 1, 1).expand(B, C, H, W)\n",
        "\n",
        "        # Define α: +1, −1, or 0\n",
        "        alpha = torch.where(\n",
        "            (decoder_mask == 1.0) & (μ1_gt_μ2_exp == 1.0),\n",
        "            torch.ones_like(features),\n",
        "            torch.where(\n",
        "                (decoder_mask == 0.0) & (μ1_lt_μ2_exp == 1.0),\n",
        "                -torch.ones_like(features),\n",
        "                torch.zeros_like(features)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Apply α to features\n",
        "        weighted_features = features * alpha\n",
        "\n",
        "        # Sum channels → saliency map\n",
        "        saliency = weighted_features.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Resize to original size\n",
        "        output = F.interpolate(saliency, size=original_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MeanBasedDecoderV1(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\tHas data depedent flow like conditions, torch.tensor conversion that is a problem in import. So, Don’t use shapes like feature.shape[1] in control flow., Avoid conditions like if tensor.sum() > 0: inside the module., Use tensor ops like torch.where, torch.cond, or precompute indices.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MeanBasedDecoder, self).__init__()\n",
        "\n",
        "    def forward(self, features, original_size, decoder_weights):\n",
        "        # features: (B, C, H, W)\n",
        "        batch_size, channels, height, width = features.shape\n",
        "        decoder_weights = decoder_weights.to(features.device)\n",
        "\n",
        "        # Create foreground and background masks\n",
        "        foreground_mask = (decoder_weights == 1)  # shape: (C,)\n",
        "        background_mask = (decoder_weights == 0)\n",
        "\n",
        "        # Compute μ1 and μ2 (mean over foreground/background channels)\n",
        "        mean1 = features[:, foreground_mask, :, :].mean(dim=1)  # shape: (B, H, W)\n",
        "        mean2 = features[:, background_mask, :, :].mean(dim=1)  # shape: (B, H, W)\n",
        "\n",
        "        # Expand for broadcasting: shape → (B, C, H, W)\n",
        "        mean1_exp = mean1.unsqueeze(1)\n",
        "        mean2_exp = mean2.unsqueeze(1)\n",
        "\n",
        "        # Expand weights for computation: (C,) → (1, C, 1, 1)\n",
        "        weights = decoder_weights.view(1, channels, 1, 1).float()\n",
        "\n",
        "        # Construct α vector: +1, -1, or 0 based on paper logic\n",
        "        alpha = torch.where(\n",
        "            (weights == 1) & (mean1_exp > mean2_exp), torch.ones_like(weights),\n",
        "            torch.where((weights == 0) & (mean1_exp < mean2_exp), -torch.ones_like(weights), torch.zeros_like(weights))\n",
        "        )\n",
        "\n",
        "        # Apply weights to features\n",
        "        weighted_features = features * alpha  # (B, C, H, W)\n",
        "\n",
        "        # Sum across channels to produce output map\n",
        "        output = weighted_features.sum(dim=1, keepdim=True)  # shape: (B, 1, H, W)\n",
        "\n",
        "        # Interpolate to original size\n",
        "        output_resized = F.interpolate(output, size=original_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return output_resized\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, config, original_size):\n",
        "        super(ConvNet, self).__init__()\n",
        "        layers = []\n",
        "        input_channels = 3  # Assuming RGB input; modify as needed\n",
        "        stdev = config.get(\"stdev_factor\", 0.01)\n",
        "\n",
        "        for i in range(1, config[\"nlayers\"] + 1):\n",
        "            layer_cfg = config[f\"layer{i}\"]\n",
        "            conv_cfg = layer_cfg[\"conv\"]\n",
        "            pool_cfg = layer_cfg[\"pooling\"]\n",
        "            use_relu = layer_cfg.get(\"relu\", False)\n",
        "\n",
        "            # Convolution layer\n",
        "            layers.append(nn.Conv2d(\n",
        "                in_channels=input_channels,\n",
        "                out_channels=conv_cfg[\"noutput_channels\"],\n",
        "                kernel_size=tuple(conv_cfg[\"kernel_size\"][:2]),\n",
        "                dilation=tuple(conv_cfg[\"dilation_rate\"][:2]),\n",
        "                padding=1\n",
        "            ))\n",
        "\n",
        "            # Pooling layer\n",
        "            pool_type = pool_cfg[\"type\"]\n",
        "            pool_args = {\n",
        "                'kernel_size': tuple(pool_cfg[\"size\"][:2]),\n",
        "                'stride': pool_cfg[\"stride\"],\n",
        "                'padding': 1\n",
        "            }\n",
        "            if pool_type == \"avg_pool\":\n",
        "                layers.append(nn.AvgPool2d(**pool_args))\n",
        "            elif pool_type == \"max_pool\":\n",
        "                layers.append(nn.MaxPool2d(**pool_args))\n",
        "\n",
        "            # Activation\n",
        "            if use_relu:\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "            input_channels = conv_cfg[\"noutput_channels\"]\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "        #self.classifier = nn.Linear(30 * 8 * 8, 10)  # Adjust size & output classes as needed\n",
        "\n",
        "        decoder_input_size = 30 #config[f\"layer{config[\"nlayers\"]}\"][\"conv\"][\"noutput_channels\"]\n",
        "\n",
        "        adaptation_function=\"robust_weights\"\n",
        "        decoder_type = 'decoder_3'\n",
        "        filter_by_size=False\n",
        "       # device='cpu'\n",
        "        #self.decoder = FLIMAdaptiveDecoderLayer(decoder_input_size, adaptation_function=adaptation_function, decoder_type=decoder_type, filter_by_size=filter_by_size)\n",
        "\n",
        "        self.decoder =  MeanBasedDecoder()\n",
        "\n",
        "        self.original_size = original_size #(512,510)\n",
        "        self.decoder_weights = torch.randint(low=0, high=2, size=(30,))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        #x = torch.flatten(x, 1)\n",
        "        #x = self.classifier(x)\n",
        "        x = self.decoder(x, self.original_size, self.decoder_weights )\n",
        "        return x\n",
        "\n",
        "# Example: Build model\n",
        "model = ConvNet(config, (224, 224)).eval()\n",
        "print(model)\n",
        "\n",
        "### Test implementation\n",
        "sample_inputs = torch.randn(1, 3, 224, 224)\n",
        "with torch.no_grad():\n",
        "\tout = model(sample_inputs)\n",
        "\tprint(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptmpYdjcvv52",
        "outputId": "d655e13e-5224-422e-f87d-80495086567f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): AvgPool2d(kernel_size=(2, 2), stride=1, padding=1)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): AvgPool2d(kernel_size=(2, 2), stride=1, padding=1)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (decoder): MeanBasedDecoder()\n",
            ")\n",
            "torch.Size([1, 1, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastSal().eval().to(\"cpu\")#SeaNet(pretrained=False)#FastSal().eval().to(\"cpu\") #U2NET().eval().to(\"cpu\")\n",
        "#print(model)\n",
        "\n",
        "### Test implementation\n",
        "sample_inputs = torch.randn(1, 3, 224, 224)\n",
        "with torch.no_grad():\n",
        "\tout = model(sample_inputs)\n",
        "\t#print(out.shape)\n"
      ],
      "metadata": {
        "id": "8s-rBo0mlqV0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.fx import symbolic_trace\n",
        "traced = symbolic_trace(model)\n",
        "print(traced.graph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9tUEZfRw0DF",
        "outputId": "9628241b-8127-4c68-d8fb-3f1330116a20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph():\n",
            "    %input_1 : [num_users=6] = placeholder[target=input]\n",
            "    %backbone_level1_0_conv : [num_users=1] = call_module[target=backbone.level1_0.conv](args = (%input_1,), kwargs = {})\n",
            "    %backbone_level1_0_pool : [num_users=1] = call_module[target=backbone.level1_0.pool](args = (%input_1,), kwargs = {})\n",
            "    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level1_0_conv, %backbone_level1_0_pool], 1), kwargs = {})\n",
            "    %backbone_level1_0_bn : [num_users=1] = call_module[target=backbone.level1_0.bn](args = (%cat,), kwargs = {})\n",
            "    %backbone_level1_0_act : [num_users=3] = call_module[target=backbone.level1_0.act](args = (%backbone_level1_0_bn,), kwargs = {})\n",
            "    %backbone_level1_0_conv_1 : [num_users=1] = call_module[target=backbone.level1.0.conv](args = (%backbone_level1_0_act,), kwargs = {})\n",
            "    %add : [num_users=1] = call_function[target=operator.add](args = (%backbone_level1_0_act, %backbone_level1_0_conv_1), kwargs = {})\n",
            "    %backbone_level1_0_bn_1 : [num_users=1] = call_module[target=backbone.level1.0.bn](args = (%add,), kwargs = {})\n",
            "    %backbone_level1_0_act_1 : [num_users=5] = call_module[target=backbone.level1.0.act](args = (%backbone_level1_0_bn_1,), kwargs = {})\n",
            "    %size : [num_users=4] = call_method[target=size](args = (%backbone_level1_0_act_1,), kwargs = {})\n",
            "    %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%size, 0), kwargs = {})\n",
            "    %getitem_1 : [num_users=2] = call_function[target=operator.getitem](args = (%size, 1), kwargs = {})\n",
            "    %getitem_2 : [num_users=0] = call_function[target=operator.getitem](args = (%size, 2), kwargs = {})\n",
            "    %getitem_3 : [num_users=0] = call_function[target=operator.getitem](args = (%size, 3), kwargs = {})\n",
            "    %backbone_level1_1_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level1.1.channel_att.avg_pool](args = (%backbone_level1_0_act_1,), kwargs = {})\n",
            "    %view : [num_users=1] = call_method[target=view](args = (%backbone_level1_1_channel_att_avg_pool, %getitem, %getitem_1), kwargs = {})\n",
            "    %backbone_level1_1_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level1.1.channel_att.fc.0](args = (%view,), kwargs = {})\n",
            "    %backbone_level1_1_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level1.1.channel_att.fc.1](args = (%backbone_level1_1_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level1_1_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level1.1.channel_att.fc.2](args = (%backbone_level1_1_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level1_1_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level1.1.channel_att.fc.3](args = (%backbone_level1_1_channel_att_fc_2,), kwargs = {})\n",
            "    %view_1 : [num_users=1] = call_method[target=view](args = (%backbone_level1_1_channel_att_fc_3, %getitem, %getitem_1, 1, 1), kwargs = {})\n",
            "    %expand_as : [num_users=1] = call_method[target=expand_as](args = (%view_1, %backbone_level1_0_act_1), kwargs = {})\n",
            "    %mul : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level1_0_act_1, %expand_as), kwargs = {})\n",
            "    %backbone_level1_1_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level1.1.spatialatt.spatial.conv](args = (%mul,), kwargs = {})\n",
            "    %backbone_level1_1_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level1.1.spatialatt.spatial.bn](args = (%backbone_level1_1_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level1_1_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_1 : [num_users=1] = call_function[target=operator.mul](args = (%mul, %sigmoid), kwargs = {})\n",
            "    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level1_0_act_1, %mul_1), kwargs = {})\n",
            "    %backbone_branch1 : [num_users=1] = call_module[target=backbone.branch1](args = (%backbone_level1_0_act,), kwargs = {})\n",
            "    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%backbone_branch1, %add_1), kwargs = {})\n",
            "    %backbone_br1_0 : [num_users=1] = call_module[target=backbone.br1.0](args = (%add_2,), kwargs = {})\n",
            "    %backbone_br1_1 : [num_users=4] = call_module[target=backbone.br1.1](args = (%backbone_br1_0,), kwargs = {})\n",
            "    %backbone_level2_0_conv0 : [num_users=1] = call_module[target=backbone.level2_0.conv0](args = (%backbone_br1_1,), kwargs = {})\n",
            "    %backbone_level2_0_conv1 : [num_users=1] = call_module[target=backbone.level2_0.conv1](args = (%backbone_level2_0_conv0,), kwargs = {})\n",
            "    %backbone_level2_0_pool : [num_users=1] = call_module[target=backbone.level2_0.pool](args = (%backbone_br1_1,), kwargs = {})\n",
            "    %cat_1 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level2_0_conv1, %backbone_level2_0_pool], 1), kwargs = {})\n",
            "    %backbone_level2_0_bn : [num_users=1] = call_module[target=backbone.level2_0.bn](args = (%cat_1,), kwargs = {})\n",
            "    %backbone_level2_0_act : [num_users=2] = call_module[target=backbone.level2_0.act](args = (%backbone_level2_0_bn,), kwargs = {})\n",
            "    %backbone_level2_0 : [num_users=4] = call_module[target=backbone.level2.0](args = (%backbone_level2_0_act,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_0_conv : [num_users=1] = call_module[target=backbone.level2.1.recp7.0.conv](args = (%backbone_level2_0,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_0_bn : [num_users=1] = call_module[target=backbone.level2.1.recp7.0.bn](args = (%backbone_level2_1_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_0_relu : [num_users=1] = call_module[target=backbone.level2.1.recp7.0.relu](args = (%backbone_level2_1_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_1_conv : [num_users=1] = call_module[target=backbone.level2.1.recp7.1.conv](args = (%backbone_level2_1_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_1_bn : [num_users=1] = call_module[target=backbone.level2.1.recp7.1.bn](args = (%backbone_level2_1_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_1_relu : [num_users=1] = call_module[target=backbone.level2.1.recp7.1.relu](args = (%backbone_level2_1_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_2_conv : [num_users=1] = call_module[target=backbone.level2.1.recp7.2.conv](args = (%backbone_level2_1_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_2_bn : [num_users=1] = call_module[target=backbone.level2.1.recp7.2.bn](args = (%backbone_level2_1_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_3_conv : [num_users=1] = call_module[target=backbone.level2.1.recp7.3.conv](args = (%backbone_level2_1_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_3_bn : [num_users=1] = call_module[target=backbone.level2.1.recp7.3.bn](args = (%backbone_level2_1_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp7_3_relu : [num_users=3] = call_module[target=backbone.level2.1.recp7.3.relu](args = (%backbone_level2_1_recp7_3_bn,), kwargs = {})\n",
            "    %cat_2 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level2_0, %backbone_level2_1_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level2_1_recp5_0_conv : [num_users=1] = call_module[target=backbone.level2.1.recp5.0.conv](args = (%cat_2,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_0_bn : [num_users=1] = call_module[target=backbone.level2.1.recp5.0.bn](args = (%backbone_level2_1_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_0_relu : [num_users=1] = call_module[target=backbone.level2.1.recp5.0.relu](args = (%backbone_level2_1_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_1_conv : [num_users=1] = call_module[target=backbone.level2.1.recp5.1.conv](args = (%backbone_level2_1_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_1_bn : [num_users=1] = call_module[target=backbone.level2.1.recp5.1.bn](args = (%backbone_level2_1_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_1_relu : [num_users=1] = call_module[target=backbone.level2.1.recp5.1.relu](args = (%backbone_level2_1_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_2_conv : [num_users=1] = call_module[target=backbone.level2.1.recp5.2.conv](args = (%backbone_level2_1_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_2_bn : [num_users=1] = call_module[target=backbone.level2.1.recp5.2.bn](args = (%backbone_level2_1_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_3_conv : [num_users=1] = call_module[target=backbone.level2.1.recp5.3.conv](args = (%backbone_level2_1_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_3_bn : [num_users=1] = call_module[target=backbone.level2.1.recp5.3.bn](args = (%backbone_level2_1_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp5_3_relu : [num_users=2] = call_module[target=backbone.level2.1.recp5.3.relu](args = (%backbone_level2_1_recp5_3_bn,), kwargs = {})\n",
            "    %cat_3 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level2_0, %backbone_level2_1_recp7_3_relu, %backbone_level2_1_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level2_1_recp3_0_conv : [num_users=1] = call_module[target=backbone.level2.1.recp3.0.conv](args = (%cat_3,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_0_bn : [num_users=1] = call_module[target=backbone.level2.1.recp3.0.bn](args = (%backbone_level2_1_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_0_relu : [num_users=1] = call_module[target=backbone.level2.1.recp3.0.relu](args = (%backbone_level2_1_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_1_conv : [num_users=1] = call_module[target=backbone.level2.1.recp3.1.conv](args = (%backbone_level2_1_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_1_bn : [num_users=1] = call_module[target=backbone.level2.1.recp3.1.bn](args = (%backbone_level2_1_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_1_relu : [num_users=1] = call_module[target=backbone.level2.1.recp3.1.relu](args = (%backbone_level2_1_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_2_conv : [num_users=1] = call_module[target=backbone.level2.1.recp3.2.conv](args = (%backbone_level2_1_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_2_bn : [num_users=1] = call_module[target=backbone.level2.1.recp3.2.bn](args = (%backbone_level2_1_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_3_conv : [num_users=1] = call_module[target=backbone.level2.1.recp3.3.conv](args = (%backbone_level2_1_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_3_bn : [num_users=1] = call_module[target=backbone.level2.1.recp3.3.bn](args = (%backbone_level2_1_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp3_3_relu : [num_users=1] = call_module[target=backbone.level2.1.recp3.3.relu](args = (%backbone_level2_1_recp3_3_bn,), kwargs = {})\n",
            "    %cat_4 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level2_0, %backbone_level2_1_recp7_3_relu, %backbone_level2_1_recp5_3_relu, %backbone_level2_1_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level2_1_recp1_0_conv : [num_users=1] = call_module[target=backbone.level2.1.recp1.0.conv](args = (%cat_4,), kwargs = {})\n",
            "    %backbone_level2_1_recp1_0_bn : [num_users=1] = call_module[target=backbone.level2.1.recp1.0.bn](args = (%backbone_level2_1_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level2_1_recp1_0_relu : [num_users=5] = call_module[target=backbone.level2.1.recp1.0.relu](args = (%backbone_level2_1_recp1_0_bn,), kwargs = {})\n",
            "    %size_1 : [num_users=4] = call_method[target=size](args = (%backbone_level2_1_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_4 : [num_users=2] = call_function[target=operator.getitem](args = (%size_1, 0), kwargs = {})\n",
            "    %getitem_5 : [num_users=2] = call_function[target=operator.getitem](args = (%size_1, 1), kwargs = {})\n",
            "    %getitem_6 : [num_users=0] = call_function[target=operator.getitem](args = (%size_1, 2), kwargs = {})\n",
            "    %getitem_7 : [num_users=0] = call_function[target=operator.getitem](args = (%size_1, 3), kwargs = {})\n",
            "    %backbone_level2_2_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level2.2.channel_att.avg_pool](args = (%backbone_level2_1_recp1_0_relu,), kwargs = {})\n",
            "    %view_2 : [num_users=1] = call_method[target=view](args = (%backbone_level2_2_channel_att_avg_pool, %getitem_4, %getitem_5), kwargs = {})\n",
            "    %backbone_level2_2_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level2.2.channel_att.fc.0](args = (%view_2,), kwargs = {})\n",
            "    %backbone_level2_2_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level2.2.channel_att.fc.1](args = (%backbone_level2_2_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level2_2_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level2.2.channel_att.fc.2](args = (%backbone_level2_2_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level2_2_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level2.2.channel_att.fc.3](args = (%backbone_level2_2_channel_att_fc_2,), kwargs = {})\n",
            "    %view_3 : [num_users=1] = call_method[target=view](args = (%backbone_level2_2_channel_att_fc_3, %getitem_4, %getitem_5, 1, 1), kwargs = {})\n",
            "    %expand_as_1 : [num_users=1] = call_method[target=expand_as](args = (%view_3, %backbone_level2_1_recp1_0_relu), kwargs = {})\n",
            "    %mul_2 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level2_1_recp1_0_relu, %expand_as_1), kwargs = {})\n",
            "    %backbone_level2_2_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level2.2.spatialatt.spatial.conv](args = (%mul_2,), kwargs = {})\n",
            "    %backbone_level2_2_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level2.2.spatialatt.spatial.bn](args = (%backbone_level2_2_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_1 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level2_2_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_3 : [num_users=1] = call_function[target=operator.mul](args = (%mul_2, %sigmoid_1), kwargs = {})\n",
            "    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level2_1_recp1_0_relu, %mul_3), kwargs = {})\n",
            "    %backbone_branch2 : [num_users=1] = call_module[target=backbone.branch2](args = (%backbone_level2_0_act,), kwargs = {})\n",
            "    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%backbone_branch2, %add_3), kwargs = {})\n",
            "    %backbone_br2_0 : [num_users=1] = call_module[target=backbone.br2.0](args = (%add_4,), kwargs = {})\n",
            "    %backbone_br2_1 : [num_users=4] = call_module[target=backbone.br2.1](args = (%backbone_br2_0,), kwargs = {})\n",
            "    %backbone_level3_0_conv0 : [num_users=1] = call_module[target=backbone.level3_0.conv0](args = (%backbone_br2_1,), kwargs = {})\n",
            "    %backbone_level3_0_conv1 : [num_users=1] = call_module[target=backbone.level3_0.conv1](args = (%backbone_level3_0_conv0,), kwargs = {})\n",
            "    %backbone_level3_0_pool : [num_users=1] = call_module[target=backbone.level3_0.pool](args = (%backbone_br2_1,), kwargs = {})\n",
            "    %cat_5 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_0_conv1, %backbone_level3_0_pool], 1), kwargs = {})\n",
            "    %backbone_level3_0_bn : [num_users=1] = call_module[target=backbone.level3_0.bn](args = (%cat_5,), kwargs = {})\n",
            "    %backbone_level3_0_act : [num_users=2] = call_module[target=backbone.level3_0.act](args = (%backbone_level3_0_bn,), kwargs = {})\n",
            "    %backbone_level3_0 : [num_users=4] = call_module[target=backbone.level3.0](args = (%backbone_level3_0_act,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_0_conv : [num_users=1] = call_module[target=backbone.level3.1.recp7.0.conv](args = (%backbone_level3_0,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_0_bn : [num_users=1] = call_module[target=backbone.level3.1.recp7.0.bn](args = (%backbone_level3_1_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_0_relu : [num_users=1] = call_module[target=backbone.level3.1.recp7.0.relu](args = (%backbone_level3_1_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_1_conv : [num_users=1] = call_module[target=backbone.level3.1.recp7.1.conv](args = (%backbone_level3_1_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_1_bn : [num_users=1] = call_module[target=backbone.level3.1.recp7.1.bn](args = (%backbone_level3_1_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_1_relu : [num_users=1] = call_module[target=backbone.level3.1.recp7.1.relu](args = (%backbone_level3_1_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_2_conv : [num_users=1] = call_module[target=backbone.level3.1.recp7.2.conv](args = (%backbone_level3_1_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_2_bn : [num_users=1] = call_module[target=backbone.level3.1.recp7.2.bn](args = (%backbone_level3_1_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_3_conv : [num_users=1] = call_module[target=backbone.level3.1.recp7.3.conv](args = (%backbone_level3_1_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_3_bn : [num_users=1] = call_module[target=backbone.level3.1.recp7.3.bn](args = (%backbone_level3_1_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp7_3_relu : [num_users=3] = call_module[target=backbone.level3.1.recp7.3.relu](args = (%backbone_level3_1_recp7_3_bn,), kwargs = {})\n",
            "    %cat_6 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_0, %backbone_level3_1_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_1_recp5_0_conv : [num_users=1] = call_module[target=backbone.level3.1.recp5.0.conv](args = (%cat_6,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_0_bn : [num_users=1] = call_module[target=backbone.level3.1.recp5.0.bn](args = (%backbone_level3_1_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_0_relu : [num_users=1] = call_module[target=backbone.level3.1.recp5.0.relu](args = (%backbone_level3_1_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_1_conv : [num_users=1] = call_module[target=backbone.level3.1.recp5.1.conv](args = (%backbone_level3_1_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_1_bn : [num_users=1] = call_module[target=backbone.level3.1.recp5.1.bn](args = (%backbone_level3_1_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_1_relu : [num_users=1] = call_module[target=backbone.level3.1.recp5.1.relu](args = (%backbone_level3_1_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_2_conv : [num_users=1] = call_module[target=backbone.level3.1.recp5.2.conv](args = (%backbone_level3_1_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_2_bn : [num_users=1] = call_module[target=backbone.level3.1.recp5.2.bn](args = (%backbone_level3_1_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_3_conv : [num_users=1] = call_module[target=backbone.level3.1.recp5.3.conv](args = (%backbone_level3_1_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_3_bn : [num_users=1] = call_module[target=backbone.level3.1.recp5.3.bn](args = (%backbone_level3_1_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp5_3_relu : [num_users=2] = call_module[target=backbone.level3.1.recp5.3.relu](args = (%backbone_level3_1_recp5_3_bn,), kwargs = {})\n",
            "    %cat_7 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_0, %backbone_level3_1_recp7_3_relu, %backbone_level3_1_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_1_recp3_0_conv : [num_users=1] = call_module[target=backbone.level3.1.recp3.0.conv](args = (%cat_7,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_0_bn : [num_users=1] = call_module[target=backbone.level3.1.recp3.0.bn](args = (%backbone_level3_1_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_0_relu : [num_users=1] = call_module[target=backbone.level3.1.recp3.0.relu](args = (%backbone_level3_1_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_1_conv : [num_users=1] = call_module[target=backbone.level3.1.recp3.1.conv](args = (%backbone_level3_1_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_1_bn : [num_users=1] = call_module[target=backbone.level3.1.recp3.1.bn](args = (%backbone_level3_1_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_1_relu : [num_users=1] = call_module[target=backbone.level3.1.recp3.1.relu](args = (%backbone_level3_1_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_2_conv : [num_users=1] = call_module[target=backbone.level3.1.recp3.2.conv](args = (%backbone_level3_1_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_2_bn : [num_users=1] = call_module[target=backbone.level3.1.recp3.2.bn](args = (%backbone_level3_1_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_3_conv : [num_users=1] = call_module[target=backbone.level3.1.recp3.3.conv](args = (%backbone_level3_1_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_3_bn : [num_users=1] = call_module[target=backbone.level3.1.recp3.3.bn](args = (%backbone_level3_1_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp3_3_relu : [num_users=1] = call_module[target=backbone.level3.1.recp3.3.relu](args = (%backbone_level3_1_recp3_3_bn,), kwargs = {})\n",
            "    %cat_8 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_0, %backbone_level3_1_recp7_3_relu, %backbone_level3_1_recp5_3_relu, %backbone_level3_1_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_1_recp1_0_conv : [num_users=1] = call_module[target=backbone.level3.1.recp1.0.conv](args = (%cat_8,), kwargs = {})\n",
            "    %backbone_level3_1_recp1_0_bn : [num_users=1] = call_module[target=backbone.level3.1.recp1.0.bn](args = (%backbone_level3_1_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level3_1_recp1_0_relu : [num_users=5] = call_module[target=backbone.level3.1.recp1.0.relu](args = (%backbone_level3_1_recp1_0_bn,), kwargs = {})\n",
            "    %size_2 : [num_users=4] = call_method[target=size](args = (%backbone_level3_1_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_8 : [num_users=2] = call_function[target=operator.getitem](args = (%size_2, 0), kwargs = {})\n",
            "    %getitem_9 : [num_users=2] = call_function[target=operator.getitem](args = (%size_2, 1), kwargs = {})\n",
            "    %getitem_10 : [num_users=0] = call_function[target=operator.getitem](args = (%size_2, 2), kwargs = {})\n",
            "    %getitem_11 : [num_users=0] = call_function[target=operator.getitem](args = (%size_2, 3), kwargs = {})\n",
            "    %backbone_level3_2_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level3.2.channel_att.avg_pool](args = (%backbone_level3_1_recp1_0_relu,), kwargs = {})\n",
            "    %view_4 : [num_users=1] = call_method[target=view](args = (%backbone_level3_2_channel_att_avg_pool, %getitem_8, %getitem_9), kwargs = {})\n",
            "    %backbone_level3_2_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level3.2.channel_att.fc.0](args = (%view_4,), kwargs = {})\n",
            "    %backbone_level3_2_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level3.2.channel_att.fc.1](args = (%backbone_level3_2_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level3_2_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level3.2.channel_att.fc.2](args = (%backbone_level3_2_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level3_2_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level3.2.channel_att.fc.3](args = (%backbone_level3_2_channel_att_fc_2,), kwargs = {})\n",
            "    %view_5 : [num_users=1] = call_method[target=view](args = (%backbone_level3_2_channel_att_fc_3, %getitem_8, %getitem_9, 1, 1), kwargs = {})\n",
            "    %expand_as_2 : [num_users=1] = call_method[target=expand_as](args = (%view_5, %backbone_level3_1_recp1_0_relu), kwargs = {})\n",
            "    %mul_4 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level3_1_recp1_0_relu, %expand_as_2), kwargs = {})\n",
            "    %backbone_level3_2_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level3.2.spatialatt.spatial.conv](args = (%mul_4,), kwargs = {})\n",
            "    %backbone_level3_2_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level3.2.spatialatt.spatial.bn](args = (%backbone_level3_2_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_2 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level3_2_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_5 : [num_users=1] = call_function[target=operator.mul](args = (%mul_4, %sigmoid_2), kwargs = {})\n",
            "    %add_5 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level3_1_recp1_0_relu, %mul_5), kwargs = {})\n",
            "    %backbone_level3_3 : [num_users=4] = call_module[target=backbone.level3.3](args = (%add_5,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_0_conv : [num_users=1] = call_module[target=backbone.level3.4.recp7.0.conv](args = (%backbone_level3_3,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_0_bn : [num_users=1] = call_module[target=backbone.level3.4.recp7.0.bn](args = (%backbone_level3_4_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_0_relu : [num_users=1] = call_module[target=backbone.level3.4.recp7.0.relu](args = (%backbone_level3_4_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_1_conv : [num_users=1] = call_module[target=backbone.level3.4.recp7.1.conv](args = (%backbone_level3_4_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_1_bn : [num_users=1] = call_module[target=backbone.level3.4.recp7.1.bn](args = (%backbone_level3_4_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_1_relu : [num_users=1] = call_module[target=backbone.level3.4.recp7.1.relu](args = (%backbone_level3_4_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_2_conv : [num_users=1] = call_module[target=backbone.level3.4.recp7.2.conv](args = (%backbone_level3_4_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_2_bn : [num_users=1] = call_module[target=backbone.level3.4.recp7.2.bn](args = (%backbone_level3_4_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_3_conv : [num_users=1] = call_module[target=backbone.level3.4.recp7.3.conv](args = (%backbone_level3_4_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_3_bn : [num_users=1] = call_module[target=backbone.level3.4.recp7.3.bn](args = (%backbone_level3_4_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp7_3_relu : [num_users=3] = call_module[target=backbone.level3.4.recp7.3.relu](args = (%backbone_level3_4_recp7_3_bn,), kwargs = {})\n",
            "    %cat_9 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_3, %backbone_level3_4_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_4_recp5_0_conv : [num_users=1] = call_module[target=backbone.level3.4.recp5.0.conv](args = (%cat_9,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_0_bn : [num_users=1] = call_module[target=backbone.level3.4.recp5.0.bn](args = (%backbone_level3_4_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_0_relu : [num_users=1] = call_module[target=backbone.level3.4.recp5.0.relu](args = (%backbone_level3_4_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_1_conv : [num_users=1] = call_module[target=backbone.level3.4.recp5.1.conv](args = (%backbone_level3_4_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_1_bn : [num_users=1] = call_module[target=backbone.level3.4.recp5.1.bn](args = (%backbone_level3_4_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_1_relu : [num_users=1] = call_module[target=backbone.level3.4.recp5.1.relu](args = (%backbone_level3_4_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_2_conv : [num_users=1] = call_module[target=backbone.level3.4.recp5.2.conv](args = (%backbone_level3_4_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_2_bn : [num_users=1] = call_module[target=backbone.level3.4.recp5.2.bn](args = (%backbone_level3_4_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_3_conv : [num_users=1] = call_module[target=backbone.level3.4.recp5.3.conv](args = (%backbone_level3_4_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_3_bn : [num_users=1] = call_module[target=backbone.level3.4.recp5.3.bn](args = (%backbone_level3_4_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp5_3_relu : [num_users=2] = call_module[target=backbone.level3.4.recp5.3.relu](args = (%backbone_level3_4_recp5_3_bn,), kwargs = {})\n",
            "    %cat_10 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_3, %backbone_level3_4_recp7_3_relu, %backbone_level3_4_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_4_recp3_0_conv : [num_users=1] = call_module[target=backbone.level3.4.recp3.0.conv](args = (%cat_10,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_0_bn : [num_users=1] = call_module[target=backbone.level3.4.recp3.0.bn](args = (%backbone_level3_4_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_0_relu : [num_users=1] = call_module[target=backbone.level3.4.recp3.0.relu](args = (%backbone_level3_4_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_1_conv : [num_users=1] = call_module[target=backbone.level3.4.recp3.1.conv](args = (%backbone_level3_4_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_1_bn : [num_users=1] = call_module[target=backbone.level3.4.recp3.1.bn](args = (%backbone_level3_4_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_1_relu : [num_users=1] = call_module[target=backbone.level3.4.recp3.1.relu](args = (%backbone_level3_4_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_2_conv : [num_users=1] = call_module[target=backbone.level3.4.recp3.2.conv](args = (%backbone_level3_4_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_2_bn : [num_users=1] = call_module[target=backbone.level3.4.recp3.2.bn](args = (%backbone_level3_4_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_3_conv : [num_users=1] = call_module[target=backbone.level3.4.recp3.3.conv](args = (%backbone_level3_4_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_3_bn : [num_users=1] = call_module[target=backbone.level3.4.recp3.3.bn](args = (%backbone_level3_4_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp3_3_relu : [num_users=1] = call_module[target=backbone.level3.4.recp3.3.relu](args = (%backbone_level3_4_recp3_3_bn,), kwargs = {})\n",
            "    %cat_11 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_3, %backbone_level3_4_recp7_3_relu, %backbone_level3_4_recp5_3_relu, %backbone_level3_4_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_4_recp1_0_conv : [num_users=1] = call_module[target=backbone.level3.4.recp1.0.conv](args = (%cat_11,), kwargs = {})\n",
            "    %backbone_level3_4_recp1_0_bn : [num_users=1] = call_module[target=backbone.level3.4.recp1.0.bn](args = (%backbone_level3_4_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level3_4_recp1_0_relu : [num_users=5] = call_module[target=backbone.level3.4.recp1.0.relu](args = (%backbone_level3_4_recp1_0_bn,), kwargs = {})\n",
            "    %size_3 : [num_users=4] = call_method[target=size](args = (%backbone_level3_4_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_12 : [num_users=2] = call_function[target=operator.getitem](args = (%size_3, 0), kwargs = {})\n",
            "    %getitem_13 : [num_users=2] = call_function[target=operator.getitem](args = (%size_3, 1), kwargs = {})\n",
            "    %getitem_14 : [num_users=0] = call_function[target=operator.getitem](args = (%size_3, 2), kwargs = {})\n",
            "    %getitem_15 : [num_users=0] = call_function[target=operator.getitem](args = (%size_3, 3), kwargs = {})\n",
            "    %backbone_level3_5_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level3.5.channel_att.avg_pool](args = (%backbone_level3_4_recp1_0_relu,), kwargs = {})\n",
            "    %view_6 : [num_users=1] = call_method[target=view](args = (%backbone_level3_5_channel_att_avg_pool, %getitem_12, %getitem_13), kwargs = {})\n",
            "    %backbone_level3_5_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level3.5.channel_att.fc.0](args = (%view_6,), kwargs = {})\n",
            "    %backbone_level3_5_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level3.5.channel_att.fc.1](args = (%backbone_level3_5_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level3_5_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level3.5.channel_att.fc.2](args = (%backbone_level3_5_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level3_5_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level3.5.channel_att.fc.3](args = (%backbone_level3_5_channel_att_fc_2,), kwargs = {})\n",
            "    %view_7 : [num_users=1] = call_method[target=view](args = (%backbone_level3_5_channel_att_fc_3, %getitem_12, %getitem_13, 1, 1), kwargs = {})\n",
            "    %expand_as_3 : [num_users=1] = call_method[target=expand_as](args = (%view_7, %backbone_level3_4_recp1_0_relu), kwargs = {})\n",
            "    %mul_6 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level3_4_recp1_0_relu, %expand_as_3), kwargs = {})\n",
            "    %backbone_level3_5_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level3.5.spatialatt.spatial.conv](args = (%mul_6,), kwargs = {})\n",
            "    %backbone_level3_5_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level3.5.spatialatt.spatial.bn](args = (%backbone_level3_5_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_3 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level3_5_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_7 : [num_users=1] = call_function[target=operator.mul](args = (%mul_6, %sigmoid_3), kwargs = {})\n",
            "    %add_6 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level3_4_recp1_0_relu, %mul_7), kwargs = {})\n",
            "    %backbone_level3_6 : [num_users=4] = call_module[target=backbone.level3.6](args = (%add_6,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_0_conv : [num_users=1] = call_module[target=backbone.level3.7.recp7.0.conv](args = (%backbone_level3_6,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_0_bn : [num_users=1] = call_module[target=backbone.level3.7.recp7.0.bn](args = (%backbone_level3_7_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_0_relu : [num_users=1] = call_module[target=backbone.level3.7.recp7.0.relu](args = (%backbone_level3_7_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_1_conv : [num_users=1] = call_module[target=backbone.level3.7.recp7.1.conv](args = (%backbone_level3_7_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_1_bn : [num_users=1] = call_module[target=backbone.level3.7.recp7.1.bn](args = (%backbone_level3_7_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_1_relu : [num_users=1] = call_module[target=backbone.level3.7.recp7.1.relu](args = (%backbone_level3_7_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_2_conv : [num_users=1] = call_module[target=backbone.level3.7.recp7.2.conv](args = (%backbone_level3_7_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_2_bn : [num_users=1] = call_module[target=backbone.level3.7.recp7.2.bn](args = (%backbone_level3_7_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_3_conv : [num_users=1] = call_module[target=backbone.level3.7.recp7.3.conv](args = (%backbone_level3_7_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_3_bn : [num_users=1] = call_module[target=backbone.level3.7.recp7.3.bn](args = (%backbone_level3_7_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp7_3_relu : [num_users=3] = call_module[target=backbone.level3.7.recp7.3.relu](args = (%backbone_level3_7_recp7_3_bn,), kwargs = {})\n",
            "    %cat_12 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_6, %backbone_level3_7_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_7_recp5_0_conv : [num_users=1] = call_module[target=backbone.level3.7.recp5.0.conv](args = (%cat_12,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_0_bn : [num_users=1] = call_module[target=backbone.level3.7.recp5.0.bn](args = (%backbone_level3_7_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_0_relu : [num_users=1] = call_module[target=backbone.level3.7.recp5.0.relu](args = (%backbone_level3_7_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_1_conv : [num_users=1] = call_module[target=backbone.level3.7.recp5.1.conv](args = (%backbone_level3_7_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_1_bn : [num_users=1] = call_module[target=backbone.level3.7.recp5.1.bn](args = (%backbone_level3_7_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_1_relu : [num_users=1] = call_module[target=backbone.level3.7.recp5.1.relu](args = (%backbone_level3_7_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_2_conv : [num_users=1] = call_module[target=backbone.level3.7.recp5.2.conv](args = (%backbone_level3_7_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_2_bn : [num_users=1] = call_module[target=backbone.level3.7.recp5.2.bn](args = (%backbone_level3_7_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_3_conv : [num_users=1] = call_module[target=backbone.level3.7.recp5.3.conv](args = (%backbone_level3_7_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_3_bn : [num_users=1] = call_module[target=backbone.level3.7.recp5.3.bn](args = (%backbone_level3_7_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp5_3_relu : [num_users=2] = call_module[target=backbone.level3.7.recp5.3.relu](args = (%backbone_level3_7_recp5_3_bn,), kwargs = {})\n",
            "    %cat_13 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_6, %backbone_level3_7_recp7_3_relu, %backbone_level3_7_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_7_recp3_0_conv : [num_users=1] = call_module[target=backbone.level3.7.recp3.0.conv](args = (%cat_13,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_0_bn : [num_users=1] = call_module[target=backbone.level3.7.recp3.0.bn](args = (%backbone_level3_7_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_0_relu : [num_users=1] = call_module[target=backbone.level3.7.recp3.0.relu](args = (%backbone_level3_7_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_1_conv : [num_users=1] = call_module[target=backbone.level3.7.recp3.1.conv](args = (%backbone_level3_7_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_1_bn : [num_users=1] = call_module[target=backbone.level3.7.recp3.1.bn](args = (%backbone_level3_7_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_1_relu : [num_users=1] = call_module[target=backbone.level3.7.recp3.1.relu](args = (%backbone_level3_7_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_2_conv : [num_users=1] = call_module[target=backbone.level3.7.recp3.2.conv](args = (%backbone_level3_7_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_2_bn : [num_users=1] = call_module[target=backbone.level3.7.recp3.2.bn](args = (%backbone_level3_7_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_3_conv : [num_users=1] = call_module[target=backbone.level3.7.recp3.3.conv](args = (%backbone_level3_7_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_3_bn : [num_users=1] = call_module[target=backbone.level3.7.recp3.3.bn](args = (%backbone_level3_7_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp3_3_relu : [num_users=1] = call_module[target=backbone.level3.7.recp3.3.relu](args = (%backbone_level3_7_recp3_3_bn,), kwargs = {})\n",
            "    %cat_14 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level3_6, %backbone_level3_7_recp7_3_relu, %backbone_level3_7_recp5_3_relu, %backbone_level3_7_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level3_7_recp1_0_conv : [num_users=1] = call_module[target=backbone.level3.7.recp1.0.conv](args = (%cat_14,), kwargs = {})\n",
            "    %backbone_level3_7_recp1_0_bn : [num_users=1] = call_module[target=backbone.level3.7.recp1.0.bn](args = (%backbone_level3_7_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level3_7_recp1_0_relu : [num_users=5] = call_module[target=backbone.level3.7.recp1.0.relu](args = (%backbone_level3_7_recp1_0_bn,), kwargs = {})\n",
            "    %size_4 : [num_users=4] = call_method[target=size](args = (%backbone_level3_7_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_16 : [num_users=2] = call_function[target=operator.getitem](args = (%size_4, 0), kwargs = {})\n",
            "    %getitem_17 : [num_users=2] = call_function[target=operator.getitem](args = (%size_4, 1), kwargs = {})\n",
            "    %getitem_18 : [num_users=0] = call_function[target=operator.getitem](args = (%size_4, 2), kwargs = {})\n",
            "    %getitem_19 : [num_users=0] = call_function[target=operator.getitem](args = (%size_4, 3), kwargs = {})\n",
            "    %backbone_level3_8_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level3.8.channel_att.avg_pool](args = (%backbone_level3_7_recp1_0_relu,), kwargs = {})\n",
            "    %view_8 : [num_users=1] = call_method[target=view](args = (%backbone_level3_8_channel_att_avg_pool, %getitem_16, %getitem_17), kwargs = {})\n",
            "    %backbone_level3_8_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level3.8.channel_att.fc.0](args = (%view_8,), kwargs = {})\n",
            "    %backbone_level3_8_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level3.8.channel_att.fc.1](args = (%backbone_level3_8_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level3_8_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level3.8.channel_att.fc.2](args = (%backbone_level3_8_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level3_8_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level3.8.channel_att.fc.3](args = (%backbone_level3_8_channel_att_fc_2,), kwargs = {})\n",
            "    %view_9 : [num_users=1] = call_method[target=view](args = (%backbone_level3_8_channel_att_fc_3, %getitem_16, %getitem_17, 1, 1), kwargs = {})\n",
            "    %expand_as_4 : [num_users=1] = call_method[target=expand_as](args = (%view_9, %backbone_level3_7_recp1_0_relu), kwargs = {})\n",
            "    %mul_8 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level3_7_recp1_0_relu, %expand_as_4), kwargs = {})\n",
            "    %backbone_level3_8_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level3.8.spatialatt.spatial.conv](args = (%mul_8,), kwargs = {})\n",
            "    %backbone_level3_8_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level3.8.spatialatt.spatial.bn](args = (%backbone_level3_8_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_4 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level3_8_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_9 : [num_users=1] = call_function[target=operator.mul](args = (%mul_8, %sigmoid_4), kwargs = {})\n",
            "    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level3_7_recp1_0_relu, %mul_9), kwargs = {})\n",
            "    %backbone_branch3 : [num_users=1] = call_module[target=backbone.branch3](args = (%backbone_level3_0_act,), kwargs = {})\n",
            "    %add_8 : [num_users=1] = call_function[target=operator.add](args = (%backbone_branch3, %add_7), kwargs = {})\n",
            "    %backbone_br3_0 : [num_users=1] = call_module[target=backbone.br3.0](args = (%add_8,), kwargs = {})\n",
            "    %backbone_br3_1 : [num_users=4] = call_module[target=backbone.br3.1](args = (%backbone_br3_0,), kwargs = {})\n",
            "    %backbone_level4_0_conv0 : [num_users=1] = call_module[target=backbone.level4_0.conv0](args = (%backbone_br3_1,), kwargs = {})\n",
            "    %backbone_level4_0_conv1 : [num_users=1] = call_module[target=backbone.level4_0.conv1](args = (%backbone_level4_0_conv0,), kwargs = {})\n",
            "    %backbone_level4_0_pool : [num_users=1] = call_module[target=backbone.level4_0.pool](args = (%backbone_br3_1,), kwargs = {})\n",
            "    %cat_15 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_0_conv1, %backbone_level4_0_pool], 1), kwargs = {})\n",
            "    %backbone_level4_0_bn : [num_users=1] = call_module[target=backbone.level4_0.bn](args = (%cat_15,), kwargs = {})\n",
            "    %backbone_level4_0_act : [num_users=2] = call_module[target=backbone.level4_0.act](args = (%backbone_level4_0_bn,), kwargs = {})\n",
            "    %backbone_level4_0 : [num_users=4] = call_module[target=backbone.level4.0](args = (%backbone_level4_0_act,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_0_conv : [num_users=1] = call_module[target=backbone.level4.1.recp7.0.conv](args = (%backbone_level4_0,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_0_bn : [num_users=1] = call_module[target=backbone.level4.1.recp7.0.bn](args = (%backbone_level4_1_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_0_relu : [num_users=1] = call_module[target=backbone.level4.1.recp7.0.relu](args = (%backbone_level4_1_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_1_conv : [num_users=1] = call_module[target=backbone.level4.1.recp7.1.conv](args = (%backbone_level4_1_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_1_bn : [num_users=1] = call_module[target=backbone.level4.1.recp7.1.bn](args = (%backbone_level4_1_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_1_relu : [num_users=1] = call_module[target=backbone.level4.1.recp7.1.relu](args = (%backbone_level4_1_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_2_conv : [num_users=1] = call_module[target=backbone.level4.1.recp7.2.conv](args = (%backbone_level4_1_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_2_bn : [num_users=1] = call_module[target=backbone.level4.1.recp7.2.bn](args = (%backbone_level4_1_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_3_conv : [num_users=1] = call_module[target=backbone.level4.1.recp7.3.conv](args = (%backbone_level4_1_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_3_bn : [num_users=1] = call_module[target=backbone.level4.1.recp7.3.bn](args = (%backbone_level4_1_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp7_3_relu : [num_users=3] = call_module[target=backbone.level4.1.recp7.3.relu](args = (%backbone_level4_1_recp7_3_bn,), kwargs = {})\n",
            "    %cat_16 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_0, %backbone_level4_1_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_1_recp5_0_conv : [num_users=1] = call_module[target=backbone.level4.1.recp5.0.conv](args = (%cat_16,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_0_bn : [num_users=1] = call_module[target=backbone.level4.1.recp5.0.bn](args = (%backbone_level4_1_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_0_relu : [num_users=1] = call_module[target=backbone.level4.1.recp5.0.relu](args = (%backbone_level4_1_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_1_conv : [num_users=1] = call_module[target=backbone.level4.1.recp5.1.conv](args = (%backbone_level4_1_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_1_bn : [num_users=1] = call_module[target=backbone.level4.1.recp5.1.bn](args = (%backbone_level4_1_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_1_relu : [num_users=1] = call_module[target=backbone.level4.1.recp5.1.relu](args = (%backbone_level4_1_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_2_conv : [num_users=1] = call_module[target=backbone.level4.1.recp5.2.conv](args = (%backbone_level4_1_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_2_bn : [num_users=1] = call_module[target=backbone.level4.1.recp5.2.bn](args = (%backbone_level4_1_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_3_conv : [num_users=1] = call_module[target=backbone.level4.1.recp5.3.conv](args = (%backbone_level4_1_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_3_bn : [num_users=1] = call_module[target=backbone.level4.1.recp5.3.bn](args = (%backbone_level4_1_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp5_3_relu : [num_users=2] = call_module[target=backbone.level4.1.recp5.3.relu](args = (%backbone_level4_1_recp5_3_bn,), kwargs = {})\n",
            "    %cat_17 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_0, %backbone_level4_1_recp7_3_relu, %backbone_level4_1_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_1_recp3_0_conv : [num_users=1] = call_module[target=backbone.level4.1.recp3.0.conv](args = (%cat_17,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_0_bn : [num_users=1] = call_module[target=backbone.level4.1.recp3.0.bn](args = (%backbone_level4_1_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_0_relu : [num_users=1] = call_module[target=backbone.level4.1.recp3.0.relu](args = (%backbone_level4_1_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_1_conv : [num_users=1] = call_module[target=backbone.level4.1.recp3.1.conv](args = (%backbone_level4_1_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_1_bn : [num_users=1] = call_module[target=backbone.level4.1.recp3.1.bn](args = (%backbone_level4_1_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_1_relu : [num_users=1] = call_module[target=backbone.level4.1.recp3.1.relu](args = (%backbone_level4_1_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_2_conv : [num_users=1] = call_module[target=backbone.level4.1.recp3.2.conv](args = (%backbone_level4_1_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_2_bn : [num_users=1] = call_module[target=backbone.level4.1.recp3.2.bn](args = (%backbone_level4_1_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_3_conv : [num_users=1] = call_module[target=backbone.level4.1.recp3.3.conv](args = (%backbone_level4_1_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_3_bn : [num_users=1] = call_module[target=backbone.level4.1.recp3.3.bn](args = (%backbone_level4_1_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp3_3_relu : [num_users=1] = call_module[target=backbone.level4.1.recp3.3.relu](args = (%backbone_level4_1_recp3_3_bn,), kwargs = {})\n",
            "    %cat_18 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_0, %backbone_level4_1_recp7_3_relu, %backbone_level4_1_recp5_3_relu, %backbone_level4_1_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_1_recp1_0_conv : [num_users=1] = call_module[target=backbone.level4.1.recp1.0.conv](args = (%cat_18,), kwargs = {})\n",
            "    %backbone_level4_1_recp1_0_bn : [num_users=1] = call_module[target=backbone.level4.1.recp1.0.bn](args = (%backbone_level4_1_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level4_1_recp1_0_relu : [num_users=5] = call_module[target=backbone.level4.1.recp1.0.relu](args = (%backbone_level4_1_recp1_0_bn,), kwargs = {})\n",
            "    %size_5 : [num_users=4] = call_method[target=size](args = (%backbone_level4_1_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_20 : [num_users=2] = call_function[target=operator.getitem](args = (%size_5, 0), kwargs = {})\n",
            "    %getitem_21 : [num_users=2] = call_function[target=operator.getitem](args = (%size_5, 1), kwargs = {})\n",
            "    %getitem_22 : [num_users=0] = call_function[target=operator.getitem](args = (%size_5, 2), kwargs = {})\n",
            "    %getitem_23 : [num_users=0] = call_function[target=operator.getitem](args = (%size_5, 3), kwargs = {})\n",
            "    %backbone_level4_2_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level4.2.channel_att.avg_pool](args = (%backbone_level4_1_recp1_0_relu,), kwargs = {})\n",
            "    %view_10 : [num_users=1] = call_method[target=view](args = (%backbone_level4_2_channel_att_avg_pool, %getitem_20, %getitem_21), kwargs = {})\n",
            "    %backbone_level4_2_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level4.2.channel_att.fc.0](args = (%view_10,), kwargs = {})\n",
            "    %backbone_level4_2_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level4.2.channel_att.fc.1](args = (%backbone_level4_2_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level4_2_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level4.2.channel_att.fc.2](args = (%backbone_level4_2_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level4_2_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level4.2.channel_att.fc.3](args = (%backbone_level4_2_channel_att_fc_2,), kwargs = {})\n",
            "    %view_11 : [num_users=1] = call_method[target=view](args = (%backbone_level4_2_channel_att_fc_3, %getitem_20, %getitem_21, 1, 1), kwargs = {})\n",
            "    %expand_as_5 : [num_users=1] = call_method[target=expand_as](args = (%view_11, %backbone_level4_1_recp1_0_relu), kwargs = {})\n",
            "    %mul_10 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level4_1_recp1_0_relu, %expand_as_5), kwargs = {})\n",
            "    %backbone_level4_2_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level4.2.spatialatt.spatial.conv](args = (%mul_10,), kwargs = {})\n",
            "    %backbone_level4_2_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level4.2.spatialatt.spatial.bn](args = (%backbone_level4_2_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_5 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level4_2_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_11 : [num_users=1] = call_function[target=operator.mul](args = (%mul_10, %sigmoid_5), kwargs = {})\n",
            "    %add_9 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level4_1_recp1_0_relu, %mul_11), kwargs = {})\n",
            "    %backbone_level4_3 : [num_users=4] = call_module[target=backbone.level4.3](args = (%add_9,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_0_conv : [num_users=1] = call_module[target=backbone.level4.4.recp7.0.conv](args = (%backbone_level4_3,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_0_bn : [num_users=1] = call_module[target=backbone.level4.4.recp7.0.bn](args = (%backbone_level4_4_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_0_relu : [num_users=1] = call_module[target=backbone.level4.4.recp7.0.relu](args = (%backbone_level4_4_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_1_conv : [num_users=1] = call_module[target=backbone.level4.4.recp7.1.conv](args = (%backbone_level4_4_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_1_bn : [num_users=1] = call_module[target=backbone.level4.4.recp7.1.bn](args = (%backbone_level4_4_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_1_relu : [num_users=1] = call_module[target=backbone.level4.4.recp7.1.relu](args = (%backbone_level4_4_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_2_conv : [num_users=1] = call_module[target=backbone.level4.4.recp7.2.conv](args = (%backbone_level4_4_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_2_bn : [num_users=1] = call_module[target=backbone.level4.4.recp7.2.bn](args = (%backbone_level4_4_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_3_conv : [num_users=1] = call_module[target=backbone.level4.4.recp7.3.conv](args = (%backbone_level4_4_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_3_bn : [num_users=1] = call_module[target=backbone.level4.4.recp7.3.bn](args = (%backbone_level4_4_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp7_3_relu : [num_users=3] = call_module[target=backbone.level4.4.recp7.3.relu](args = (%backbone_level4_4_recp7_3_bn,), kwargs = {})\n",
            "    %cat_19 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_3, %backbone_level4_4_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_4_recp5_0_conv : [num_users=1] = call_module[target=backbone.level4.4.recp5.0.conv](args = (%cat_19,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_0_bn : [num_users=1] = call_module[target=backbone.level4.4.recp5.0.bn](args = (%backbone_level4_4_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_0_relu : [num_users=1] = call_module[target=backbone.level4.4.recp5.0.relu](args = (%backbone_level4_4_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_1_conv : [num_users=1] = call_module[target=backbone.level4.4.recp5.1.conv](args = (%backbone_level4_4_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_1_bn : [num_users=1] = call_module[target=backbone.level4.4.recp5.1.bn](args = (%backbone_level4_4_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_1_relu : [num_users=1] = call_module[target=backbone.level4.4.recp5.1.relu](args = (%backbone_level4_4_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_2_conv : [num_users=1] = call_module[target=backbone.level4.4.recp5.2.conv](args = (%backbone_level4_4_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_2_bn : [num_users=1] = call_module[target=backbone.level4.4.recp5.2.bn](args = (%backbone_level4_4_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_3_conv : [num_users=1] = call_module[target=backbone.level4.4.recp5.3.conv](args = (%backbone_level4_4_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_3_bn : [num_users=1] = call_module[target=backbone.level4.4.recp5.3.bn](args = (%backbone_level4_4_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp5_3_relu : [num_users=2] = call_module[target=backbone.level4.4.recp5.3.relu](args = (%backbone_level4_4_recp5_3_bn,), kwargs = {})\n",
            "    %cat_20 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_3, %backbone_level4_4_recp7_3_relu, %backbone_level4_4_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_4_recp3_0_conv : [num_users=1] = call_module[target=backbone.level4.4.recp3.0.conv](args = (%cat_20,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_0_bn : [num_users=1] = call_module[target=backbone.level4.4.recp3.0.bn](args = (%backbone_level4_4_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_0_relu : [num_users=1] = call_module[target=backbone.level4.4.recp3.0.relu](args = (%backbone_level4_4_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_1_conv : [num_users=1] = call_module[target=backbone.level4.4.recp3.1.conv](args = (%backbone_level4_4_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_1_bn : [num_users=1] = call_module[target=backbone.level4.4.recp3.1.bn](args = (%backbone_level4_4_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_1_relu : [num_users=1] = call_module[target=backbone.level4.4.recp3.1.relu](args = (%backbone_level4_4_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_2_conv : [num_users=1] = call_module[target=backbone.level4.4.recp3.2.conv](args = (%backbone_level4_4_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_2_bn : [num_users=1] = call_module[target=backbone.level4.4.recp3.2.bn](args = (%backbone_level4_4_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_3_conv : [num_users=1] = call_module[target=backbone.level4.4.recp3.3.conv](args = (%backbone_level4_4_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_3_bn : [num_users=1] = call_module[target=backbone.level4.4.recp3.3.bn](args = (%backbone_level4_4_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp3_3_relu : [num_users=1] = call_module[target=backbone.level4.4.recp3.3.relu](args = (%backbone_level4_4_recp3_3_bn,), kwargs = {})\n",
            "    %cat_21 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_3, %backbone_level4_4_recp7_3_relu, %backbone_level4_4_recp5_3_relu, %backbone_level4_4_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_4_recp1_0_conv : [num_users=1] = call_module[target=backbone.level4.4.recp1.0.conv](args = (%cat_21,), kwargs = {})\n",
            "    %backbone_level4_4_recp1_0_bn : [num_users=1] = call_module[target=backbone.level4.4.recp1.0.bn](args = (%backbone_level4_4_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level4_4_recp1_0_relu : [num_users=5] = call_module[target=backbone.level4.4.recp1.0.relu](args = (%backbone_level4_4_recp1_0_bn,), kwargs = {})\n",
            "    %size_6 : [num_users=4] = call_method[target=size](args = (%backbone_level4_4_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_24 : [num_users=2] = call_function[target=operator.getitem](args = (%size_6, 0), kwargs = {})\n",
            "    %getitem_25 : [num_users=2] = call_function[target=operator.getitem](args = (%size_6, 1), kwargs = {})\n",
            "    %getitem_26 : [num_users=0] = call_function[target=operator.getitem](args = (%size_6, 2), kwargs = {})\n",
            "    %getitem_27 : [num_users=0] = call_function[target=operator.getitem](args = (%size_6, 3), kwargs = {})\n",
            "    %backbone_level4_5_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level4.5.channel_att.avg_pool](args = (%backbone_level4_4_recp1_0_relu,), kwargs = {})\n",
            "    %view_12 : [num_users=1] = call_method[target=view](args = (%backbone_level4_5_channel_att_avg_pool, %getitem_24, %getitem_25), kwargs = {})\n",
            "    %backbone_level4_5_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level4.5.channel_att.fc.0](args = (%view_12,), kwargs = {})\n",
            "    %backbone_level4_5_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level4.5.channel_att.fc.1](args = (%backbone_level4_5_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level4_5_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level4.5.channel_att.fc.2](args = (%backbone_level4_5_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level4_5_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level4.5.channel_att.fc.3](args = (%backbone_level4_5_channel_att_fc_2,), kwargs = {})\n",
            "    %view_13 : [num_users=1] = call_method[target=view](args = (%backbone_level4_5_channel_att_fc_3, %getitem_24, %getitem_25, 1, 1), kwargs = {})\n",
            "    %expand_as_6 : [num_users=1] = call_method[target=expand_as](args = (%view_13, %backbone_level4_4_recp1_0_relu), kwargs = {})\n",
            "    %mul_12 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level4_4_recp1_0_relu, %expand_as_6), kwargs = {})\n",
            "    %backbone_level4_5_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level4.5.spatialatt.spatial.conv](args = (%mul_12,), kwargs = {})\n",
            "    %backbone_level4_5_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level4.5.spatialatt.spatial.bn](args = (%backbone_level4_5_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_6 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level4_5_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_13 : [num_users=1] = call_function[target=operator.mul](args = (%mul_12, %sigmoid_6), kwargs = {})\n",
            "    %add_10 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level4_4_recp1_0_relu, %mul_13), kwargs = {})\n",
            "    %backbone_level4_6 : [num_users=4] = call_module[target=backbone.level4.6](args = (%add_10,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_0_conv : [num_users=1] = call_module[target=backbone.level4.7.recp7.0.conv](args = (%backbone_level4_6,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_0_bn : [num_users=1] = call_module[target=backbone.level4.7.recp7.0.bn](args = (%backbone_level4_7_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_0_relu : [num_users=1] = call_module[target=backbone.level4.7.recp7.0.relu](args = (%backbone_level4_7_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_1_conv : [num_users=1] = call_module[target=backbone.level4.7.recp7.1.conv](args = (%backbone_level4_7_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_1_bn : [num_users=1] = call_module[target=backbone.level4.7.recp7.1.bn](args = (%backbone_level4_7_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_1_relu : [num_users=1] = call_module[target=backbone.level4.7.recp7.1.relu](args = (%backbone_level4_7_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_2_conv : [num_users=1] = call_module[target=backbone.level4.7.recp7.2.conv](args = (%backbone_level4_7_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_2_bn : [num_users=1] = call_module[target=backbone.level4.7.recp7.2.bn](args = (%backbone_level4_7_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_3_conv : [num_users=1] = call_module[target=backbone.level4.7.recp7.3.conv](args = (%backbone_level4_7_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_3_bn : [num_users=1] = call_module[target=backbone.level4.7.recp7.3.bn](args = (%backbone_level4_7_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp7_3_relu : [num_users=3] = call_module[target=backbone.level4.7.recp7.3.relu](args = (%backbone_level4_7_recp7_3_bn,), kwargs = {})\n",
            "    %cat_22 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_6, %backbone_level4_7_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_7_recp5_0_conv : [num_users=1] = call_module[target=backbone.level4.7.recp5.0.conv](args = (%cat_22,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_0_bn : [num_users=1] = call_module[target=backbone.level4.7.recp5.0.bn](args = (%backbone_level4_7_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_0_relu : [num_users=1] = call_module[target=backbone.level4.7.recp5.0.relu](args = (%backbone_level4_7_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_1_conv : [num_users=1] = call_module[target=backbone.level4.7.recp5.1.conv](args = (%backbone_level4_7_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_1_bn : [num_users=1] = call_module[target=backbone.level4.7.recp5.1.bn](args = (%backbone_level4_7_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_1_relu : [num_users=1] = call_module[target=backbone.level4.7.recp5.1.relu](args = (%backbone_level4_7_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_2_conv : [num_users=1] = call_module[target=backbone.level4.7.recp5.2.conv](args = (%backbone_level4_7_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_2_bn : [num_users=1] = call_module[target=backbone.level4.7.recp5.2.bn](args = (%backbone_level4_7_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_3_conv : [num_users=1] = call_module[target=backbone.level4.7.recp5.3.conv](args = (%backbone_level4_7_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_3_bn : [num_users=1] = call_module[target=backbone.level4.7.recp5.3.bn](args = (%backbone_level4_7_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp5_3_relu : [num_users=2] = call_module[target=backbone.level4.7.recp5.3.relu](args = (%backbone_level4_7_recp5_3_bn,), kwargs = {})\n",
            "    %cat_23 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_6, %backbone_level4_7_recp7_3_relu, %backbone_level4_7_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_7_recp3_0_conv : [num_users=1] = call_module[target=backbone.level4.7.recp3.0.conv](args = (%cat_23,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_0_bn : [num_users=1] = call_module[target=backbone.level4.7.recp3.0.bn](args = (%backbone_level4_7_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_0_relu : [num_users=1] = call_module[target=backbone.level4.7.recp3.0.relu](args = (%backbone_level4_7_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_1_conv : [num_users=1] = call_module[target=backbone.level4.7.recp3.1.conv](args = (%backbone_level4_7_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_1_bn : [num_users=1] = call_module[target=backbone.level4.7.recp3.1.bn](args = (%backbone_level4_7_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_1_relu : [num_users=1] = call_module[target=backbone.level4.7.recp3.1.relu](args = (%backbone_level4_7_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_2_conv : [num_users=1] = call_module[target=backbone.level4.7.recp3.2.conv](args = (%backbone_level4_7_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_2_bn : [num_users=1] = call_module[target=backbone.level4.7.recp3.2.bn](args = (%backbone_level4_7_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_3_conv : [num_users=1] = call_module[target=backbone.level4.7.recp3.3.conv](args = (%backbone_level4_7_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_3_bn : [num_users=1] = call_module[target=backbone.level4.7.recp3.3.bn](args = (%backbone_level4_7_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp3_3_relu : [num_users=1] = call_module[target=backbone.level4.7.recp3.3.relu](args = (%backbone_level4_7_recp3_3_bn,), kwargs = {})\n",
            "    %cat_24 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_6, %backbone_level4_7_recp7_3_relu, %backbone_level4_7_recp5_3_relu, %backbone_level4_7_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_7_recp1_0_conv : [num_users=1] = call_module[target=backbone.level4.7.recp1.0.conv](args = (%cat_24,), kwargs = {})\n",
            "    %backbone_level4_7_recp1_0_bn : [num_users=1] = call_module[target=backbone.level4.7.recp1.0.bn](args = (%backbone_level4_7_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level4_7_recp1_0_relu : [num_users=5] = call_module[target=backbone.level4.7.recp1.0.relu](args = (%backbone_level4_7_recp1_0_bn,), kwargs = {})\n",
            "    %size_7 : [num_users=4] = call_method[target=size](args = (%backbone_level4_7_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_28 : [num_users=2] = call_function[target=operator.getitem](args = (%size_7, 0), kwargs = {})\n",
            "    %getitem_29 : [num_users=2] = call_function[target=operator.getitem](args = (%size_7, 1), kwargs = {})\n",
            "    %getitem_30 : [num_users=0] = call_function[target=operator.getitem](args = (%size_7, 2), kwargs = {})\n",
            "    %getitem_31 : [num_users=0] = call_function[target=operator.getitem](args = (%size_7, 3), kwargs = {})\n",
            "    %backbone_level4_8_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level4.8.channel_att.avg_pool](args = (%backbone_level4_7_recp1_0_relu,), kwargs = {})\n",
            "    %view_14 : [num_users=1] = call_method[target=view](args = (%backbone_level4_8_channel_att_avg_pool, %getitem_28, %getitem_29), kwargs = {})\n",
            "    %backbone_level4_8_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level4.8.channel_att.fc.0](args = (%view_14,), kwargs = {})\n",
            "    %backbone_level4_8_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level4.8.channel_att.fc.1](args = (%backbone_level4_8_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level4_8_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level4.8.channel_att.fc.2](args = (%backbone_level4_8_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level4_8_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level4.8.channel_att.fc.3](args = (%backbone_level4_8_channel_att_fc_2,), kwargs = {})\n",
            "    %view_15 : [num_users=1] = call_method[target=view](args = (%backbone_level4_8_channel_att_fc_3, %getitem_28, %getitem_29, 1, 1), kwargs = {})\n",
            "    %expand_as_7 : [num_users=1] = call_method[target=expand_as](args = (%view_15, %backbone_level4_7_recp1_0_relu), kwargs = {})\n",
            "    %mul_14 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level4_7_recp1_0_relu, %expand_as_7), kwargs = {})\n",
            "    %backbone_level4_8_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level4.8.spatialatt.spatial.conv](args = (%mul_14,), kwargs = {})\n",
            "    %backbone_level4_8_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level4.8.spatialatt.spatial.bn](args = (%backbone_level4_8_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_7 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level4_8_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_15 : [num_users=1] = call_function[target=operator.mul](args = (%mul_14, %sigmoid_7), kwargs = {})\n",
            "    %add_11 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level4_7_recp1_0_relu, %mul_15), kwargs = {})\n",
            "    %backbone_level4_9 : [num_users=4] = call_module[target=backbone.level4.9](args = (%add_11,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_0_conv : [num_users=1] = call_module[target=backbone.level4.10.recp7.0.conv](args = (%backbone_level4_9,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_0_bn : [num_users=1] = call_module[target=backbone.level4.10.recp7.0.bn](args = (%backbone_level4_10_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_0_relu : [num_users=1] = call_module[target=backbone.level4.10.recp7.0.relu](args = (%backbone_level4_10_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_1_conv : [num_users=1] = call_module[target=backbone.level4.10.recp7.1.conv](args = (%backbone_level4_10_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_1_bn : [num_users=1] = call_module[target=backbone.level4.10.recp7.1.bn](args = (%backbone_level4_10_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_1_relu : [num_users=1] = call_module[target=backbone.level4.10.recp7.1.relu](args = (%backbone_level4_10_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_2_conv : [num_users=1] = call_module[target=backbone.level4.10.recp7.2.conv](args = (%backbone_level4_10_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_2_bn : [num_users=1] = call_module[target=backbone.level4.10.recp7.2.bn](args = (%backbone_level4_10_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_3_conv : [num_users=1] = call_module[target=backbone.level4.10.recp7.3.conv](args = (%backbone_level4_10_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_3_bn : [num_users=1] = call_module[target=backbone.level4.10.recp7.3.bn](args = (%backbone_level4_10_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp7_3_relu : [num_users=3] = call_module[target=backbone.level4.10.recp7.3.relu](args = (%backbone_level4_10_recp7_3_bn,), kwargs = {})\n",
            "    %cat_25 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_9, %backbone_level4_10_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_10_recp5_0_conv : [num_users=1] = call_module[target=backbone.level4.10.recp5.0.conv](args = (%cat_25,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_0_bn : [num_users=1] = call_module[target=backbone.level4.10.recp5.0.bn](args = (%backbone_level4_10_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_0_relu : [num_users=1] = call_module[target=backbone.level4.10.recp5.0.relu](args = (%backbone_level4_10_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_1_conv : [num_users=1] = call_module[target=backbone.level4.10.recp5.1.conv](args = (%backbone_level4_10_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_1_bn : [num_users=1] = call_module[target=backbone.level4.10.recp5.1.bn](args = (%backbone_level4_10_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_1_relu : [num_users=1] = call_module[target=backbone.level4.10.recp5.1.relu](args = (%backbone_level4_10_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_2_conv : [num_users=1] = call_module[target=backbone.level4.10.recp5.2.conv](args = (%backbone_level4_10_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_2_bn : [num_users=1] = call_module[target=backbone.level4.10.recp5.2.bn](args = (%backbone_level4_10_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_3_conv : [num_users=1] = call_module[target=backbone.level4.10.recp5.3.conv](args = (%backbone_level4_10_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_3_bn : [num_users=1] = call_module[target=backbone.level4.10.recp5.3.bn](args = (%backbone_level4_10_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp5_3_relu : [num_users=2] = call_module[target=backbone.level4.10.recp5.3.relu](args = (%backbone_level4_10_recp5_3_bn,), kwargs = {})\n",
            "    %cat_26 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_9, %backbone_level4_10_recp7_3_relu, %backbone_level4_10_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_10_recp3_0_conv : [num_users=1] = call_module[target=backbone.level4.10.recp3.0.conv](args = (%cat_26,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_0_bn : [num_users=1] = call_module[target=backbone.level4.10.recp3.0.bn](args = (%backbone_level4_10_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_0_relu : [num_users=1] = call_module[target=backbone.level4.10.recp3.0.relu](args = (%backbone_level4_10_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_1_conv : [num_users=1] = call_module[target=backbone.level4.10.recp3.1.conv](args = (%backbone_level4_10_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_1_bn : [num_users=1] = call_module[target=backbone.level4.10.recp3.1.bn](args = (%backbone_level4_10_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_1_relu : [num_users=1] = call_module[target=backbone.level4.10.recp3.1.relu](args = (%backbone_level4_10_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_2_conv : [num_users=1] = call_module[target=backbone.level4.10.recp3.2.conv](args = (%backbone_level4_10_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_2_bn : [num_users=1] = call_module[target=backbone.level4.10.recp3.2.bn](args = (%backbone_level4_10_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_3_conv : [num_users=1] = call_module[target=backbone.level4.10.recp3.3.conv](args = (%backbone_level4_10_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_3_bn : [num_users=1] = call_module[target=backbone.level4.10.recp3.3.bn](args = (%backbone_level4_10_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp3_3_relu : [num_users=1] = call_module[target=backbone.level4.10.recp3.3.relu](args = (%backbone_level4_10_recp3_3_bn,), kwargs = {})\n",
            "    %cat_27 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_9, %backbone_level4_10_recp7_3_relu, %backbone_level4_10_recp5_3_relu, %backbone_level4_10_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_10_recp1_0_conv : [num_users=1] = call_module[target=backbone.level4.10.recp1.0.conv](args = (%cat_27,), kwargs = {})\n",
            "    %backbone_level4_10_recp1_0_bn : [num_users=1] = call_module[target=backbone.level4.10.recp1.0.bn](args = (%backbone_level4_10_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level4_10_recp1_0_relu : [num_users=5] = call_module[target=backbone.level4.10.recp1.0.relu](args = (%backbone_level4_10_recp1_0_bn,), kwargs = {})\n",
            "    %size_8 : [num_users=4] = call_method[target=size](args = (%backbone_level4_10_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_32 : [num_users=2] = call_function[target=operator.getitem](args = (%size_8, 0), kwargs = {})\n",
            "    %getitem_33 : [num_users=2] = call_function[target=operator.getitem](args = (%size_8, 1), kwargs = {})\n",
            "    %getitem_34 : [num_users=0] = call_function[target=operator.getitem](args = (%size_8, 2), kwargs = {})\n",
            "    %getitem_35 : [num_users=0] = call_function[target=operator.getitem](args = (%size_8, 3), kwargs = {})\n",
            "    %backbone_level4_11_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level4.11.channel_att.avg_pool](args = (%backbone_level4_10_recp1_0_relu,), kwargs = {})\n",
            "    %view_16 : [num_users=1] = call_method[target=view](args = (%backbone_level4_11_channel_att_avg_pool, %getitem_32, %getitem_33), kwargs = {})\n",
            "    %backbone_level4_11_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level4.11.channel_att.fc.0](args = (%view_16,), kwargs = {})\n",
            "    %backbone_level4_11_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level4.11.channel_att.fc.1](args = (%backbone_level4_11_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level4_11_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level4.11.channel_att.fc.2](args = (%backbone_level4_11_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level4_11_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level4.11.channel_att.fc.3](args = (%backbone_level4_11_channel_att_fc_2,), kwargs = {})\n",
            "    %view_17 : [num_users=1] = call_method[target=view](args = (%backbone_level4_11_channel_att_fc_3, %getitem_32, %getitem_33, 1, 1), kwargs = {})\n",
            "    %expand_as_8 : [num_users=1] = call_method[target=expand_as](args = (%view_17, %backbone_level4_10_recp1_0_relu), kwargs = {})\n",
            "    %mul_16 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level4_10_recp1_0_relu, %expand_as_8), kwargs = {})\n",
            "    %backbone_level4_11_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level4.11.spatialatt.spatial.conv](args = (%mul_16,), kwargs = {})\n",
            "    %backbone_level4_11_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level4.11.spatialatt.spatial.bn](args = (%backbone_level4_11_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_8 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level4_11_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_17 : [num_users=1] = call_function[target=operator.mul](args = (%mul_16, %sigmoid_8), kwargs = {})\n",
            "    %add_12 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level4_10_recp1_0_relu, %mul_17), kwargs = {})\n",
            "    %backbone_level4_12 : [num_users=4] = call_module[target=backbone.level4.12](args = (%add_12,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_0_conv : [num_users=1] = call_module[target=backbone.level4.13.recp7.0.conv](args = (%backbone_level4_12,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_0_bn : [num_users=1] = call_module[target=backbone.level4.13.recp7.0.bn](args = (%backbone_level4_13_recp7_0_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_0_relu : [num_users=1] = call_module[target=backbone.level4.13.recp7.0.relu](args = (%backbone_level4_13_recp7_0_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_1_conv : [num_users=1] = call_module[target=backbone.level4.13.recp7.1.conv](args = (%backbone_level4_13_recp7_0_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_1_bn : [num_users=1] = call_module[target=backbone.level4.13.recp7.1.bn](args = (%backbone_level4_13_recp7_1_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_1_relu : [num_users=1] = call_module[target=backbone.level4.13.recp7.1.relu](args = (%backbone_level4_13_recp7_1_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_2_conv : [num_users=1] = call_module[target=backbone.level4.13.recp7.2.conv](args = (%backbone_level4_13_recp7_1_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_2_bn : [num_users=1] = call_module[target=backbone.level4.13.recp7.2.bn](args = (%backbone_level4_13_recp7_2_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_3_conv : [num_users=1] = call_module[target=backbone.level4.13.recp7.3.conv](args = (%backbone_level4_13_recp7_2_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_3_bn : [num_users=1] = call_module[target=backbone.level4.13.recp7.3.bn](args = (%backbone_level4_13_recp7_3_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp7_3_relu : [num_users=3] = call_module[target=backbone.level4.13.recp7.3.relu](args = (%backbone_level4_13_recp7_3_bn,), kwargs = {})\n",
            "    %cat_28 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_12, %backbone_level4_13_recp7_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_13_recp5_0_conv : [num_users=1] = call_module[target=backbone.level4.13.recp5.0.conv](args = (%cat_28,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_0_bn : [num_users=1] = call_module[target=backbone.level4.13.recp5.0.bn](args = (%backbone_level4_13_recp5_0_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_0_relu : [num_users=1] = call_module[target=backbone.level4.13.recp5.0.relu](args = (%backbone_level4_13_recp5_0_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_1_conv : [num_users=1] = call_module[target=backbone.level4.13.recp5.1.conv](args = (%backbone_level4_13_recp5_0_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_1_bn : [num_users=1] = call_module[target=backbone.level4.13.recp5.1.bn](args = (%backbone_level4_13_recp5_1_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_1_relu : [num_users=1] = call_module[target=backbone.level4.13.recp5.1.relu](args = (%backbone_level4_13_recp5_1_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_2_conv : [num_users=1] = call_module[target=backbone.level4.13.recp5.2.conv](args = (%backbone_level4_13_recp5_1_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_2_bn : [num_users=1] = call_module[target=backbone.level4.13.recp5.2.bn](args = (%backbone_level4_13_recp5_2_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_3_conv : [num_users=1] = call_module[target=backbone.level4.13.recp5.3.conv](args = (%backbone_level4_13_recp5_2_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_3_bn : [num_users=1] = call_module[target=backbone.level4.13.recp5.3.bn](args = (%backbone_level4_13_recp5_3_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp5_3_relu : [num_users=2] = call_module[target=backbone.level4.13.recp5.3.relu](args = (%backbone_level4_13_recp5_3_bn,), kwargs = {})\n",
            "    %cat_29 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_12, %backbone_level4_13_recp7_3_relu, %backbone_level4_13_recp5_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_13_recp3_0_conv : [num_users=1] = call_module[target=backbone.level4.13.recp3.0.conv](args = (%cat_29,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_0_bn : [num_users=1] = call_module[target=backbone.level4.13.recp3.0.bn](args = (%backbone_level4_13_recp3_0_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_0_relu : [num_users=1] = call_module[target=backbone.level4.13.recp3.0.relu](args = (%backbone_level4_13_recp3_0_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_1_conv : [num_users=1] = call_module[target=backbone.level4.13.recp3.1.conv](args = (%backbone_level4_13_recp3_0_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_1_bn : [num_users=1] = call_module[target=backbone.level4.13.recp3.1.bn](args = (%backbone_level4_13_recp3_1_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_1_relu : [num_users=1] = call_module[target=backbone.level4.13.recp3.1.relu](args = (%backbone_level4_13_recp3_1_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_2_conv : [num_users=1] = call_module[target=backbone.level4.13.recp3.2.conv](args = (%backbone_level4_13_recp3_1_relu,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_2_bn : [num_users=1] = call_module[target=backbone.level4.13.recp3.2.bn](args = (%backbone_level4_13_recp3_2_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_3_conv : [num_users=1] = call_module[target=backbone.level4.13.recp3.3.conv](args = (%backbone_level4_13_recp3_2_bn,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_3_bn : [num_users=1] = call_module[target=backbone.level4.13.recp3.3.bn](args = (%backbone_level4_13_recp3_3_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp3_3_relu : [num_users=1] = call_module[target=backbone.level4.13.recp3.3.relu](args = (%backbone_level4_13_recp3_3_bn,), kwargs = {})\n",
            "    %cat_30 : [num_users=1] = call_function[target=torch.cat](args = ([%backbone_level4_12, %backbone_level4_13_recp7_3_relu, %backbone_level4_13_recp5_3_relu, %backbone_level4_13_recp3_3_relu],), kwargs = {dim: 1})\n",
            "    %backbone_level4_13_recp1_0_conv : [num_users=1] = call_module[target=backbone.level4.13.recp1.0.conv](args = (%cat_30,), kwargs = {})\n",
            "    %backbone_level4_13_recp1_0_bn : [num_users=1] = call_module[target=backbone.level4.13.recp1.0.bn](args = (%backbone_level4_13_recp1_0_conv,), kwargs = {})\n",
            "    %backbone_level4_13_recp1_0_relu : [num_users=5] = call_module[target=backbone.level4.13.recp1.0.relu](args = (%backbone_level4_13_recp1_0_bn,), kwargs = {})\n",
            "    %size_9 : [num_users=4] = call_method[target=size](args = (%backbone_level4_13_recp1_0_relu,), kwargs = {})\n",
            "    %getitem_36 : [num_users=2] = call_function[target=operator.getitem](args = (%size_9, 0), kwargs = {})\n",
            "    %getitem_37 : [num_users=2] = call_function[target=operator.getitem](args = (%size_9, 1), kwargs = {})\n",
            "    %getitem_38 : [num_users=0] = call_function[target=operator.getitem](args = (%size_9, 2), kwargs = {})\n",
            "    %getitem_39 : [num_users=0] = call_function[target=operator.getitem](args = (%size_9, 3), kwargs = {})\n",
            "    %backbone_level4_14_channel_att_avg_pool : [num_users=1] = call_module[target=backbone.level4.14.channel_att.avg_pool](args = (%backbone_level4_13_recp1_0_relu,), kwargs = {})\n",
            "    %view_18 : [num_users=1] = call_method[target=view](args = (%backbone_level4_14_channel_att_avg_pool, %getitem_36, %getitem_37), kwargs = {})\n",
            "    %backbone_level4_14_channel_att_fc_0 : [num_users=1] = call_module[target=backbone.level4.14.channel_att.fc.0](args = (%view_18,), kwargs = {})\n",
            "    %backbone_level4_14_channel_att_fc_1 : [num_users=1] = call_module[target=backbone.level4.14.channel_att.fc.1](args = (%backbone_level4_14_channel_att_fc_0,), kwargs = {})\n",
            "    %backbone_level4_14_channel_att_fc_2 : [num_users=1] = call_module[target=backbone.level4.14.channel_att.fc.2](args = (%backbone_level4_14_channel_att_fc_1,), kwargs = {})\n",
            "    %backbone_level4_14_channel_att_fc_3 : [num_users=1] = call_module[target=backbone.level4.14.channel_att.fc.3](args = (%backbone_level4_14_channel_att_fc_2,), kwargs = {})\n",
            "    %view_19 : [num_users=1] = call_method[target=view](args = (%backbone_level4_14_channel_att_fc_3, %getitem_36, %getitem_37, 1, 1), kwargs = {})\n",
            "    %expand_as_9 : [num_users=1] = call_method[target=expand_as](args = (%view_19, %backbone_level4_13_recp1_0_relu), kwargs = {})\n",
            "    %mul_18 : [num_users=2] = call_function[target=operator.mul](args = (%backbone_level4_13_recp1_0_relu, %expand_as_9), kwargs = {})\n",
            "    %backbone_level4_14_spatialatt_spatial_conv : [num_users=1] = call_module[target=backbone.level4.14.spatialatt.spatial.conv](args = (%mul_18,), kwargs = {})\n",
            "    %backbone_level4_14_spatialatt_spatial_bn : [num_users=1] = call_module[target=backbone.level4.14.spatialatt.spatial.bn](args = (%backbone_level4_14_spatialatt_spatial_conv,), kwargs = {})\n",
            "    %sigmoid_9 : [num_users=1] = call_function[target=torch.sigmoid](args = (%backbone_level4_14_spatialatt_spatial_bn,), kwargs = {})\n",
            "    %mul_19 : [num_users=1] = call_function[target=operator.mul](args = (%mul_18, %sigmoid_9), kwargs = {})\n",
            "    %add_13 : [num_users=1] = call_function[target=operator.add](args = (%backbone_level4_13_recp1_0_relu, %mul_19), kwargs = {})\n",
            "    %backbone_branch4 : [num_users=1] = call_module[target=backbone.branch4](args = (%backbone_level4_0_act,), kwargs = {})\n",
            "    %add_14 : [num_users=1] = call_function[target=operator.add](args = (%backbone_branch4, %add_13), kwargs = {})\n",
            "    %backbone_br4_0 : [num_users=1] = call_module[target=backbone.br4.0](args = (%add_14,), kwargs = {})\n",
            "    %backbone_br4_1 : [num_users=1] = call_module[target=backbone.br4.1](args = (%backbone_br4_0,), kwargs = {})\n",
            "    %size_10 : [num_users=1] = call_method[target=size](args = (%backbone_br3_1,), kwargs = {})\n",
            "    %getitem_40 : [num_users=1] = call_function[target=operator.getitem](args = (%size_10, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate : [num_users=2] = call_function[target=torch.nn.functional.interpolate](args = (%backbone_br4_1,), kwargs = {size: %getitem_40, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %up3_conv4_conv0 : [num_users=1] = call_module[target=up3_conv4.conv0](args = (%interpolate,), kwargs = {})\n",
            "    %chunk : [num_users=2] = call_function[target=torch.chunk](args = (%up3_conv4_conv0, 2), kwargs = {dim: 1})\n",
            "    %getitem_41 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_42 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %up3_conv4_conv1 : [num_users=1] = call_module[target=up3_conv4.conv1](args = (%getitem_41,), kwargs = {})\n",
            "    %up3_conv4_conv2 : [num_users=1] = call_module[target=up3_conv4.conv2](args = (%getitem_42,), kwargs = {})\n",
            "    %cat_31 : [num_users=1] = call_function[target=torch.cat](args = ([%up3_conv4_conv1, %up3_conv4_conv2],), kwargs = {dim: 1})\n",
            "    %up3_conv4_bn : [num_users=1] = call_module[target=up3_conv4.bn](args = (%cat_31,), kwargs = {})\n",
            "    %up3_conv3 : [num_users=1] = call_module[target=up3_conv3](args = (%backbone_br3_1,), kwargs = {})\n",
            "    %up3_bn3 : [num_users=1] = call_module[target=up3_bn3](args = (%up3_conv3,), kwargs = {})\n",
            "    %add_15 : [num_users=1] = call_function[target=operator.add](args = (%up3_conv4_bn, %up3_bn3), kwargs = {})\n",
            "    %up3_act : [num_users=1] = call_module[target=up3_act](args = (%add_15,), kwargs = {})\n",
            "    %size_11 : [num_users=1] = call_method[target=size](args = (%backbone_br2_1,), kwargs = {})\n",
            "    %getitem_43 : [num_users=1] = call_function[target=operator.getitem](args = (%size_11, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_1 : [num_users=2] = call_function[target=torch.nn.functional.interpolate](args = (%up3_act,), kwargs = {size: %getitem_43, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %up2_conv3_conv0 : [num_users=1] = call_module[target=up2_conv3.conv0](args = (%interpolate_1,), kwargs = {})\n",
            "    %chunk_1 : [num_users=2] = call_function[target=torch.chunk](args = (%up2_conv3_conv0, 2), kwargs = {dim: 1})\n",
            "    %getitem_44 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk_1, 0), kwargs = {})\n",
            "    %getitem_45 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk_1, 1), kwargs = {})\n",
            "    %up2_conv3_conv1 : [num_users=1] = call_module[target=up2_conv3.conv1](args = (%getitem_44,), kwargs = {})\n",
            "    %up2_conv3_conv2 : [num_users=1] = call_module[target=up2_conv3.conv2](args = (%getitem_45,), kwargs = {})\n",
            "    %cat_32 : [num_users=1] = call_function[target=torch.cat](args = ([%up2_conv3_conv1, %up2_conv3_conv2],), kwargs = {dim: 1})\n",
            "    %up2_conv3_bn : [num_users=1] = call_module[target=up2_conv3.bn](args = (%cat_32,), kwargs = {})\n",
            "    %up2_conv2 : [num_users=1] = call_module[target=up2_conv2](args = (%backbone_br2_1,), kwargs = {})\n",
            "    %up2_bn2 : [num_users=1] = call_module[target=up2_bn2](args = (%up2_conv2,), kwargs = {})\n",
            "    %add_16 : [num_users=1] = call_function[target=operator.add](args = (%up2_conv3_bn, %up2_bn2), kwargs = {})\n",
            "    %up2_act : [num_users=1] = call_module[target=up2_act](args = (%add_16,), kwargs = {})\n",
            "    %size_12 : [num_users=1] = call_method[target=size](args = (%backbone_br1_1,), kwargs = {})\n",
            "    %getitem_46 : [num_users=1] = call_function[target=operator.getitem](args = (%size_12, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_2 : [num_users=2] = call_function[target=torch.nn.functional.interpolate](args = (%up2_act,), kwargs = {size: %getitem_46, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %up1_conv2_conv0 : [num_users=1] = call_module[target=up1_conv2.conv0](args = (%interpolate_2,), kwargs = {})\n",
            "    %chunk_2 : [num_users=2] = call_function[target=torch.chunk](args = (%up1_conv2_conv0, 2), kwargs = {dim: 1})\n",
            "    %getitem_47 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk_2, 0), kwargs = {})\n",
            "    %getitem_48 : [num_users=1] = call_function[target=operator.getitem](args = (%chunk_2, 1), kwargs = {})\n",
            "    %up1_conv2_conv1 : [num_users=1] = call_module[target=up1_conv2.conv1](args = (%getitem_47,), kwargs = {})\n",
            "    %up1_conv2_conv2 : [num_users=1] = call_module[target=up1_conv2.conv2](args = (%getitem_48,), kwargs = {})\n",
            "    %cat_33 : [num_users=1] = call_function[target=torch.cat](args = ([%up1_conv2_conv1, %up1_conv2_conv2],), kwargs = {dim: 1})\n",
            "    %up1_conv2_bn : [num_users=1] = call_module[target=up1_conv2.bn](args = (%cat_33,), kwargs = {})\n",
            "    %up1_conv1 : [num_users=1] = call_module[target=up1_conv1](args = (%backbone_br1_1,), kwargs = {})\n",
            "    %up1_bn1 : [num_users=1] = call_module[target=up1_bn1](args = (%up1_conv1,), kwargs = {})\n",
            "    %add_17 : [num_users=1] = call_function[target=operator.add](args = (%up1_conv2_bn, %up1_bn1), kwargs = {})\n",
            "    %up1_act : [num_users=1] = call_module[target=up1_act](args = (%add_17,), kwargs = {})\n",
            "    %classifier4_0 : [num_users=1] = call_module[target=classifier4.0](args = (%interpolate,), kwargs = {})\n",
            "    %classifier4_1 : [num_users=1] = call_module[target=classifier4.1](args = (%classifier4_0,), kwargs = {})\n",
            "    %sigmoid_10 : [num_users=1] = call_function[target=torch.sigmoid](args = (%classifier4_1,), kwargs = {})\n",
            "    %classifier3_0 : [num_users=1] = call_module[target=classifier3.0](args = (%interpolate_1,), kwargs = {})\n",
            "    %classifier3_1 : [num_users=1] = call_module[target=classifier3.1](args = (%classifier3_0,), kwargs = {})\n",
            "    %sigmoid_11 : [num_users=1] = call_function[target=torch.sigmoid](args = (%classifier3_1,), kwargs = {})\n",
            "    %classifier2_0 : [num_users=1] = call_module[target=classifier2.0](args = (%interpolate_2,), kwargs = {})\n",
            "    %classifier2_1 : [num_users=1] = call_module[target=classifier2.1](args = (%classifier2_0,), kwargs = {})\n",
            "    %sigmoid_12 : [num_users=1] = call_function[target=torch.sigmoid](args = (%classifier2_1,), kwargs = {})\n",
            "    %classifier1_0 : [num_users=1] = call_module[target=classifier1.0](args = (%up1_act,), kwargs = {})\n",
            "    %classifier1_1 : [num_users=1] = call_module[target=classifier1.1](args = (%classifier1_0,), kwargs = {})\n",
            "    %sigmoid_13 : [num_users=1] = call_function[target=torch.sigmoid](args = (%classifier1_1,), kwargs = {})\n",
            "    %size_13 : [num_users=1] = call_method[target=size](args = (%input_1,), kwargs = {})\n",
            "    %getitem_49 : [num_users=1] = call_function[target=operator.getitem](args = (%size_13, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_3 : [num_users=1] = call_function[target=torch.nn.functional.interpolate](args = (%sigmoid_10,), kwargs = {size: %getitem_49, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %size_14 : [num_users=1] = call_method[target=size](args = (%input_1,), kwargs = {})\n",
            "    %getitem_50 : [num_users=1] = call_function[target=operator.getitem](args = (%size_14, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_4 : [num_users=1] = call_function[target=torch.nn.functional.interpolate](args = (%sigmoid_11,), kwargs = {size: %getitem_50, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %size_15 : [num_users=1] = call_method[target=size](args = (%input_1,), kwargs = {})\n",
            "    %getitem_51 : [num_users=1] = call_function[target=operator.getitem](args = (%size_15, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_5 : [num_users=1] = call_function[target=torch.nn.functional.interpolate](args = (%sigmoid_12,), kwargs = {size: %getitem_51, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    %size_16 : [num_users=1] = call_method[target=size](args = (%input_1,), kwargs = {})\n",
            "    %getitem_52 : [num_users=1] = call_function[target=operator.getitem](args = (%size_16, slice(2, None, None)), kwargs = {})\n",
            "    %interpolate_6 : [num_users=1] = call_function[target=torch.nn.functional.interpolate](args = (%sigmoid_13,), kwargs = {size: %getitem_52, scale_factor: None, mode: bilinear, align_corners: False, recompute_scale_factor: None, antialias: False})\n",
            "    return (interpolate_6, interpolate_5, interpolate_4, interpolate_3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad == False:\n",
        "      print(f\"{name}: requires_grad={param.requires_grad}\")\n"
      ],
      "metadata": {
        "id": "iIB8_YQcmVTe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcerz5aJoevY",
        "outputId": "e79cf393-f037-45a3-a359-ed62e4e78775"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backbone.level1_0.conv.weight: torch.float32\n",
            "backbone.level1_0.bn.weight: torch.float32\n",
            "backbone.level1_0.bn.bias: torch.float32\n",
            "backbone.level1_0.act.weight: torch.float32\n",
            "backbone.level1.0.conv.weight: torch.float32\n",
            "backbone.level1.0.bn.weight: torch.float32\n",
            "backbone.level1.0.bn.bias: torch.float32\n",
            "backbone.level1.0.act.weight: torch.float32\n",
            "backbone.level1.1.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level1.1.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level1.1.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level1.1.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level1.1.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.branch1.weight: torch.float32\n",
            "backbone.br1.0.weight: torch.float32\n",
            "backbone.br1.0.bias: torch.float32\n",
            "backbone.br1.1.weight: torch.float32\n",
            "backbone.level2_0.conv0.weight: torch.float32\n",
            "backbone.level2_0.conv1.weight: torch.float32\n",
            "backbone.level2_0.bn.weight: torch.float32\n",
            "backbone.level2_0.bn.bias: torch.float32\n",
            "backbone.level2_0.act.weight: torch.float32\n",
            "backbone.level2.1.recp7.0.conv.weight: torch.float32\n",
            "backbone.level2.1.recp7.0.bn.weight: torch.float32\n",
            "backbone.level2.1.recp7.0.bn.bias: torch.float32\n",
            "backbone.level2.1.recp7.0.relu.weight: torch.float32\n",
            "backbone.level2.1.recp7.1.conv.weight: torch.float32\n",
            "backbone.level2.1.recp7.1.bn.weight: torch.float32\n",
            "backbone.level2.1.recp7.1.bn.bias: torch.float32\n",
            "backbone.level2.1.recp7.1.relu.weight: torch.float32\n",
            "backbone.level2.1.recp7.2.conv.weight: torch.float32\n",
            "backbone.level2.1.recp7.2.bn.weight: torch.float32\n",
            "backbone.level2.1.recp7.2.bn.bias: torch.float32\n",
            "backbone.level2.1.recp7.3.conv.weight: torch.float32\n",
            "backbone.level2.1.recp7.3.bn.weight: torch.float32\n",
            "backbone.level2.1.recp7.3.bn.bias: torch.float32\n",
            "backbone.level2.1.recp7.3.relu.weight: torch.float32\n",
            "backbone.level2.1.recp5.0.conv.weight: torch.float32\n",
            "backbone.level2.1.recp5.0.bn.weight: torch.float32\n",
            "backbone.level2.1.recp5.0.bn.bias: torch.float32\n",
            "backbone.level2.1.recp5.0.relu.weight: torch.float32\n",
            "backbone.level2.1.recp5.1.conv.weight: torch.float32\n",
            "backbone.level2.1.recp5.1.bn.weight: torch.float32\n",
            "backbone.level2.1.recp5.1.bn.bias: torch.float32\n",
            "backbone.level2.1.recp5.1.relu.weight: torch.float32\n",
            "backbone.level2.1.recp5.2.conv.weight: torch.float32\n",
            "backbone.level2.1.recp5.2.bn.weight: torch.float32\n",
            "backbone.level2.1.recp5.2.bn.bias: torch.float32\n",
            "backbone.level2.1.recp5.3.conv.weight: torch.float32\n",
            "backbone.level2.1.recp5.3.bn.weight: torch.float32\n",
            "backbone.level2.1.recp5.3.bn.bias: torch.float32\n",
            "backbone.level2.1.recp5.3.relu.weight: torch.float32\n",
            "backbone.level2.1.recp3.0.conv.weight: torch.float32\n",
            "backbone.level2.1.recp3.0.bn.weight: torch.float32\n",
            "backbone.level2.1.recp3.0.bn.bias: torch.float32\n",
            "backbone.level2.1.recp3.0.relu.weight: torch.float32\n",
            "backbone.level2.1.recp3.1.conv.weight: torch.float32\n",
            "backbone.level2.1.recp3.1.bn.weight: torch.float32\n",
            "backbone.level2.1.recp3.1.bn.bias: torch.float32\n",
            "backbone.level2.1.recp3.1.relu.weight: torch.float32\n",
            "backbone.level2.1.recp3.2.conv.weight: torch.float32\n",
            "backbone.level2.1.recp3.2.bn.weight: torch.float32\n",
            "backbone.level2.1.recp3.2.bn.bias: torch.float32\n",
            "backbone.level2.1.recp3.3.conv.weight: torch.float32\n",
            "backbone.level2.1.recp3.3.bn.weight: torch.float32\n",
            "backbone.level2.1.recp3.3.bn.bias: torch.float32\n",
            "backbone.level2.1.recp3.3.relu.weight: torch.float32\n",
            "backbone.level2.1.recp1.0.conv.weight: torch.float32\n",
            "backbone.level2.1.recp1.0.bn.weight: torch.float32\n",
            "backbone.level2.1.recp1.0.bn.bias: torch.float32\n",
            "backbone.level2.1.recp1.0.relu.weight: torch.float32\n",
            "backbone.level2.2.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level2.2.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level2.2.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level2.2.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level2.2.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.branch2.weight: torch.float32\n",
            "backbone.br2.0.weight: torch.float32\n",
            "backbone.br2.0.bias: torch.float32\n",
            "backbone.br2.1.weight: torch.float32\n",
            "backbone.level3_0.conv0.weight: torch.float32\n",
            "backbone.level3_0.conv1.weight: torch.float32\n",
            "backbone.level3_0.bn.weight: torch.float32\n",
            "backbone.level3_0.bn.bias: torch.float32\n",
            "backbone.level3_0.act.weight: torch.float32\n",
            "backbone.level3.1.recp7.0.conv.weight: torch.float32\n",
            "backbone.level3.1.recp7.0.bn.weight: torch.float32\n",
            "backbone.level3.1.recp7.0.bn.bias: torch.float32\n",
            "backbone.level3.1.recp7.0.relu.weight: torch.float32\n",
            "backbone.level3.1.recp7.1.conv.weight: torch.float32\n",
            "backbone.level3.1.recp7.1.bn.weight: torch.float32\n",
            "backbone.level3.1.recp7.1.bn.bias: torch.float32\n",
            "backbone.level3.1.recp7.1.relu.weight: torch.float32\n",
            "backbone.level3.1.recp7.2.conv.weight: torch.float32\n",
            "backbone.level3.1.recp7.2.bn.weight: torch.float32\n",
            "backbone.level3.1.recp7.2.bn.bias: torch.float32\n",
            "backbone.level3.1.recp7.3.conv.weight: torch.float32\n",
            "backbone.level3.1.recp7.3.bn.weight: torch.float32\n",
            "backbone.level3.1.recp7.3.bn.bias: torch.float32\n",
            "backbone.level3.1.recp7.3.relu.weight: torch.float32\n",
            "backbone.level3.1.recp5.0.conv.weight: torch.float32\n",
            "backbone.level3.1.recp5.0.bn.weight: torch.float32\n",
            "backbone.level3.1.recp5.0.bn.bias: torch.float32\n",
            "backbone.level3.1.recp5.0.relu.weight: torch.float32\n",
            "backbone.level3.1.recp5.1.conv.weight: torch.float32\n",
            "backbone.level3.1.recp5.1.bn.weight: torch.float32\n",
            "backbone.level3.1.recp5.1.bn.bias: torch.float32\n",
            "backbone.level3.1.recp5.1.relu.weight: torch.float32\n",
            "backbone.level3.1.recp5.2.conv.weight: torch.float32\n",
            "backbone.level3.1.recp5.2.bn.weight: torch.float32\n",
            "backbone.level3.1.recp5.2.bn.bias: torch.float32\n",
            "backbone.level3.1.recp5.3.conv.weight: torch.float32\n",
            "backbone.level3.1.recp5.3.bn.weight: torch.float32\n",
            "backbone.level3.1.recp5.3.bn.bias: torch.float32\n",
            "backbone.level3.1.recp5.3.relu.weight: torch.float32\n",
            "backbone.level3.1.recp3.0.conv.weight: torch.float32\n",
            "backbone.level3.1.recp3.0.bn.weight: torch.float32\n",
            "backbone.level3.1.recp3.0.bn.bias: torch.float32\n",
            "backbone.level3.1.recp3.0.relu.weight: torch.float32\n",
            "backbone.level3.1.recp3.1.conv.weight: torch.float32\n",
            "backbone.level3.1.recp3.1.bn.weight: torch.float32\n",
            "backbone.level3.1.recp3.1.bn.bias: torch.float32\n",
            "backbone.level3.1.recp3.1.relu.weight: torch.float32\n",
            "backbone.level3.1.recp3.2.conv.weight: torch.float32\n",
            "backbone.level3.1.recp3.2.bn.weight: torch.float32\n",
            "backbone.level3.1.recp3.2.bn.bias: torch.float32\n",
            "backbone.level3.1.recp3.3.conv.weight: torch.float32\n",
            "backbone.level3.1.recp3.3.bn.weight: torch.float32\n",
            "backbone.level3.1.recp3.3.bn.bias: torch.float32\n",
            "backbone.level3.1.recp3.3.relu.weight: torch.float32\n",
            "backbone.level3.1.recp1.0.conv.weight: torch.float32\n",
            "backbone.level3.1.recp1.0.bn.weight: torch.float32\n",
            "backbone.level3.1.recp1.0.bn.bias: torch.float32\n",
            "backbone.level3.1.recp1.0.relu.weight: torch.float32\n",
            "backbone.level3.2.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level3.2.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level3.2.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level3.2.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level3.2.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level3.4.recp7.0.conv.weight: torch.float32\n",
            "backbone.level3.4.recp7.0.bn.weight: torch.float32\n",
            "backbone.level3.4.recp7.0.bn.bias: torch.float32\n",
            "backbone.level3.4.recp7.0.relu.weight: torch.float32\n",
            "backbone.level3.4.recp7.1.conv.weight: torch.float32\n",
            "backbone.level3.4.recp7.1.bn.weight: torch.float32\n",
            "backbone.level3.4.recp7.1.bn.bias: torch.float32\n",
            "backbone.level3.4.recp7.1.relu.weight: torch.float32\n",
            "backbone.level3.4.recp7.2.conv.weight: torch.float32\n",
            "backbone.level3.4.recp7.2.bn.weight: torch.float32\n",
            "backbone.level3.4.recp7.2.bn.bias: torch.float32\n",
            "backbone.level3.4.recp7.3.conv.weight: torch.float32\n",
            "backbone.level3.4.recp7.3.bn.weight: torch.float32\n",
            "backbone.level3.4.recp7.3.bn.bias: torch.float32\n",
            "backbone.level3.4.recp7.3.relu.weight: torch.float32\n",
            "backbone.level3.4.recp5.0.conv.weight: torch.float32\n",
            "backbone.level3.4.recp5.0.bn.weight: torch.float32\n",
            "backbone.level3.4.recp5.0.bn.bias: torch.float32\n",
            "backbone.level3.4.recp5.0.relu.weight: torch.float32\n",
            "backbone.level3.4.recp5.1.conv.weight: torch.float32\n",
            "backbone.level3.4.recp5.1.bn.weight: torch.float32\n",
            "backbone.level3.4.recp5.1.bn.bias: torch.float32\n",
            "backbone.level3.4.recp5.1.relu.weight: torch.float32\n",
            "backbone.level3.4.recp5.2.conv.weight: torch.float32\n",
            "backbone.level3.4.recp5.2.bn.weight: torch.float32\n",
            "backbone.level3.4.recp5.2.bn.bias: torch.float32\n",
            "backbone.level3.4.recp5.3.conv.weight: torch.float32\n",
            "backbone.level3.4.recp5.3.bn.weight: torch.float32\n",
            "backbone.level3.4.recp5.3.bn.bias: torch.float32\n",
            "backbone.level3.4.recp5.3.relu.weight: torch.float32\n",
            "backbone.level3.4.recp3.0.conv.weight: torch.float32\n",
            "backbone.level3.4.recp3.0.bn.weight: torch.float32\n",
            "backbone.level3.4.recp3.0.bn.bias: torch.float32\n",
            "backbone.level3.4.recp3.0.relu.weight: torch.float32\n",
            "backbone.level3.4.recp3.1.conv.weight: torch.float32\n",
            "backbone.level3.4.recp3.1.bn.weight: torch.float32\n",
            "backbone.level3.4.recp3.1.bn.bias: torch.float32\n",
            "backbone.level3.4.recp3.1.relu.weight: torch.float32\n",
            "backbone.level3.4.recp3.2.conv.weight: torch.float32\n",
            "backbone.level3.4.recp3.2.bn.weight: torch.float32\n",
            "backbone.level3.4.recp3.2.bn.bias: torch.float32\n",
            "backbone.level3.4.recp3.3.conv.weight: torch.float32\n",
            "backbone.level3.4.recp3.3.bn.weight: torch.float32\n",
            "backbone.level3.4.recp3.3.bn.bias: torch.float32\n",
            "backbone.level3.4.recp3.3.relu.weight: torch.float32\n",
            "backbone.level3.4.recp1.0.conv.weight: torch.float32\n",
            "backbone.level3.4.recp1.0.bn.weight: torch.float32\n",
            "backbone.level3.4.recp1.0.bn.bias: torch.float32\n",
            "backbone.level3.4.recp1.0.relu.weight: torch.float32\n",
            "backbone.level3.5.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level3.5.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level3.5.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level3.5.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level3.5.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level3.7.recp7.0.conv.weight: torch.float32\n",
            "backbone.level3.7.recp7.0.bn.weight: torch.float32\n",
            "backbone.level3.7.recp7.0.bn.bias: torch.float32\n",
            "backbone.level3.7.recp7.0.relu.weight: torch.float32\n",
            "backbone.level3.7.recp7.1.conv.weight: torch.float32\n",
            "backbone.level3.7.recp7.1.bn.weight: torch.float32\n",
            "backbone.level3.7.recp7.1.bn.bias: torch.float32\n",
            "backbone.level3.7.recp7.1.relu.weight: torch.float32\n",
            "backbone.level3.7.recp7.2.conv.weight: torch.float32\n",
            "backbone.level3.7.recp7.2.bn.weight: torch.float32\n",
            "backbone.level3.7.recp7.2.bn.bias: torch.float32\n",
            "backbone.level3.7.recp7.3.conv.weight: torch.float32\n",
            "backbone.level3.7.recp7.3.bn.weight: torch.float32\n",
            "backbone.level3.7.recp7.3.bn.bias: torch.float32\n",
            "backbone.level3.7.recp7.3.relu.weight: torch.float32\n",
            "backbone.level3.7.recp5.0.conv.weight: torch.float32\n",
            "backbone.level3.7.recp5.0.bn.weight: torch.float32\n",
            "backbone.level3.7.recp5.0.bn.bias: torch.float32\n",
            "backbone.level3.7.recp5.0.relu.weight: torch.float32\n",
            "backbone.level3.7.recp5.1.conv.weight: torch.float32\n",
            "backbone.level3.7.recp5.1.bn.weight: torch.float32\n",
            "backbone.level3.7.recp5.1.bn.bias: torch.float32\n",
            "backbone.level3.7.recp5.1.relu.weight: torch.float32\n",
            "backbone.level3.7.recp5.2.conv.weight: torch.float32\n",
            "backbone.level3.7.recp5.2.bn.weight: torch.float32\n",
            "backbone.level3.7.recp5.2.bn.bias: torch.float32\n",
            "backbone.level3.7.recp5.3.conv.weight: torch.float32\n",
            "backbone.level3.7.recp5.3.bn.weight: torch.float32\n",
            "backbone.level3.7.recp5.3.bn.bias: torch.float32\n",
            "backbone.level3.7.recp5.3.relu.weight: torch.float32\n",
            "backbone.level3.7.recp3.0.conv.weight: torch.float32\n",
            "backbone.level3.7.recp3.0.bn.weight: torch.float32\n",
            "backbone.level3.7.recp3.0.bn.bias: torch.float32\n",
            "backbone.level3.7.recp3.0.relu.weight: torch.float32\n",
            "backbone.level3.7.recp3.1.conv.weight: torch.float32\n",
            "backbone.level3.7.recp3.1.bn.weight: torch.float32\n",
            "backbone.level3.7.recp3.1.bn.bias: torch.float32\n",
            "backbone.level3.7.recp3.1.relu.weight: torch.float32\n",
            "backbone.level3.7.recp3.2.conv.weight: torch.float32\n",
            "backbone.level3.7.recp3.2.bn.weight: torch.float32\n",
            "backbone.level3.7.recp3.2.bn.bias: torch.float32\n",
            "backbone.level3.7.recp3.3.conv.weight: torch.float32\n",
            "backbone.level3.7.recp3.3.bn.weight: torch.float32\n",
            "backbone.level3.7.recp3.3.bn.bias: torch.float32\n",
            "backbone.level3.7.recp3.3.relu.weight: torch.float32\n",
            "backbone.level3.7.recp1.0.conv.weight: torch.float32\n",
            "backbone.level3.7.recp1.0.bn.weight: torch.float32\n",
            "backbone.level3.7.recp1.0.bn.bias: torch.float32\n",
            "backbone.level3.7.recp1.0.relu.weight: torch.float32\n",
            "backbone.level3.8.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level3.8.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level3.8.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level3.8.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level3.8.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.branch3.weight: torch.float32\n",
            "backbone.br3.0.weight: torch.float32\n",
            "backbone.br3.0.bias: torch.float32\n",
            "backbone.br3.1.weight: torch.float32\n",
            "backbone.level4_0.conv0.weight: torch.float32\n",
            "backbone.level4_0.conv1.weight: torch.float32\n",
            "backbone.level4_0.bn.weight: torch.float32\n",
            "backbone.level4_0.bn.bias: torch.float32\n",
            "backbone.level4_0.act.weight: torch.float32\n",
            "backbone.level4.1.recp7.0.conv.weight: torch.float32\n",
            "backbone.level4.1.recp7.0.bn.weight: torch.float32\n",
            "backbone.level4.1.recp7.0.bn.bias: torch.float32\n",
            "backbone.level4.1.recp7.0.relu.weight: torch.float32\n",
            "backbone.level4.1.recp7.1.conv.weight: torch.float32\n",
            "backbone.level4.1.recp7.1.bn.weight: torch.float32\n",
            "backbone.level4.1.recp7.1.bn.bias: torch.float32\n",
            "backbone.level4.1.recp7.1.relu.weight: torch.float32\n",
            "backbone.level4.1.recp7.2.conv.weight: torch.float32\n",
            "backbone.level4.1.recp7.2.bn.weight: torch.float32\n",
            "backbone.level4.1.recp7.2.bn.bias: torch.float32\n",
            "backbone.level4.1.recp7.3.conv.weight: torch.float32\n",
            "backbone.level4.1.recp7.3.bn.weight: torch.float32\n",
            "backbone.level4.1.recp7.3.bn.bias: torch.float32\n",
            "backbone.level4.1.recp7.3.relu.weight: torch.float32\n",
            "backbone.level4.1.recp5.0.conv.weight: torch.float32\n",
            "backbone.level4.1.recp5.0.bn.weight: torch.float32\n",
            "backbone.level4.1.recp5.0.bn.bias: torch.float32\n",
            "backbone.level4.1.recp5.0.relu.weight: torch.float32\n",
            "backbone.level4.1.recp5.1.conv.weight: torch.float32\n",
            "backbone.level4.1.recp5.1.bn.weight: torch.float32\n",
            "backbone.level4.1.recp5.1.bn.bias: torch.float32\n",
            "backbone.level4.1.recp5.1.relu.weight: torch.float32\n",
            "backbone.level4.1.recp5.2.conv.weight: torch.float32\n",
            "backbone.level4.1.recp5.2.bn.weight: torch.float32\n",
            "backbone.level4.1.recp5.2.bn.bias: torch.float32\n",
            "backbone.level4.1.recp5.3.conv.weight: torch.float32\n",
            "backbone.level4.1.recp5.3.bn.weight: torch.float32\n",
            "backbone.level4.1.recp5.3.bn.bias: torch.float32\n",
            "backbone.level4.1.recp5.3.relu.weight: torch.float32\n",
            "backbone.level4.1.recp3.0.conv.weight: torch.float32\n",
            "backbone.level4.1.recp3.0.bn.weight: torch.float32\n",
            "backbone.level4.1.recp3.0.bn.bias: torch.float32\n",
            "backbone.level4.1.recp3.0.relu.weight: torch.float32\n",
            "backbone.level4.1.recp3.1.conv.weight: torch.float32\n",
            "backbone.level4.1.recp3.1.bn.weight: torch.float32\n",
            "backbone.level4.1.recp3.1.bn.bias: torch.float32\n",
            "backbone.level4.1.recp3.1.relu.weight: torch.float32\n",
            "backbone.level4.1.recp3.2.conv.weight: torch.float32\n",
            "backbone.level4.1.recp3.2.bn.weight: torch.float32\n",
            "backbone.level4.1.recp3.2.bn.bias: torch.float32\n",
            "backbone.level4.1.recp3.3.conv.weight: torch.float32\n",
            "backbone.level4.1.recp3.3.bn.weight: torch.float32\n",
            "backbone.level4.1.recp3.3.bn.bias: torch.float32\n",
            "backbone.level4.1.recp3.3.relu.weight: torch.float32\n",
            "backbone.level4.1.recp1.0.conv.weight: torch.float32\n",
            "backbone.level4.1.recp1.0.bn.weight: torch.float32\n",
            "backbone.level4.1.recp1.0.bn.bias: torch.float32\n",
            "backbone.level4.1.recp1.0.relu.weight: torch.float32\n",
            "backbone.level4.2.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level4.2.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level4.2.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level4.2.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level4.2.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level4.4.recp7.0.conv.weight: torch.float32\n",
            "backbone.level4.4.recp7.0.bn.weight: torch.float32\n",
            "backbone.level4.4.recp7.0.bn.bias: torch.float32\n",
            "backbone.level4.4.recp7.0.relu.weight: torch.float32\n",
            "backbone.level4.4.recp7.1.conv.weight: torch.float32\n",
            "backbone.level4.4.recp7.1.bn.weight: torch.float32\n",
            "backbone.level4.4.recp7.1.bn.bias: torch.float32\n",
            "backbone.level4.4.recp7.1.relu.weight: torch.float32\n",
            "backbone.level4.4.recp7.2.conv.weight: torch.float32\n",
            "backbone.level4.4.recp7.2.bn.weight: torch.float32\n",
            "backbone.level4.4.recp7.2.bn.bias: torch.float32\n",
            "backbone.level4.4.recp7.3.conv.weight: torch.float32\n",
            "backbone.level4.4.recp7.3.bn.weight: torch.float32\n",
            "backbone.level4.4.recp7.3.bn.bias: torch.float32\n",
            "backbone.level4.4.recp7.3.relu.weight: torch.float32\n",
            "backbone.level4.4.recp5.0.conv.weight: torch.float32\n",
            "backbone.level4.4.recp5.0.bn.weight: torch.float32\n",
            "backbone.level4.4.recp5.0.bn.bias: torch.float32\n",
            "backbone.level4.4.recp5.0.relu.weight: torch.float32\n",
            "backbone.level4.4.recp5.1.conv.weight: torch.float32\n",
            "backbone.level4.4.recp5.1.bn.weight: torch.float32\n",
            "backbone.level4.4.recp5.1.bn.bias: torch.float32\n",
            "backbone.level4.4.recp5.1.relu.weight: torch.float32\n",
            "backbone.level4.4.recp5.2.conv.weight: torch.float32\n",
            "backbone.level4.4.recp5.2.bn.weight: torch.float32\n",
            "backbone.level4.4.recp5.2.bn.bias: torch.float32\n",
            "backbone.level4.4.recp5.3.conv.weight: torch.float32\n",
            "backbone.level4.4.recp5.3.bn.weight: torch.float32\n",
            "backbone.level4.4.recp5.3.bn.bias: torch.float32\n",
            "backbone.level4.4.recp5.3.relu.weight: torch.float32\n",
            "backbone.level4.4.recp3.0.conv.weight: torch.float32\n",
            "backbone.level4.4.recp3.0.bn.weight: torch.float32\n",
            "backbone.level4.4.recp3.0.bn.bias: torch.float32\n",
            "backbone.level4.4.recp3.0.relu.weight: torch.float32\n",
            "backbone.level4.4.recp3.1.conv.weight: torch.float32\n",
            "backbone.level4.4.recp3.1.bn.weight: torch.float32\n",
            "backbone.level4.4.recp3.1.bn.bias: torch.float32\n",
            "backbone.level4.4.recp3.1.relu.weight: torch.float32\n",
            "backbone.level4.4.recp3.2.conv.weight: torch.float32\n",
            "backbone.level4.4.recp3.2.bn.weight: torch.float32\n",
            "backbone.level4.4.recp3.2.bn.bias: torch.float32\n",
            "backbone.level4.4.recp3.3.conv.weight: torch.float32\n",
            "backbone.level4.4.recp3.3.bn.weight: torch.float32\n",
            "backbone.level4.4.recp3.3.bn.bias: torch.float32\n",
            "backbone.level4.4.recp3.3.relu.weight: torch.float32\n",
            "backbone.level4.4.recp1.0.conv.weight: torch.float32\n",
            "backbone.level4.4.recp1.0.bn.weight: torch.float32\n",
            "backbone.level4.4.recp1.0.bn.bias: torch.float32\n",
            "backbone.level4.4.recp1.0.relu.weight: torch.float32\n",
            "backbone.level4.5.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level4.5.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level4.5.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level4.5.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level4.5.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level4.7.recp7.0.conv.weight: torch.float32\n",
            "backbone.level4.7.recp7.0.bn.weight: torch.float32\n",
            "backbone.level4.7.recp7.0.bn.bias: torch.float32\n",
            "backbone.level4.7.recp7.0.relu.weight: torch.float32\n",
            "backbone.level4.7.recp7.1.conv.weight: torch.float32\n",
            "backbone.level4.7.recp7.1.bn.weight: torch.float32\n",
            "backbone.level4.7.recp7.1.bn.bias: torch.float32\n",
            "backbone.level4.7.recp7.1.relu.weight: torch.float32\n",
            "backbone.level4.7.recp7.2.conv.weight: torch.float32\n",
            "backbone.level4.7.recp7.2.bn.weight: torch.float32\n",
            "backbone.level4.7.recp7.2.bn.bias: torch.float32\n",
            "backbone.level4.7.recp7.3.conv.weight: torch.float32\n",
            "backbone.level4.7.recp7.3.bn.weight: torch.float32\n",
            "backbone.level4.7.recp7.3.bn.bias: torch.float32\n",
            "backbone.level4.7.recp7.3.relu.weight: torch.float32\n",
            "backbone.level4.7.recp5.0.conv.weight: torch.float32\n",
            "backbone.level4.7.recp5.0.bn.weight: torch.float32\n",
            "backbone.level4.7.recp5.0.bn.bias: torch.float32\n",
            "backbone.level4.7.recp5.0.relu.weight: torch.float32\n",
            "backbone.level4.7.recp5.1.conv.weight: torch.float32\n",
            "backbone.level4.7.recp5.1.bn.weight: torch.float32\n",
            "backbone.level4.7.recp5.1.bn.bias: torch.float32\n",
            "backbone.level4.7.recp5.1.relu.weight: torch.float32\n",
            "backbone.level4.7.recp5.2.conv.weight: torch.float32\n",
            "backbone.level4.7.recp5.2.bn.weight: torch.float32\n",
            "backbone.level4.7.recp5.2.bn.bias: torch.float32\n",
            "backbone.level4.7.recp5.3.conv.weight: torch.float32\n",
            "backbone.level4.7.recp5.3.bn.weight: torch.float32\n",
            "backbone.level4.7.recp5.3.bn.bias: torch.float32\n",
            "backbone.level4.7.recp5.3.relu.weight: torch.float32\n",
            "backbone.level4.7.recp3.0.conv.weight: torch.float32\n",
            "backbone.level4.7.recp3.0.bn.weight: torch.float32\n",
            "backbone.level4.7.recp3.0.bn.bias: torch.float32\n",
            "backbone.level4.7.recp3.0.relu.weight: torch.float32\n",
            "backbone.level4.7.recp3.1.conv.weight: torch.float32\n",
            "backbone.level4.7.recp3.1.bn.weight: torch.float32\n",
            "backbone.level4.7.recp3.1.bn.bias: torch.float32\n",
            "backbone.level4.7.recp3.1.relu.weight: torch.float32\n",
            "backbone.level4.7.recp3.2.conv.weight: torch.float32\n",
            "backbone.level4.7.recp3.2.bn.weight: torch.float32\n",
            "backbone.level4.7.recp3.2.bn.bias: torch.float32\n",
            "backbone.level4.7.recp3.3.conv.weight: torch.float32\n",
            "backbone.level4.7.recp3.3.bn.weight: torch.float32\n",
            "backbone.level4.7.recp3.3.bn.bias: torch.float32\n",
            "backbone.level4.7.recp3.3.relu.weight: torch.float32\n",
            "backbone.level4.7.recp1.0.conv.weight: torch.float32\n",
            "backbone.level4.7.recp1.0.bn.weight: torch.float32\n",
            "backbone.level4.7.recp1.0.bn.bias: torch.float32\n",
            "backbone.level4.7.recp1.0.relu.weight: torch.float32\n",
            "backbone.level4.8.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level4.8.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level4.8.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level4.8.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level4.8.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level4.10.recp7.0.conv.weight: torch.float32\n",
            "backbone.level4.10.recp7.0.bn.weight: torch.float32\n",
            "backbone.level4.10.recp7.0.bn.bias: torch.float32\n",
            "backbone.level4.10.recp7.0.relu.weight: torch.float32\n",
            "backbone.level4.10.recp7.1.conv.weight: torch.float32\n",
            "backbone.level4.10.recp7.1.bn.weight: torch.float32\n",
            "backbone.level4.10.recp7.1.bn.bias: torch.float32\n",
            "backbone.level4.10.recp7.1.relu.weight: torch.float32\n",
            "backbone.level4.10.recp7.2.conv.weight: torch.float32\n",
            "backbone.level4.10.recp7.2.bn.weight: torch.float32\n",
            "backbone.level4.10.recp7.2.bn.bias: torch.float32\n",
            "backbone.level4.10.recp7.3.conv.weight: torch.float32\n",
            "backbone.level4.10.recp7.3.bn.weight: torch.float32\n",
            "backbone.level4.10.recp7.3.bn.bias: torch.float32\n",
            "backbone.level4.10.recp7.3.relu.weight: torch.float32\n",
            "backbone.level4.10.recp5.0.conv.weight: torch.float32\n",
            "backbone.level4.10.recp5.0.bn.weight: torch.float32\n",
            "backbone.level4.10.recp5.0.bn.bias: torch.float32\n",
            "backbone.level4.10.recp5.0.relu.weight: torch.float32\n",
            "backbone.level4.10.recp5.1.conv.weight: torch.float32\n",
            "backbone.level4.10.recp5.1.bn.weight: torch.float32\n",
            "backbone.level4.10.recp5.1.bn.bias: torch.float32\n",
            "backbone.level4.10.recp5.1.relu.weight: torch.float32\n",
            "backbone.level4.10.recp5.2.conv.weight: torch.float32\n",
            "backbone.level4.10.recp5.2.bn.weight: torch.float32\n",
            "backbone.level4.10.recp5.2.bn.bias: torch.float32\n",
            "backbone.level4.10.recp5.3.conv.weight: torch.float32\n",
            "backbone.level4.10.recp5.3.bn.weight: torch.float32\n",
            "backbone.level4.10.recp5.3.bn.bias: torch.float32\n",
            "backbone.level4.10.recp5.3.relu.weight: torch.float32\n",
            "backbone.level4.10.recp3.0.conv.weight: torch.float32\n",
            "backbone.level4.10.recp3.0.bn.weight: torch.float32\n",
            "backbone.level4.10.recp3.0.bn.bias: torch.float32\n",
            "backbone.level4.10.recp3.0.relu.weight: torch.float32\n",
            "backbone.level4.10.recp3.1.conv.weight: torch.float32\n",
            "backbone.level4.10.recp3.1.bn.weight: torch.float32\n",
            "backbone.level4.10.recp3.1.bn.bias: torch.float32\n",
            "backbone.level4.10.recp3.1.relu.weight: torch.float32\n",
            "backbone.level4.10.recp3.2.conv.weight: torch.float32\n",
            "backbone.level4.10.recp3.2.bn.weight: torch.float32\n",
            "backbone.level4.10.recp3.2.bn.bias: torch.float32\n",
            "backbone.level4.10.recp3.3.conv.weight: torch.float32\n",
            "backbone.level4.10.recp3.3.bn.weight: torch.float32\n",
            "backbone.level4.10.recp3.3.bn.bias: torch.float32\n",
            "backbone.level4.10.recp3.3.relu.weight: torch.float32\n",
            "backbone.level4.10.recp1.0.conv.weight: torch.float32\n",
            "backbone.level4.10.recp1.0.bn.weight: torch.float32\n",
            "backbone.level4.10.recp1.0.bn.bias: torch.float32\n",
            "backbone.level4.10.recp1.0.relu.weight: torch.float32\n",
            "backbone.level4.11.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level4.11.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level4.11.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level4.11.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level4.11.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.level4.13.recp7.0.conv.weight: torch.float32\n",
            "backbone.level4.13.recp7.0.bn.weight: torch.float32\n",
            "backbone.level4.13.recp7.0.bn.bias: torch.float32\n",
            "backbone.level4.13.recp7.0.relu.weight: torch.float32\n",
            "backbone.level4.13.recp7.1.conv.weight: torch.float32\n",
            "backbone.level4.13.recp7.1.bn.weight: torch.float32\n",
            "backbone.level4.13.recp7.1.bn.bias: torch.float32\n",
            "backbone.level4.13.recp7.1.relu.weight: torch.float32\n",
            "backbone.level4.13.recp7.2.conv.weight: torch.float32\n",
            "backbone.level4.13.recp7.2.bn.weight: torch.float32\n",
            "backbone.level4.13.recp7.2.bn.bias: torch.float32\n",
            "backbone.level4.13.recp7.3.conv.weight: torch.float32\n",
            "backbone.level4.13.recp7.3.bn.weight: torch.float32\n",
            "backbone.level4.13.recp7.3.bn.bias: torch.float32\n",
            "backbone.level4.13.recp7.3.relu.weight: torch.float32\n",
            "backbone.level4.13.recp5.0.conv.weight: torch.float32\n",
            "backbone.level4.13.recp5.0.bn.weight: torch.float32\n",
            "backbone.level4.13.recp5.0.bn.bias: torch.float32\n",
            "backbone.level4.13.recp5.0.relu.weight: torch.float32\n",
            "backbone.level4.13.recp5.1.conv.weight: torch.float32\n",
            "backbone.level4.13.recp5.1.bn.weight: torch.float32\n",
            "backbone.level4.13.recp5.1.bn.bias: torch.float32\n",
            "backbone.level4.13.recp5.1.relu.weight: torch.float32\n",
            "backbone.level4.13.recp5.2.conv.weight: torch.float32\n",
            "backbone.level4.13.recp5.2.bn.weight: torch.float32\n",
            "backbone.level4.13.recp5.2.bn.bias: torch.float32\n",
            "backbone.level4.13.recp5.3.conv.weight: torch.float32\n",
            "backbone.level4.13.recp5.3.bn.weight: torch.float32\n",
            "backbone.level4.13.recp5.3.bn.bias: torch.float32\n",
            "backbone.level4.13.recp5.3.relu.weight: torch.float32\n",
            "backbone.level4.13.recp3.0.conv.weight: torch.float32\n",
            "backbone.level4.13.recp3.0.bn.weight: torch.float32\n",
            "backbone.level4.13.recp3.0.bn.bias: torch.float32\n",
            "backbone.level4.13.recp3.0.relu.weight: torch.float32\n",
            "backbone.level4.13.recp3.1.conv.weight: torch.float32\n",
            "backbone.level4.13.recp3.1.bn.weight: torch.float32\n",
            "backbone.level4.13.recp3.1.bn.bias: torch.float32\n",
            "backbone.level4.13.recp3.1.relu.weight: torch.float32\n",
            "backbone.level4.13.recp3.2.conv.weight: torch.float32\n",
            "backbone.level4.13.recp3.2.bn.weight: torch.float32\n",
            "backbone.level4.13.recp3.2.bn.bias: torch.float32\n",
            "backbone.level4.13.recp3.3.conv.weight: torch.float32\n",
            "backbone.level4.13.recp3.3.bn.weight: torch.float32\n",
            "backbone.level4.13.recp3.3.bn.bias: torch.float32\n",
            "backbone.level4.13.recp3.3.relu.weight: torch.float32\n",
            "backbone.level4.13.recp1.0.conv.weight: torch.float32\n",
            "backbone.level4.13.recp1.0.bn.weight: torch.float32\n",
            "backbone.level4.13.recp1.0.bn.bias: torch.float32\n",
            "backbone.level4.13.recp1.0.relu.weight: torch.float32\n",
            "backbone.level4.14.channel_att.fc.0.weight: torch.float32\n",
            "backbone.level4.14.channel_att.fc.2.weight: torch.float32\n",
            "backbone.level4.14.spatialatt.spatial.conv.weight: torch.float32\n",
            "backbone.level4.14.spatialatt.spatial.bn.weight: torch.float32\n",
            "backbone.level4.14.spatialatt.spatial.bn.bias: torch.float32\n",
            "backbone.branch4.weight: torch.float32\n",
            "backbone.br4.0.weight: torch.float32\n",
            "backbone.br4.0.bias: torch.float32\n",
            "backbone.br4.1.weight: torch.float32\n",
            "up3_conv4.conv0.weight: torch.float32\n",
            "up3_conv4.conv1.weight: torch.float32\n",
            "up3_conv4.conv2.weight: torch.float32\n",
            "up3_conv4.bn.weight: torch.float32\n",
            "up3_conv4.bn.bias: torch.float32\n",
            "up3_conv3.weight: torch.float32\n",
            "up3_bn3.weight: torch.float32\n",
            "up3_bn3.bias: torch.float32\n",
            "up3_act.weight: torch.float32\n",
            "up2_conv3.conv0.weight: torch.float32\n",
            "up2_conv3.conv1.weight: torch.float32\n",
            "up2_conv3.conv2.weight: torch.float32\n",
            "up2_conv3.bn.weight: torch.float32\n",
            "up2_conv3.bn.bias: torch.float32\n",
            "up2_conv2.weight: torch.float32\n",
            "up2_bn2.weight: torch.float32\n",
            "up2_bn2.bias: torch.float32\n",
            "up2_act.weight: torch.float32\n",
            "up1_conv2.conv0.weight: torch.float32\n",
            "up1_conv2.conv1.weight: torch.float32\n",
            "up1_conv2.conv2.weight: torch.float32\n",
            "up1_conv2.bn.weight: torch.float32\n",
            "up1_conv2.bn.bias: torch.float32\n",
            "up1_conv1.weight: torch.float32\n",
            "up1_bn1.weight: torch.float32\n",
            "up1_bn1.bias: torch.float32\n",
            "up1_act.weight: torch.float32\n",
            "classifier4.1.weight: torch.float32\n",
            "classifier3.1.weight: torch.float32\n",
            "classifier2.1.weight: torch.float32\n",
            "classifier1.1.weight: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, buffer in model.named_buffers():\n",
        "    print(f\"{name}: {buffer.dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "0kH3dnHZonQP",
        "outputId": "fe8be02b-b94a-401b-f4be-096b4b6b6e91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-149694585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name}: {buffer.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.pytorch.org/executorch/stable/backends-xnnpack.html\n",
        "\n",
        "\n",
        "https://blog.tensorflow.org/2023/11/half-precision-inference-doubles-on-device-inference-performance.html\n",
        "\n",
        "Interestingly, XNNPack does support FP16 inference — but only on devices with hardware support (like ARMv8.2 CPUs). If your model has FP16 weights and includes the right metadata, XNNPack can transparently switch to FP16 for performance gains"
      ],
      "metadata": {
        "id": "zcx-pOESoy8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(torch.float32)\n"
      ],
      "metadata": {
        "id": "_eFjtTLeo6R0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijUpsCtoAiuB"
      },
      "source": [
        "# Export\n",
        "\n",
        "https://docs.pytorch.org/executorch/stable/getting-started.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMpReOlu-rS9",
        "outputId": "4e5d4377-99e8-4271-d87c-60cd6ab87c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the BSD-style license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
        "from executorch.exir import to_edge_transform_and_lower\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    model = FastSal().eval() #U2NET().eval().to(\"cpu\")#SeaNet(pretrained=False).eval().to(\"cpu\")#FastSal().eval().to(\"cpu\")\n",
        "    sample_inputs = (torch.randn(1, 3, 224, 224), )\n",
        "\n",
        "    et_program = to_edge_transform_and_lower(\n",
        "        torch.export.export(model, sample_inputs),\n",
        "        partitioner=[XnnpackPartitioner()],\n",
        "    ).to_executorch()\n",
        "\n",
        "    with open(\"hvpnet_xnnpack_fp32.pte\", \"wb\") as file:\n",
        "        #et_program.write_to_file(file)\n",
        "        file.write(et_program.buffer)\n",
        "\n",
        "    print(\"Finished!\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from executorch.runtime import Runtime\n",
        "from typing import List\n",
        "\n",
        "runtime = Runtime.get()\n",
        "\n",
        "input_tensor: torch.Tensor = torch.randn(1, 3, 224, 224)\n",
        "program = runtime.load_program(\"hvpnet_xnnpack_fp32.pte\")\n",
        "method = program.load_method(\"forward\")\n",
        "output: List[torch.Tensor] = method.execute([input_tensor])\n",
        "print(\"Run succesfully via executorch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1BoucsHv7Xn",
        "outputId": "f217f32d-0661-425a-ee22-37bd2e309a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[program.cpp:135] InternalConsistency verification requested but not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "b8fjDZ0Jg5MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from executorch.runtime import Runtime"
      ],
      "metadata": {
        "id": "x42JpfDOg1tv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import executorch\n",
        "executorch.extension.\n",
        "print(executorch.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "pHZOXyIXt0hp",
        "outputId": "a2d96717-f554-40d6-a9d0-02cbddee2345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'executorch' has no attribute 'version'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3471241134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'executorch' has no attribute 'version'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(\"hvpnet_xnnpack_fp32.pte\")\n",
        "method = program.load_method(\"forward\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "MO6_lfr2g6YW",
        "outputId": "d3d84ca8-53aa-4179-8b47-3a0d61356f80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Runtime.load_program() got an unexpected keyword argument 'program_verification'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-170318850.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_program\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hvpnet_xnnpack_fp32.pte\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mprogram_verification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Minimal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Runtime.load_program() got an unexpected keyword argument 'program_verification'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method.metadata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7swumypWq9Jw",
        "outputId": "a99dee66-7934-47d9-d5c9-2dbae5d210a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MethodMeta(name='forward', num_inputs=1, input_tensor_meta=['TensorInfo(sizes=[1, 3, 224, 224], dtype=Float, is_memory_planned=True, nbytes=602112)'], num_outputs=7, output_tensor_meta=['TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)', 'TensorInfo(sizes=[1, 1, 224, 224], dtype=Float, is_memory_planned=True, nbytes=200704)'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "sample_image = torch.randn(1, 3, 224, 224).to(\"cpu\").to(torch.float32)\n",
        "sample_image.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckMLWw8ErJJa",
        "outputId": "16d685ef-1a55-48fb-a092-be273d6f96fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_image = sample_image.to(torch.float32)\n"
      ],
      "metadata": {
        "id": "kyRq6rr_rIY-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/var/colab/app.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "SwqA78Yosyup",
        "outputId": "ce3a1fe3-bf1e-418e-fd09-a68a3446cedf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'var' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-2599167184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvar\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'var' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = method.execute((sample_image,))\n",
        "    print(\"Success:\", result)\n",
        "except Exception as e:\n",
        "    print(\"ExecuTorch crash:\", e)\n"
      ],
      "metadata": {
        "id": "rNT5kfZshAcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5UzPPlFkm80",
        "outputId": "962c8b68-cc8a-499b-d6f1-941ca2b73669"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show executorch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B15qPU6ucr6",
        "outputId": "7f101d50-5c9e-4217-a958-6ff53b53378e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: executorch\n",
            "Version: 0.6.0\n",
            "Summary: On-device AI across mobile, embedded and edge for PyTorch\n",
            "Home-page: https://pytorch.org/executorch/\n",
            "Author: \n",
            "Author-email: PyTorch Team <packages@pytorch.org>\n",
            "License: BSD License\n",
            "\n",
            "For \"ExecuTorch\" software\n",
            "\n",
            "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
            "Copyright 2023 Arm Limited and/or its affiliates.\n",
            "Copyright (c) Qualcomm Innovation Center, Inc.\n",
            "Copyright (c) 2023 Apple Inc.\n",
            "Copyright (c) 2024 MediaTek Inc.\n",
            "\n",
            "Redistribution and use in source and binary forms, with or without modification,\n",
            "are permitted provided that the following conditions are met:\n",
            "\n",
            " * Redistributions of source code must retain the above copyright notice, this\n",
            "   list of conditions and the following disclaimer.\n",
            "\n",
            " * Redistributions in binary form must reproduce the above copyright notice,\n",
            "   this list of conditions and the following disclaimer in the documentation\n",
            "   and/or other materials provided with the distribution.\n",
            "\n",
            " * Neither the name Meta nor the names of its contributors may be used to\n",
            "   endorse or promote products derived from this software without specific\n",
            "   prior written permission.\n",
            "\n",
            "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
            "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
            "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
            "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
            "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
            "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
            "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: expecttest, flatbuffers, hypothesis, mpmath, numpy, packaging, pandas, parameterized, pytest, pytest-rerunfailures, pytest-xdist, pyyaml, ruamel.yaml, sympy, tabulate, torch, torchao, torchaudio, torchvision, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX"
      ],
      "metadata": {
        "id": "HUAQdL-F7rG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime onnx onnxsim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq3gzYgW8DZQ",
        "outputId": "2bff7f69-14f2-4c52-9492-adb9ac359ab6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.1)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, onnxsim\n",
            "Successfully installed onnx-1.18.0 onnxsim-0.4.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = FastSal()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"hvpnet.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['classifier1', 'classifier2', 'classifier3', 'classifier4'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'classifier1': {0: 'batch_size'},\n",
        "        'classifier2': {0: 'batch_size'},\n",
        "        'classifier3': {0: 'batch_size'},\n",
        "        'classifier4': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSFsVHpW7rqe",
        "outputId": "24f961ae-3d95-40b2-94a3-9ddee78afa10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX model exported as fastsal.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = U2NET()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"u2net.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['out1', 'out2', 'out3', 'out4', 'out5', 'out6'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'out1': {0: 'batch_size'},\n",
        "        'out2': {0: 'batch_size'},\n",
        "        'out3': {0: 'batch_size'},\n",
        "        'out4': {0: 'batch_size'},\n",
        "        'out5': {0: 'batch_size'},\n",
        "        'out6': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdKpfdH0OiTk",
        "outputId": "fb966f07-631a-4088-b71f-98257d180a2b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-836780321.py:25: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
            "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX model exported as .onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = U2NET()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"u2net.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['out1', 'out2', 'out3', 'out4', 'out5', 'out6'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'out1': {0: 'batch_size'},\n",
        "        'out2': {0: 'batch_size'},\n",
        "        'out3': {0: 'batch_size'},\n",
        "        'out4': {0: 'batch_size'},\n",
        "        'out5': {0: 'batch_size'},\n",
        "        'out6': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "id": "Pr6mJ7xTr9UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = SeaNet(pretrained=False)\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"seanet.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['s12', 's34_up', 's5_up', 'sigmoid1', 'sigmoid2', 'sigmoid3', 'edge1', 'edge2'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        's12': {0: 'batch_size'},\n",
        "        's34_up': {0: 'batch_size'},\n",
        "        's5_up': {0: 'batch_size'},\n",
        "        'sigmoid1': {0: 'batch_size'},\n",
        "        'sigmoid2': {0: 'batch_size'},\n",
        "        'sigmoid3': {0: 'batch_size'},\n",
        "        'edge1': {0: 'batch_size'},\n",
        "        'edge2': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WOWZ6XQsdzn",
        "outputId": "3eeaaac0-fd75-49d8-be99-4a7a2ab4c743"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX model exported as .onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet(config, (224, 224)).eval()\n",
        "\n",
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = ConvNet(config, (224, 224)).eval()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"flim_small.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['x'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'x': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht4OyVAxwehC",
        "outputId": "2a1a6cc5-c1d1-4a0f-f91f-e16269652333"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX model exported as .onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Create the model and set to evaluation mode\n",
        "model = FastSal()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"samnet.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=[ 'output_main', 'output_side1', 'output_side2', 'output_side3', 'output_side4' ],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'output_main': {0: 'batch_size'},\n",
        "        'output_side1': {0: 'batch_size'},\n",
        "        'output_side2': {0: 'batch_size'},\n",
        "        'output_side3': {0: 'batch_size'},\n",
        "        'output_side4': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "print(\"✅ ONNX model exported as .onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5AKzzk9UuA3J",
        "outputId": "e28a53fa-4f3e-4126-d2c2-99726b3a221f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SymbolicValueError",
          "evalue": "Unsupported: ONNX export of operator adaptive_avg_pool2d, input size not accessible. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues  [Caused by the value 'x defined in (%x : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2468, %input.1752), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3 # /tmp/ipython-input-35-4151006688.py:203:0\n)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Add'.] \n    (node defined in /tmp/ipython-input-35-4151006688.py(203): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py(240): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/tmp/ipython-input-35-4151006688.py(158): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/tmp/ipython-input-35-4151006688.py(35): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(129): wrapper\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(138): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(1501): _get_trace_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(878): _trace_and_get_graph_from_model\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(971): _create_jit_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(1087): _model_to_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(1467): _export\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(529): export\n/usr/local/lib/python3.11/dist-packages/torch/onnx/__init__.py(396): export\n/tmp/ipython-input-38-2846696901.py(12): <cell line: 0>\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3553): run_code\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3473): run_ast_nodes\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3257): run_cell_async\n/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3030): _run_cell\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(2975): run_cell\n/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py(528): run_cell\n/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py(383): do_execute\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(730): execute_request\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(406): dispatch_shell\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(499): process_one\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(510): dispatch_queue\n/usr/lib/python3.11/asyncio/events.py(84): _run\n/usr/lib/python3.11/asyncio/base_events.py(1936): _run_once\n/usr/lib/python3.11/asyncio/base_events.py(608): run_forever\n/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py(205): start\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py(712): start\n/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py(992): launch_instance\n/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py(37): <module>\n<frozen runpy>(88): _run_code\n<frozen runpy>(198): _run_module_as_main\n)\n\n    Inputs:\n        #0: 2468 defined in (%2468 : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%input.1864, %context_path.layer5.3.fuse.conv.1.weight, %context_path.layer5.3.fuse.conv.1.bias, %context_path.layer5.3.fuse.conv.1.running_mean, %context_path.layer5.3.fuse.conv.1.running_var), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3/__main__.convbnrelu::fuse/torch.nn.modules.container.Sequential::conv/torch.nn.modules.batchnorm.BatchNorm2d::conv.1 # /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:2822:0\n    )  (type 'Tensor')\n        #1: input.1752 defined in (%input.1752 : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2406, %input.1636), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.2 # /tmp/ipython-input-35-4151006688.py:203:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: x defined in (%x : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2468, %input.1752), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3 # /tmp/ipython-input-35-4151006688.py:203:0\n    )  (type 'Tensor')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSymbolicValueError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-38-2846696901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Export to ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m torch.onnx.export(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    394\u001b[0m             )\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         export(\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1468\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         graph = _optimize_graph(\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msymbolic_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributeNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m                 }\n\u001b[0;32m-> 1736\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msymbolic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         attrs = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(g, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;31m# Dequantize arguments that are quantized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py\u001b[0m in \u001b[0;36msymbolic_fn\u001b[0;34m(g, input, output_size)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GlobalMaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m             return symbolic_helper._unimplemented(\n\u001b[0m\u001b[1;32m   1693\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input size not accessible\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m_unimplemented\u001b[0;34m(op, msg, value)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;31m# For BC reasons, the behavior for Caffe2 does not raise exception for unimplemented operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGLOBALS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_export_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_C_onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0m_onnx_unsupported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{op}, {msg}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m_onnx_unsupported\u001b[0;34m(op_name, value)\u001b[0m\n\u001b[1;32m    571\u001b[0m     )\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise errors.SymbolicValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSymbolicValueError\u001b[0m: Unsupported: ONNX export of operator adaptive_avg_pool2d, input size not accessible. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues  [Caused by the value 'x defined in (%x : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2468, %input.1752), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3 # /tmp/ipython-input-35-4151006688.py:203:0\n)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Add'.] \n    (node defined in /tmp/ipython-input-35-4151006688.py(203): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py(240): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/tmp/ipython-input-35-4151006688.py(158): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/tmp/ipython-input-35-4151006688.py(35): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1741): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(129): wrapper\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(138): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1762): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(1501): _get_trace_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(878): _trace_and_get_graph_from_model\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(971): _create_jit_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(1087): _model_to_graph\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(1467): _export\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py(529): export\n/usr/local/lib/python3.11/dist-packages/torch/onnx/__init__.py(396): export\n/tmp/ipython-input-38-2846696901.py(12): <cell line: 0>\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3553): run_code\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3473): run_ast_nodes\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3257): run_cell_async\n/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(3030): _run_cell\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py(2975): run_cell\n/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py(528): run_cell\n/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py(383): do_execute\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(730): execute_request\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(406): dispatch_shell\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(499): process_one\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py(510): dispatch_queue\n/usr/lib/python3.11/asyncio/events.py(84): _run\n/usr/lib/python3.11/asyncio/base_events.py(1936): _run_once\n/usr/lib/python3.11/asyncio/base_events.py(608): run_forever\n/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py(205): start\n/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py(712): start\n/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py(992): launch_instance\n/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py(37): <module>\n<frozen runpy>(88): _run_code\n<frozen runpy>(198): _run_module_as_main\n)\n\n    Inputs:\n        #0: 2468 defined in (%2468 : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%input.1864, %context_path.layer5.3.fuse.conv.1.weight, %context_path.layer5.3.fuse.conv.1.bias, %context_path.layer5.3.fuse.conv.1.running_mean, %context_path.layer5.3.fuse.conv.1.running_var), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3/__main__.convbnrelu::fuse/torch.nn.modules.container.Sequential::conv/torch.nn.modules.batchnorm.BatchNorm2d::conv.1 # /usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:2822:0\n    )  (type 'Tensor')\n        #1: input.1752 defined in (%input.1752 : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2406, %input.1636), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.2 # /tmp/ipython-input-35-4151006688.py:203:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: x defined in (%x : Float(*, 128, *, *, strides=[6272, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%2468, %input.1752), scope: __main__.FastSal::/__main__.VAMM_backbone::context_path/torch.nn.modules.container.Sequential::layer5/__main__.VAMM::layer5.3 # /tmp/ipython-input-35-4151006688.py:203:0\n    )  (type 'Tensor')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional\n",
        "from onnxsim import simplify\n",
        "import onnx\n",
        "\n",
        "model_path = \"fastsal.onnx\"\n",
        "onnx_model = onnx.load(model_path)\n",
        "model_simplified, check = simplify(onnx_model)\n",
        "\n",
        "if check:\n",
        "    onnx.save(model_simplified, \"fastsal_simplified.onnx\")\n",
        "    print(\"✅ Simplified model saved as fastsal_simplified.onnx\")\n",
        "else:\n",
        "    print(\"❌ Simplification failed.\")\n"
      ],
      "metadata": {
        "id": "DKOi5_lk8ezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "6NOD713p8jlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Load the ONNX model\n",
        "session = ort.InferenceSession(\"flim_small.onnx\")\n",
        "\n",
        "# Print input/output names\n",
        "print(\"Inputs:\", [i.name for i in session.get_inputs()])\n",
        "print(\"Outputs:\", [o.name for o in session.get_outputs()])\n",
        "\n",
        "# Create a dummy input tensor (batch size 1, 3 channels, 224x224 image)\n",
        "dummy_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
        "\n",
        "# Run inference\n",
        "outputs = session.run(None, {\"input\": dummy_input})\n",
        "\n",
        "# Check output shapes\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"Output {i + 1}: shape = {out.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd-IgBuV8kfL",
        "outputId": "d82013d6-4b70-4227-e96a-ed03f57dc41d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: ['input']\n",
            "Outputs: ['x']\n",
            "Output 1: shape = (1, 1, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/microsoft/onnxruntime-inference-examples/blob/main/mobile/README.md\n"
      ],
      "metadata": {
        "id": "y1GM0WoS9gFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare.py --output_dir /content/outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRTtSQa-9glp",
        "outputId": "60769908-b717-462d-febc-f7756d34780b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached MobileNet v2-1.0-int8 model from /root/.cache/onnx/hub/validated/vision/classification/mobilenet/model/cc028fe6cae7bc11a4ff53cfc9b79c920e8be65ce33a904ec3e2a8f66d77f95f_mobilenetv2-12-int8.onnx\n",
            "Using cached MobileNet v2-1.0-fp32 model from /root/.cache/onnx/hub/validated/vision/classification/mobilenet/model/c0c3f76d93fa3fd6580652a45618618a220fced18babf65774ed169de0432ad5_mobilenetv2-12.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val inputStream = assets.open(\"u2net.onnx\")\n",
        "val modelBytes = inputStream.readBytes()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "-u0Pk9xNdJCT",
        "outputId": "1d03f316-9900-409d-fa84-1ad0b49ebb85"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-21-1429801123.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-21-1429801123.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val inputStream = assets.open(\"u2net.onnx\")\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}